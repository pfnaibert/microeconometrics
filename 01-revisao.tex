\documentclass[11pt,oneside,a4paper]{article}
% \documentclass[11pt,oneside,a4paper]{letter}
\usepackage[a4paper, margin=1in]{geometry}   % MARGENS

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pacotes básicos 
\usepackage[T1]{fontenc}		% Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}		% Codificacao do documento (conversão automática dos acentos)
\usepackage[brazilian]{babel}
% \selectlanguage{english}
\usepackage{amsfonts, amsmath, amssymb, amsthm}
\usepackage{mathtools, latexsym}
% \usepackage{mathabx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pacotes de citações
\usepackage{natbib}	% Citações}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{indentfirst}		% Indenta o primeiro parágrafo de cada seção.
\usepackage{color}				% Controle das cores
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{microtype} 			% para melhorias de justificação
\usepackage{lscape}				% Gira a página em 90 graus
%\usepackage{listings}			% Formatação para inserir códigos
\usepackage[normalem]{ulem}
\usepackage[all]{xy}
\usepackage{xcolor}
\usepackage{ragged2e}           % formatação texto
\usepackage{bm}                 % bold symbols 
\usepackage[colorlinks, citecolor=blue, urlcolor=blue, linkcolor=red]{hyperref} % referencias dentro do texo (*QUEBRA MEMOIR*)
\usepackage{url}                % URL
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{subcaption}			% Faz subfiguras
\usepackage{textcase}			% MakeTextUppercase
%\usepackage{subfigure}         % subfigures
% \usepackage{setspace}         % Espaçamento
\usepackage{pdfpages}           % inclui páginas de pdfs (*FICHA CATALOGRAFICA*)
\usepackage[flushleft]{threeparttable} % notas nas tabelas
\usepackage{enumitem}
\usepackage[sharp]{easylist}
\usepackage{titling}          % personalized other things
\usepackage{fancyhdr}         % personalized page style
\usepackage{lipsum}           % dummy text
% \usepackage{exercise}       % exercises
% \usepackage[displaymath, pagewise]{lineno}           % show line numbers
\usepackage[pagewise]{lineno}           % show line numbers
% \linenumbers

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{../macros.tex}
% \pagestyle{headings}
\pagestyle{fancy}
\numberwithin{equation}{section}
% \setcounter{equation}{0}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METADATA
\title{Lista Microeconometria}
\author{Paulo F. Naibert}
% \date{25/06/2020}
% \date{\today}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}
\pagenumbering{gobble}

\begin{center}
\textbf{UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL}
\\
\textbf{PROGRAMA DE PÓS-GRADUAÇÃO em ECONOMIA}
\\
\textbf{Microeconometria -- 2015/3}

\vfill
\textbf{LISTA DE EXERCÍCIOS}

\vfill
\textbf{Autor: Paulo Ferreira Naibert } 
\\
\textbf{Professor: Hudson Torrent} 


\end{center}

\vfill

\begin{center}
\textbf{Porto Alegre \\ 25/06/2020 \\ Revisão: \today}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\pagenumbering{arabic}
\section{Panel Data and FGLS}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\textbf{Questão 1:}
Estabeleça o modelo de equações lineares, definindo a notação matricial para dados em painel.
Explique quais as hipóteses adequadas para a implementação do estimador \textbf{GLS} em um sistema de dados de painel.
Explique como implementear esse estimador na prática (\textbf{FGLS}).
Explique também como calular a matriz de covariância de $\mbs{\widehat{\beta}}_{FGLS}.$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo de equações lineares} 

\vspace{-1.5 em}
\begin{align*}
	\mbs{y}_{i} &= X_{i} \mbs{\beta} + \mbs{u}_{i},
\end{align*}

\noindent
com $i = 1, \dots, N$.
Cada $i$ tem $G=T$ equações temporais.
$G=T$ para dados em painel, onde $T$ é o número de equações temporais..


\vspace{-1.5 em}
\begin{align*}
	y_{it} &= \mbs{x}^{\prime}_{it} \mbs{\beta} + u_{it}
	\\
	 &= x_{i1} \beta_{1} + \dots + x_{iK} \beta_{K} + u_{it}
\end{align*}
com	$i = 1, \dots, N$ e	$t = 1, \dots, T$.

\vspace{1 em}
Em notação matricial para um painel:

\vspace{-1.5 em}
\begin{align*}
	X_{i} =
	\begin{pmatrix}
		x_{i1}, x_{i2}, \dots, x_{it}
	\end{pmatrix}
\end{align*}

Um exemplo de equação com intercepto mais 3 variáveis num intervalo de tempo com $T=5$:

\vspace{-1.5 em}
\begin{align*}
\begin{bmatrix}
y_{i 1} \\ y_{i 2} \\ y_{i 3} \\ y_{i 4} \\ y_{i 5}
\end{bmatrix}
=
\begin{bmatrix}
	1 & x_{1i1} & x_{2i1} & x_{3i1}	\\
	1 & x_{1i2} & x_{2i2} & x_{3i2} \\
	1 & x_{1i3} & x_{2i3} & x_{3i3} \\
	1 & x_{1i4} & x_{2i4} & x_{3i4} \\
	1 & x_{1i5} & x_{2i5} & x_{3i5}
\end{bmatrix}
\begin{bmatrix}
	\beta_{0} \\ \beta_{1} \\ \beta_{2} \\ \beta_{3}
\end{bmatrix}
+
\begin{bmatrix}
	u_{1} \\ u_{2} \\ u_{3} \\ u_{4} \\ u_{5}
\end{bmatrix}
\end{align*}

\noindent
para $i = 1, \dots, N$.
Podemos generalizar o modelo acima para $K$ variáveis e $T$ períodos de tempo.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Hipóteses}

Para implementarmos o estimador de \textbf{GLS} precisamos das seguintes hipótese:

\begin{enumerate}
\item %1

$E(X_{i} \otimes \mbs{u}_{i}) = 0$.

Para SGLS ser consistente, precisamos que $\mbs{u}_{i}$ não seja correlacionada com nenhum elemento de $X_{i}$.

\item %2

$\Omega$ é positiva definida (para ter inversa).
$E(X_{i}^{\prime} \Omega^{-1} X_{i})$ é \textbf{não} singular (para ter invesa).

Onde, $\Omega$ é a seguinte matriz \textbf{simétrica}, positiva-definida:

\vspace{-1.5 em}
\begin{align*}
\Omega = E(\mbs{u}_{i} \mbs{u}_{i}^{\prime}).
\end{align*}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação}

Agora, transformamos o sistema de equações ao realizarmos a pré-multiplicação do sistema por $\Omega^{-1/2}$:

\vspace{-1.5 em}
\begin{align*}
\Omega^{-1/2} \mbs{y}_{i} 
&=
\Omega^{-1/2} X_{i} \mbs{\beta}
+
\Omega^{-1/2} \mbs{u}_{i}
\\
\mbs{y}_{i}^{*}
&=
X_{i}^{*} \mbs{\beta}
+
\mbs{u}^{*}_{i}
\end{align*}

Estimando a equação acima por \textbf{SOLS}:

\vspace{-1.5 em}
\begin{align*}
\beta^{SOLS}
&=
\left( \sum_{i=1} X_{i}^{*'} X_{i}^{*} \right)^{-1}
\left( \sum_{i=1} X_{i}^{*'} \mbs{y}_{i}^{*} \right)
\\
&=
\left( \sum_{i=1} X_{i}^{'} \Omega^{-1/2} \Omega^{-1/2} X_{i} \right)^{-1}
\left( \sum_{i=1} X_{i}^{'} \Omega^{-1/2} \Omega^{-1/2} \mbs{y}_{i} \right)
\\
&=
\left( \sum_{i=1} X_{i}^{'} \Omega^{-1} X_{i} \right)^{-1}
\left( \sum_{i=1} X_{i}^{'} \Omega^{-1} \mbs{y}_{i} \right)
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{FSGLS: SGLS Factível}

Para obtermos $\beta^{SGLS}$ precisamos conhecer $\Omega$, o que não ocorre na prática.
Então, precisamos estimar $\Omega$ com um estimador consistente.
Para tanto usamos um procedimento de dois passos:

\begin{enumerate}
\item  % Passo 1
Estimar $\mbs{y}_{i} = X_{i} \mbs{\beta} + \mbs{u}_{i}$ via \textbf{SOLS} e guardar o resíduo estimado $\widehat{\mbs{u}_{i}}$.

\item  %Passo 2
Estimar $\Omega$ com o seguinte estimador $\widehat{\Omega}$:

\vspace{-1.5 em}
\begin{align*}
	\widehat{\Omega} 
	= 
	N^{-1} \sum_{i=1}^{N} \mbs{u}_{i} \mbs{u}_{i}'
\end{align*}
\end{enumerate}

Com a estimativa $\widehat{\Omega}$ feita, podemos obter $\beta^{FSGLS}$ pela fórmula do $\beta^{SGLS}$:

\vspace{-1.5 em}
\begin{align*}
	\beta^{FGLS}
	= 
	\left[ 
		\sum_{i} X_{i}' \widehat{\Omega}^{-1} X_{i}
	\right]^{-1}
	\left[ 
		\sum_{i} X_{i}' \widehat{\Omega}^{-1} \mbs{y}_{i}
	\right]
\end{align*}

Empilhando as $N$ observações:

\vspace{-1.5 em}
\begin{align*}
\beta^{FGLS}
= 
\left[ X \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) X \right]^{-1}
\left[ X' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) \mbs{y} \right]
\end{align*}

Reescrevendo a equação acima:

\vspace{-1.5 em}
\begin{align*}
\beta^{FGLS}
&= 
\left[  X \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) X \right]^{-1}
\left[  X' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) (X \beta + u) \right]
\\
&= 
\left[ X \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) X \right]^{-1}
\left\{ 
\left[ X' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) X \beta \right]
% \\
% &
\; +
\left[ X' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) u \right]
\right\}
\\
&= 
\beta +
\left[ X \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) X \right]^{-1}
\left[ X' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) u \right]
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Valor Esperado}

\vspace{-1 em}
\begin{align*}
E(\beta^{FGLS})
= 
\beta +
\left[ X \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) X \right]^{-1}
\left[ X' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) u \right]
\end{align*}

Concluímos que, se 
$\widehat{\Omega} \xrightarrow{\enskip p \enskip} \Omega$,
então,
$\beta^{FSGLS} \xrightarrow{\enskip p \enskip} \beta$,

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância}

\vspace{-1 em}
\begin{align*}
Var(\beta^{FGLS})
&= 
\left[ X \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) X \right]^{-1}
\left[ X' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) u \right]
\left\{ 
\left[ X \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) X \right]^{-1}
\left[ X' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) u \right]
\right\}^{\prime}
\\
&=
\left[ X \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) X \right]^{-1}
\left[
X' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) 
u u'
\left( I_{N} \otimes \widehat{\Omega}^{-1} \right) X
\right]
\left[ X \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) X \right]^{-1}
\end{align*}

Tirando o valor Esperado e supondo que:

\vspace{-1.5 em}
\begin{align*}
E(X_{i} \Omega^{-1} u_{i} u_{i}' X_{i}) = E(X_{i} \Omega^{-1})
\end{align*}
temos:

\vspace{-1.5 em}
\begin{align*}
E\left[ X' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right)
	u u'
\left( I_{N} \otimes \widehat{\Omega}^{-1} \right)' X \right]
=
E(X' \Omega^{-1} X)
\end{align*}
e temos:

\vspace{-1.5 em}
\begin{align*}
	Var(\beta^{FSGLS}) = \left[ E(X' \Omega^{-1} X \right]^{-1}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Endogeneity and GMM}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\textbf{Questão 2:}
Explique o problema de endogeneidade.
Ressalte quais características um bom instrumento deve possuir.
A partir da explicação, motive e estabeleça o estimador \textbf{GMM} para dados em painel.
Qual a variância assintótica desse estimador?
Qual a escolha ótima de $W$?
Indique quem é $W$ a fim de que o estimador de GMM coincida com o estimador de Variáveis Instrumentais.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

No seguinte modelo \textit{cross-section}:

\vspace{-1 em}
\begin{align} \label{mod1}
	y_{i} = \beta_{0} + \beta_{1} x_{1i} + \beta_{2} x_{2i} + \err_{i}
	\; ; \quad i = 1, \dots, N.
\end{align}

\noindent
A variável explicativa $x_{k}$ é dita \textbf{endógena} se ela for correlacionada com erro.
Se $x_{k}$ for não correlacionada com o erro, então $x_{k}$ é dita \textbf{exógena}.

Endogeneidade surge, normalmente, de três maneiras diferentes:

\begin{enumerate}\itemsep0pt
	\item Variável Omitida;
	\item Simultaneidade;
	\item Erro de Medida.
\end{enumerate}

No modelo \eqref{mod1} vamos supor:

\begin{itemize}\itemsep0pt
	\item $x_{1}$ é exógena.
	\item $x_{2}$ é endógena.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Hipóteses}

Assim, precisamos encontrar um instrumento $z_{i}$ para $x_{2}$, uma vez que queremos estimar $\beta_{0}$, $\beta_{1}$ e $\beta_{2}$ de maneira consistente.
Para $z_{i}$ ser um bom instrumento precisamos que $z$ tenha:

\begin{enumerate}\itemsep0pt
\item $Cov(z, \err) = 0$ $\implies$  $z$ é exógena em \eqref{mod1}.
\item $Cov(z, x_{2}) \neq 0$ $\implies$  correlação com $x_{2}$ após controlar para outras vaariáveis.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação}

Indo para o problema de dados de painel, temos:

\vspace{-1 em}
\begin{align} \label{mod2}
	\mbs{y}_{i} = X_{i} \mbs{\beta} + \mbs{u}_{i}
	\; ; \quad i = 1, \dots, N.
\end{align}

\noindent
onde 
$\mbs{y}_{i}$ é um vetor $T \times 1$,
$X_{i}$ é uma matriz $T \times K$,
$\mbs{\beta}$ é o vetor de coeficientes $K \times 1$,
$\mbs{u}_{i}$ é o vetor de erros $T \times 1$.

Se é verdade que há endogeneidade em \eqref{mod2}, então:

\vspace{-1 em}
\begin{align*}
	E(X_{i}^{\prime} \mbs{u}_{i}) \neq 0
\end{align*}

Definimos $Z_{i}$ como uma matriz $T \times L$ com $L \geq K$ de variáveis exógenas (incluindo o instrumento).
Queremos acabar com a endogeneidade, ou seja:

\vspace{-1 em}
\begin{align*}
	E(Z_{i}^{\prime} \mbs{u}_{i}) = 0
\end{align*}

Supondo $L = K$ (apenas substituímos a variável endógena por um instrumento).

\vspace{-1 em}
\begin{align*}
E[ Z_{i}^{\prime} ( \mbs{y}_{i} - X_{i} \mbs{\beta} ) ] &= 0
\\
E( Z_{i}^{\prime} \mbs{y}_{i} ) - E( Z_{i}^{\prime} X_{i} ) \mbs{\beta} &= 0
\\
E( Z_{i}^{\prime} \mbs{y}_{i} ) &= E( Z_{i}^{\prime} X_{i} ) \mbs{\beta}
\\
\Aboxed{
\mbs{\beta} &=
\left[ E( Z_{i}^{\prime} X_{i} ) \right]^{-1}
\left[ E( Z_{i}^{\prime} \mbs{y}_{i} ) \right]
}
\end{align*}

Se Usarmos estimadores amostrais:

\vspace{-1 em}
\begin{align*}
\mbs{\hat{\beta}} &=
\left[ N^{-1} \sum_{i=1}^{N} Z_{i}^{\prime} X_{i} \right]^{-1}
\left[ N^{-1} \sum_{i=1}^{N} Z_{i}^{\prime} \mbs{y}_{i} \right]
\\
\Aboxed{
\mbs{\hat{\beta}} &=
( Z^{\prime} X )^{-1} ( Z^{\prime} \mbs{y} ) }
\end{align*}

\vspace{1 em}
Se $L > K$, vamos considerar:

\vspace{-1 em}
\begin{align*}
\underset{\mbs{\beta}}{\text{Min}} \;
E( Z_{i}\mbs{u}_{i} )^2
\end{align*}
\noindent onde:

\vspace{-1 em}
\begin{align*}
E( Z_{i}\mbs{u}_{i} )^2 
&=
E[ ( Z_{i}\mbs{u}_{i} )' ( Z_{i}\mbs{u}_{i} ) ]
=
( Z' \mbs{y} - Z' X \mbs{\beta} )' ( Z' \mbs{y} - Z' X \mbs{\beta} )
\\
&=
\mbs{y}' ZZ' \mbs{y}
-
\mbs{y}' ZZ' X \mbs{\beta}
-
\mbs{\beta}' X' ZZ' \mbs{y}
+
\mbs{\beta}' X' ZZ' X \mbs{\beta}
\end{align*}

Derivando em relação em $\mbs{\beta}$ e igualando a zero:

\vspace{-1 em}
\begin{align*}
-2 \mbs{y}' ZZ' X + 2 \mbs{\beta}'X' ZZ' X &= 0
\\
\mbs{\beta}'X' ZZ' X &= \mbs{y}' ZZ' X 
\\
\mbs{\beta}' &= ( \mbs{y}' ZZ' X ) ( X' ZZ' X )^{-1}
\\
\Aboxed{
\mbs{\beta} &= ( X' ZZ' X )^{-1} ( X' ZZ' \mbs{y} ) }
\end{align*}

Um estimador mais eficiente pode ser encontrado fazendo:

\vspace{-1 em}
\begin{align*}
\underset{\mbs{\beta}}{\text{Min}} \;
E[ ( Z_{i}' \mbs{y} - Z' X \mbs{\beta} )' W ( Z_{i}' \mbs{y} - Z' X \mbs{\beta} ) ].
\end{align*}

\noindent
Escolhendo $\widehat{W}$, a priori, temos:

\vspace{-1 em}
\begin{align*}
\underset{\mbs{\beta}}{\text{Min}} \;
\left\{ 
\mbs{y}' Z \widehat{W} Z' \mbs{y}
-
\mbs{y}' Z \widehat{W} Z' X \mbs{\beta}
-
\mbs{\beta}' X'  Z \widehat{W} Z' \mbs{y}
+
\mbs{\beta}' X'  Z \widehat{W} Z' X \mbs{\beta}
\right\}
\end{align*}

Derivando em relação em $\mbs{\beta}$ e igualando a zero:

\vspace{-1 em}
\begin{align*}
-2 \mbs{y}' Z \widehat{W} Z' X + 2 \mbs{\beta}'X' Z \widehat{W} Z' X &= 0
\\
\mbs{\beta}'X' Z \widehat{W} Z' X &= \mbs{y}' Z \widehat{W} Z' X 
\\
\mbs{\beta}' &= ( \mbs{y}' Z \widehat{W} Z' X ) ( X' Z \widehat{W} Z' X )^{-1}
\\
\Aboxed{
\mbs{\beta}^{GMM} &= ( X' Z \widehat{W}' Z' X )^{-1} ( X' Z \widehat{W}' Z' \mbs{y} ) }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Valor Esperado} 

\vspace{-1 em}
\begin{align*}
\Aboxed{
E( \mbs{\beta}^{GMM} ) &=
\mbs{\beta} +
E[ ( X' Z \widehat{W}' Z' X )^{-1} ( X' Z \widehat{W}' Z' \mbs{u} ) ] }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância} 

\vspace{-1 em}
\begin{align*}
Var( \mbs{\beta}^{GMM} ) &=
E \left\{ 
\left[ ( X' Z \widehat{W}' Z' X )^{-1} ( X' Z \widehat{W}' Z' \mbs{u} ) \right]
\left[ ( X' Z \widehat{W}' Z' X )^{-1} ( X' Z \widehat{W}' Z' \mbs{u} ) \right]'
\right\}
\\ &=
E \left\{ 
( X' Z \widehat{W}' Z' X )^{-1}
X' Z \widehat{W}' Z' \mbs{u} \mbs{u}' Z \widehat{W} Z' X 
( X' Z \widehat{W} Z' X )^{-1}
\right\}.
\end{align*}

\noindent
Definindo $\Delta = E(Z' \mbs{u}\mbs{u}' Z)$ com $\Delta = W^{-1}$:

\vspace{-1 em}
\begin{align*}
Var( \mbs{\beta}^{GMM} ) &=
E \left\{ 
( X' Z \widehat{W}' Z' X )^{-1}
X' Z \widehat{W}' W^{-1} \widehat{W} Z' X 
( X' Z \widehat{W} Z' X )^{-1}
\right\}
\\ &=
E \left\{ 
( X' Z \widehat{W}' Z' X )^{-1}
( X' Z \widehat{W}' Z' X )
( X' Z \widehat{W} Z' X )^{-1}
\right\}.
\\
\Aboxed{
Var( \mbs{\beta}^{GMM} ) &=
E \left[
( X' Z \widehat{W} Z' X )^{-1}
\right] }.
\end{align*}

\noindent
Se tivéssemos definido $W = (Z'Z)^{-1}$, teríamos $\beta^{2SLS}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Random Effects (RE, EA)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\textbf{Questão 3:}
Usando o problema de variável omitida como motivação (heterogeneidade não observada), explique o modelo de \textbf{Efeitos Aleatórios} para dados em painel.
Explicite as hipóteses necessárias e indique o estimador apropriado para esse modelo, enfatizando as característica do estimador GLS.
Como podemos fazer inferência nesse caso?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

O modelo linear de \textbf{efeitos não observados}:

\vspace{-1 em}
\begin{align} \label{mod1:EA}
	y_{it} = \mbs{x}_{it} \mbs{\beta} + c_{i} + u_{it},
\end{align}

\noindent
onde
$t = 1, \dots, T$ e $i = 1, \dots, N$.

O modelo contém explicitamente um componente não observado que não varia no tempo $c_{i}$.
Abordamos esse componente como parte do erro, não como parâmetro a ser estimado.
Para a análise de \textbf{Efeitos Aleatórios, (EA) ou (RE)}, supomos que os regressões $\mbs{x}_{it}$ são \textbf{não correlacionados} com $c_{i}$, mas fazemos hipóteses mais restritas que o \textbf{POLS}; pois assim exploramos a presença de \textbf{correlação serial} do erro composto por GLS e garantimos a consitência do estimador de FGLS.

Podemos reescrever \eqref{mod1:EA} como:

\vspace{-1 em}
\begin{align} \label{mod2:EA}
	y_{it} = \mbs{x}_{it} \mbs{\beta} + v_{it},
\end{align}

\noindent
onde
$t = 1, \dots, T$, $i = 1, \dots, N$ e $\boxed{v_{it} = c_{i} + u_{it}}$ é o erro composto.

Agora, vamos empilhar os $t$'s e reescrever \eqref{mod2:EA} como:

\vspace{-1 em}
\begin{align} \label{mod3:EA}
	\mbs{y}_{i} = X_{i} \mbs{\beta} + \mbs{v}_{i},
\end{align}

\noindent
onde
$i = 1, \dots, N$ e $\boxed{\mbs{v}_{i} = c_{i} \mbs{1}_{T} + \mbs{u}_{i}}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Hipóteses de $\mbs{\widehat{\beta}}^{RE}$}

As Hipóteses que usamos para $\mbs{\widehat{\beta}}^{RE}$ são:

\begin{enumerate} \itemsep0pt
\item  
Usamos o modelo correto e $c_{i}$ não é endógeno.

\begin{enumerate}[label =\alph*)]
\item 
	$E( u_{it} \, | \,  x_{i1}, \dots, x_{iT}, c_{i} ) = 0$,
	$i = 1, \dots, N$.
\item        
	$E( c_{it} \, | \, x_{i1}, \dots, x_{iT} ) = E( c_{i} ) = 0$,
	$i = 1, \dots, N$.
\end{enumerate}

\item  Posto completo de $E( X_{i}' \Omega^{-1} X_{i} )$.

Definindo a matriz $T \times T$, $\boxed{\Omega \equiv E(\mbs{v}_{i} \mbs{v}_{i}')}$, queremos que $E( X_{i} \Omega^{-1} X_{i} )$ tenha posto completo (posto = $K$).
\end{enumerate}

A matriz $\Omega$ é simétrica $\Omega' = \Omega$ e positiva definida $\det(\Omega) > 0$.
Assim podemos achar $\Omega^{1/2}$ e $\Omega^{-1/2}$ com $\Omega = \Omega^{1/2} \Omega^{1/2}$ e $\Omega^{-1} = \Omega^{-1/2} \Omega^{-1/2}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação}

Premultiplicando \eqref{mod3:EA} port $\Omega^{-1/2}$ do dois lados, temos:

\vspace{-1 em}
\begin{align} 
\notag
\Omega^{-1/2}\mbs{y}_{i} &= \Omega^{-1/2}X_{i} \mbs{\beta} + \Omega^{-1/2}\mbs{v}_{i}
\\
\label{mod4:EA}
\mbs{y}_{i}^{*} &= X_{i}^{*} \mbs{\beta} + \mbs{v}_{i}^{*},
\end{align}

Estimando o modelo acima por POLS:

\vspace{-1 em}
\begin{align} 
\notag
\mbs{\beta}^{POLS} &= 
\left( \sum_{i=1}^{N} X_{i}^{*}' X_{i}^{*} \right)^{-1}
\left( \sum_{i=1}^{N} X_{i}^{*}' \mbs{y}_{i}^{*} \right)
\\ \notag
&=
\left( \sum_{i=1}^{N} X_{i}' \Omega^{-1} X_{i} \right)^{-1}
\left( \sum_{i=1}^{N} X_{i}' \Omega^{-1} \mbs{y}_{i} \right)
\\ \label{beta:RE:1}
&=
\left( X' (I_{N} \otimes \Omega^{-1}) X \right)^{-1}
\left( X' (I_{N} \otimes \Omega^{-1}) \mbs{y} \right).
\end{align}

O problema, agora, é estimar $\Omega$.
Supondo:
\begin{itemize}\itemsep0pt
\item $E(u_{it}u_{it}) = \sigma_{u}^{2}$;
\item $E(u_{it}u_{is}) = 0$.
\end{itemize}
Como $\Omega = E(\mbs{v}_{i} \mbs{v}_{i}') = E[ ( c_{i} \mbs{1}_{T} + \mbs{u}_{i} ) ( c_{i} \mbs{1}_{T} + \mbs{u}_{i} )' ]$, temos que:

\vspace{-1 em}
\begin{align*} 
E(v_{it}v_{it}) &=
	E( c_{i}^{2} + 2c_{i} u_{it} + u_{it}^{2}) 
	=
	\sigma_{c}^{2} + \sigma_{u}^{2}
\\
E(v_{it}v_{is})	&=
	E[ ( c_{i} + u_{it} ) ( c_{i} + u_{is} ) ]
	=
	E( c_{i}^{2} + c_{i} u_{is} + u_{it} c_{i} + u_{it} u_{is} )
	=
	\sigma_{c}^{2}.
\end{align*}

Assim, 

\vspace{-1 em}
\begin{align*}
\Omega 
= 
E(\mbs{v}_{i} \mbs{v}_{i}') = \sigma^{2}_{u} I_{T} + \sigma_{c}^{2} \mbs{1}_{T} \mbs{1}_{T}'
\end{align*}

\noindent
onde
$\sigma^{2}_{u} I_{T}$ 
é uma matriz diagonal, e 
$\sigma_{c}^{2} \mbs{1}_{T} \mbs{1}_{T}'$ é uma matriz com todos os elementos iguais a $\sigma_{c}^{2}$.

Agora, rodando POLS em \eqref{mod3:EA} e guardando os resíduos, temos:

\vspace{-1 em}
\begin{align*}
\hat{v}_{it}^{POLS}
= 
\hat{y}_{it}^{POLS} - \mbs{x}_{it} \mbs{\hat{\beta}}^{POLS}
\end{align*}

\noindent
e conseguimos estimar $\sigma_{v}^{2}$ e $\sigma_{c}^{2}$ por estimadores amostrais:

\begin{itemize}\itemsep0pt
\item 
como $\sigma_{v}^{2} = E(v_{it}^{2})$:

\vspace{-1.5 em}
\begin{align*}
\hat{\sigma}_{v}^{2} =
(NT - K)^{-1} 
\sum_{i=1}^{N}
\sum_{t=1}^{T}
\hat{v}_{it}^2
\end{align*}
\vspace{-1.5 em}

\item 
como $\sigma_{c}^{2} = E(v_{it} v_{is})$:

\vspace{-1.5 em}
\begin{align*}
\hat{\sigma}_{c}^{2} =
\left[ N \frac{T ( T-1 )}{2} - K  \right]^{-1}
\sum_{i=1}^{N}
\sum_{t=1}^{T-1}
\sum_{s=t+1}^{T}
\hat{v}_{it} \hat{v}_{is}
\end{align*}
\vspace{-1.5 em}

\item $N$ indivíduos;

\item $T$ elementos da diagonal principal de $\Omega$

\item $\frac{T ( T - 1)}{2}$ elementos da matriz triangular superior dos elementos fora da diagonal.

\item $K$ regressores.
\end{itemize}

Agora que temos $\hat{\sigma}^2_{v}$ e $\hat{\sigma}^2_{c}$ podemos achar $\hat{\sigma}^{2}_{u}$ pela equação $\boxed{\hat{\sigma}_{u}^{2} = \hat{\sigma}_{v}^{2} - \hat{\sigma}_{c}^{2}}$.
Dessa forma, achamos os $T^2$ elementos de $\widehat{\Omega}$, e podemos escrever:

\vspace{-1 em}
\begin{align*}
\widehat{\Omega}
= 
\hat{\sigma}^{2}_{u} I_{T} + \hat{\sigma}_{c}^{2} \mbs{1}_{T} \mbs{1}_{T}'
\end{align*}

Com $\widehat{\Omega}$ estimado, reescrevemos \eqref{beta:RE:1} como:

\vspace{-1 em}
\begin{align} \label{beta:RE:2}
\mbs{\beta}^{RE} = 
\left[ X' (I_{N} \otimes \widehat{\Omega}^{-1}) X \right]^{-1}
\left[ X' (I_{N} \otimes \widehat{\Omega}^{-1}) \mbs{y} \right].
\end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Valor Esperado}

\vspace{-1 em}
\begin{align*}
	\Aboxed{
E( \mbs{\beta}^{RE} ) = 
\mbs{\beta} +
\left[ X' (I_{N} \otimes \widehat{\Omega}^{-1}) X \right]^{-1}
\left[ X' (I_{N} \otimes \widehat{\Omega}^{-1}) \mbs{v} \right] }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância}

\vspace{-1 em}
\begin{align*} 
Var( \mbs{\beta}^{RE} ) = 
E
\left\{ 
\left[ X' ( I_{N} \otimes \widehat{\Omega}^{-1} ) X \right]^{-1}
\left[
X' ( I_{N} \otimes \widehat{\Omega}^{-1} )
\mbs{v} \mbs{v}'
( I_{N} \otimes \widehat{\Omega}^{-1} )' X
\right]
\left[ X' ( I_{N} \otimes \widehat{\Omega}^{-1} ) X \right]
\right\},
\end{align*}

\noindent
como $E( \mbs{v}_{i} \mbs{v}_{i}' ) =\Omega$,

\vspace{-1 em}
\begin{align*} 
	\Aboxed{
Var( \mbs{\beta}^{RE} ) = 
E
\left[ X' ( I_{N} \otimes \widehat{\Omega}^{-1} ) X \right] }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Fixed Effects (EF, FE)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\textbf{Questão 4:}
Usando o problema de variável omitida como motivação (heterogeneidade não observada), explique o modelo de \textbf{Efeitos Fixos} para dados em painel.
Explicite as hipóteses necessárias e indique o estimador apropriado para esse modelo.
Como podemos fazer inferência nesse caso?
Como podemos fazer inferência robusta nesse caso?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

O modelo linear de \textbf{efeitos não observados}:

\vspace{-1 em}
\begin{align} \label{mod1:FE}
	y_{it} = \mbs{x}_{it} \mbs{\beta} + c_{i} + u_{it},
\end{align}

\noindent
onde
$t = 1, \dots, T$ e $i = 1, \dots, N$.

O modelo contém explicitamente um componente não observado que não varia no tempo $c_{i}$.
Abordamos esse componente como parte do erro, não como parâmetro a não observado.
No caso da análise de \textbf{Efeitos Fixos (EF, FE)}, permitimos que esse componente $c_{i}$ seja correlacionado com $\mbs{x}_{it}$.
Assim, se decidíssemos estimar o modelo \eqref{mod1:FE} por POLS, ignorando $c_{i}$, teríamos problemas de inconsistência devido a \textbf{endogeneidade}.

As $T$ equações do modelo \eqref{mod1:FE} podem ser reescritas como:

\vspace{-1 em}
\begin{align} \label{mod2:FE}
	\mbs{y}_{i} = X_{i} \mbs{\beta} + c_{1} \mbs{1}_{T} + \mbs{u}_{i},
\end{align}

\noindent
com
$\mbs{v}_{i} = c_{i} \mbs{1}_{T} + \mbs{u}_{i}$ sendo os erros compostos.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Matriz $M^{0}$}

Definimos a matriz $M^{0}$ como:

\vspace{-1 em}
\begin{align*}
	M^{0} &=
	I_{T} - T^{-1} \mbs{1}_{T} \mbs{1}_{T}'
	=
	I_{T} - \mbs{1}_{T} (\mbs{1}_{T}' \mbs{1}_{T})^{-1} \mbs{1}_{T}'.
\end{align*}

\noindent
A matriz $M^{0}$ é idempotente e simétrica.

\begin{align*}
	M^{0} \mbs{x} &= \mbs{x} - \overline{\mbs{x}} \mbs{1}_{T}
	= \ddot{\mbs{x}}.
\end{align*}

Podemos transformar o modelo \eqref{mod2:FE} ao premultiplicarmos todo o modelo por $M^{0}$.

\vspace{-1 em}
\begin{align*} 
M^{0} \mbs{y}_{i} &= M^{0} X_{i} \mbs{\beta} + M^{0} ( c_{1} \mbs{1}_{T} ) + M^{0} \mbs{u}_{i},
\quad i = 1, \dots, N.
\end{align*}

\vspace{-1 em}
\begin{align*} 
M^{0} ( c_{1} \mbs{1}_{T} ) = 
( I_{T} - T^{-1} \mbs{1}_{T} \mbs{1}_{T}' ) c_{i} \mbs{1}_{T} 
=
c_{i} \mbs{1}_{T} - T^{-1} c_{i} \mbs{1}_{T} \mbs{1}_{T}' \mbs{1}_{T} 
=
c_{i} \mbs{1}_{T} - c_{i} \mbs{1}_{T} 
\implies
\boxed{ M^{0} ( c_{1} \mbs{1}_{T} ) = 0 }
\end{align*}

\vspace{-1 em}
\begin{align} \label{mod2:FE}
\ddot{\mbs{y}}_{i} &= \ddot{X}_{i} \mbs{\beta} + \ddot{\mbs{u}_{i}},
\quad i = 1, \dots, N.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação POLS}

Aplicando POLS no modelo \eqref{mod2:FE}

\vspace{-1 em}
\begin{align} \label{beta:pols:FE}
\Aboxed{
\mbs{\beta}^{FE} =
\left[ \sum_{i=1}^{N} \ddot{X}_{i}' \ddot{X}_{i} \right]^{-1}
\left[ \sum_{i=1}^{N} \ddot{X}_{i}' \ddot{\mbs{y}}_{i} \right]
}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Hipóteses}

As Hipóteses que usamos para $\mbs{\widehat{\beta}}^{FE}$ são:

\begin{description}
\setlength\itemsep{.5 em}

\item [FE.1:] Exogeneidade Estrita:
$E( u _{it} \, | \, \mbs{x}_{i1}, \dots, \mbs{x}_{iT}, c_{i}) = 0$, para $t=1, \dots, T$ e $i = 1, \dots, N$.

\item  [FE.2:] Posto completo de $E( X_{i}' \Omega^{-1} X_{i} )$ (para inverter a matriz).
$posto[ E( X_{i}' \Omega^{-1} X_{i} ) ]  = K$.

\item [FE.3:] Homoscedasticidade:
	$E(\mbs{u}_{i} \mbs{u}_{i}' \,|\, X_{i}, c_{i}) = \sigma_{u}^{2} I_{T}$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Valor Esperado}
Usando \textbf{FE.1} e \textbf{FE.2}, apenas.

\vspace{-1 em}
\begin{align*}
E( \mbs{\beta}^{FE} ) &=
\mbs{\beta} +
E\left[
\left( \sum_{i=1}^{N} \ddot{X}_{i}' \ddot{X}_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \ddot{X}_{i}' \ddot{\mbs{u}}_{i} \right)
\right]
\\
\Aboxed{
E( \mbs{\beta}^{FE} ) &=
\mbs{\beta} +
E \left[ ( \ddot{X}' \ddot{X} )^{-1} (\ddot{X}' \ddot{\mbs{u}}) \right]
}
\end{align*}

\noindent
Sabendo que 
$\ddot{X} = ( I_{N} \otimes M^{0} ) X$
e
$\ddot{\mbs{u}} = ( I_{N} \otimes M^{0} ) \mbs{u}$,
definimos:

\vspace{-1 em}
\begin{align*}
E( \mbs{\beta}^{FE} ) &=
\mbs{\beta} +
E \left\{
\left[  
X' ( I_{N} \otimes M^{0} )( I_{N} \otimes M^{0} ) X 
\right]^{-1}
\left[ 
X' ( I_{N} \otimes M^{0} )( I_{N} \otimes M^{0} ) \mbs{u}
\right]
\right\}
\\
\Aboxed{
E( \mbs{\beta}^{FE} ) &=
\mbs{\beta} +
E \left\{
\left[  
X' ( I_{N} \otimes M^{0} ) X 
\right]^{-1}
\left[ 
X' ( I_{N} \otimes M^{0} ) \mbs{u}
\right]
\right\} }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância}

Usamos a variância do estimador para inferência.
Usando \textbf{FE.1} e \textbf{FE.2}, apenas:

\vspace{-1 em}
\begin{align*} 
	\Aboxed{
Var( \mbs{\beta}^{FE} ) = 
E \left[
( \ddot{X}' \ddot{X} )^{-1}
(\ddot{X}' \ddot{\mbs{u}}) (\ddot{\mbs{u}}' \ddot{X} )
( \ddot{X}' \ddot{X} )^{-1} 
\right]}
\end{align*}


\begin{description}
\item [Pão:]
\begin{align*}
E\left[ ( \ddot{X}' \ddot{X} )^{-1} \right] &=
E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
\\ &=
E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
\end{align*}

\item [Recheio:]
\begin{align*}
E\left[
(\ddot{X}' \ddot{\mbs{u}}) (\ddot{\mbs{u}}' \ddot{X} ) 
\right] 
&=
E \left[
X' ( I_{N} \otimes M^{0} ) ( I_{N} \otimes M^{0} ) 
\mbs{u} \mbs{u}'
( I_{N} \otimes M^{0} ) ( I_{N} \otimes M^{0} ) X
\right]
\\ &=
E \left[
X' ( I_{N} \otimes M^{0} ) \mbs{u} \mbs{u}' ( I_{N} \otimes M^{0} ) X
\right]
\end{align*}
\end{description}

\vspace{-1 em}
\begin{align*} 
Var( \mbs{\beta}^{FE} ) &= \text{Pão Recheio Pão}
\\ 
\Aboxed{
Var( \mbs{\beta}^{FE} ) &= 
E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
E \left[
X' ( I_{N} \otimes M^{0} ) \mbs{u} \mbs{u}' ( I_{N} \otimes M^{0} ) X
\right]
E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância sob Homocedasticidade}

Usando \textbf{FE.3}, temos

\begin{description}
\item [Recheio':]
\begin{align*}
E \left[ X' ( I_{N} \otimes M^{0} ) \right]
\sigma^2_{u} I_{NT}
E \left[ ( I_{N} \otimes M^{0} ) X \right]
=
\sigma^2_{u}
E \left[ X' ( I_{N} \otimes M^{0} ) X \right]
\end{align*}
\end{description}

\noindent
\red{ $( I_{N} \otimes M^{0} )$ é uma matrix de dimensão $NT \times NT$, visto que $I_{N}$ é $N\times N$ e $M^{0}$ é $T \times T$.}

\vspace{-2 em}
\begin{align*}
Var( \mbs{\beta}^{FE} ) &= \text{Pão Recheio' Pão} 
\\ &=
E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
\sigma_{u}^{2} E\left[ X' ( I_{N} \otimes M^{0} ) X \right]
E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
\\  &=
E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
\sigma_{u}^{2} I_{NT}
\\
\Aboxed{ Var( \mbs{\beta}^{FE} ) &= \sigma_{u}^{2} \cdot  E\left[ X' ( I_{N} \otimes M^{0} ) X \right] }
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{First Difference (FD, PD)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\textbf{Questão 5:}
Usando o problema de variável omitida como motivação (heterogeneidade não observada), explique o modelo de \textbf{Primeira Diferença} para dados em painel.
Explicite as hipóteses necessárias e indique o estimador apropriado para esse modelo.
Como podemos fazer inferência nesse caso?
Como podemos fazer inferência robusta nesse caso?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

O modelo linear de \textbf{efeitos não observados}:

\vspace{-1 em}
\begin{align} \label{mod1:FD}
	y_{it} = \mbs{x}_{it} \mbs{\beta} + c_{i} + u_{it},
\end{align}

\noindent
para
$t = 1, \dots, T$ e $i = 1, \dots, N$.

O modelo contém explicitamente um componente não observado, $c_{i}$, que não varia no tempo.
Tratamos o componente não observado como parte do erro, não como parâmetro a ser estimado.
Aqui permitimos que $c_{i}$ seja correlacionado com $\mbs{x}_{it}$.
Deste modo, \textbf{não} podemos ignorar a sua presença e estimar \eqref{mod1:FD} por POLS, visto que isso resultaria num estimador inconsistente devido a \textbf{endogeneidade}.

Assim, transformamos o modelo para eliminar $c_{i}$ e conseguirmos fazer uma estimação consistente de $\mbs{\beta}$.
A trasnformação a ser feita é a primeira diferença.
Para tanto, seguimos os seguintes passos:

\begin{itemize}\itemsep0pt
\item Reescrevemos \eqref{mod1:FD} defasado:

\vspace{-1 em}
\begin{align}  \label{mod2:FD}
	y_{it-1} = \mbs{x}_{it-1} \mbs{\beta} + c_{i} + u_{it-1}
\end{align}

\item Tiramos a diferença entre \eqref{mod2:FD} e \eqref{mod1:FD}:

\vspace{-1 em}
\begin{align}
\nonumber
y_{it} - y_{it-1} &=
(\mbs{x}_{it} - \mbs{x}_{it-1}) \mbs{\beta} +
c_{i} - c_{i} +
u_{it} - u_{it-1}
\\
\label{mod3:FD}
\Delta y_{it} &=
\Delta \mbs{x}_{it} \mbs{\beta} +
\Delta u_{it}. 
\end{align}

\noindent
para
$t = 2, \dots, T$ e $i = 1, \dots, N$.
\end{itemize}

Reescrevendo \eqref{mod3:FD} no formato matricial empilhando $T$:

\vspace{-1 em}
\begin{align} \label{mod4:FD}
	\Delta \mbs{y}_{i} = \Delta X_{i} \mbs{\beta} + \mbs{e}_{i}
\end{align}

\noindent
com 
$\boxed{e_{it} = \Delta u_{it}}$.

\begin{itemize}\itemsep0pt
\item
$\Delta \mbs{y}_{i}$ vetor $( T - 1 ) \times 1$ 
\item
$\Delta X_{i}$  matriz  $( T - 1 ) \times K$
\item
$\mbs{\beta}$ vetor $K \times 1$
\item
$\mbs{e}_{i}$ vetor $(T - 1 ) \times 1$
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação POLS}

O estimador $\widehat{\mbs{\beta}}^{FD}$ é o POLS da regressão no modelo \eqref{mod4:FD}, assim:

\vspace{-1 em}
\begin{align} \label{beta:pols:FD}
\Aboxed{
\mbs{\beta}^{FD} =
\left[ \sum_{i=1}^{N} \Delta X_{i}' \Delta X_{i} \right]^{-1}
\left[ \sum_{i=1}^{N} \Delta X_{i}' \Delta \mbs{y}_{i} \right]
}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Hipóteses}

As Hipóteses que usamos para $\mbs{\widehat{\beta}}^{FD}$ são:

\begin{description}
\setlength\itemsep{.5 em}

\item [FD.1:] Exogeneidade Estrita:
$E( u _{it} \, | \, \mbs{x}_{i1}, \dots, \mbs{x}_{iT}, c_{i}) = 0$, para $t=1, \dots, T$ e $i = 1, \dots, N$.

\item  [FD.2:] Posto completo de $E( \Delta X_{i}' \Delta X_{i} )$ (para inverter a matriz).
$posto[ E( \Delta X_{i} ' \Delta X_{i} ) ]  = K$.

\item [FD.3:] Homoscedasticidade:
	$E(\mbs{e}_{i} \mbs{e}_{i}' \,|\, X_{i}, c_{i}) = \sigma_{e}^{2} I_{T-1}$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Valor Esperado}
Usando apenas \textbf{FD.1} e \textbf{FD.2}:

\vspace{-1 em}
\begin{align*}
E( \mbs{\beta}^{FD} ) &=
\mbs{\beta} +
E\left[
\left( \sum_{i=1}^{N} \Delta X_{i}' \Delta X_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \Delta X_{i}' \mbs{e}_{i} \right)
\right]
\\
\Aboxed{
E( \mbs{\beta}^{FD} ) &=
\mbs{\beta} +
E \left[ ( \Delta X' \Delta X )^{-1} ( \Delta X' \mbs{e} \right]
}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância}

Usando apenas \textbf{FD.1} e \textbf{FD.2}:

\vspace{-1 em}
\begin{align*} 
\Aboxed{
Var( \mbs{\beta}^{FD} ) = 
E \left[
( \Delta X' \Delta X )^{-1}
( \Delta X' \mbs{e}  \mbs{e}' \Delta X )
( \Delta X' \Delta X )^{-1} 
\right]}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância sob Homocedasticidade}

Usando \textbf{FD.3}, temos

\vspace{-1 em}
\begin{align*} 
Var( \mbs{\beta}^{FD} ) &= 
\sigma_{e}^{2}
E \left[
( \Delta X' \Delta X )^{-1}
( \Delta X' \Delta X )
( \Delta X' \Delta X )^{-1} 
\right]
\\
\Aboxed{
Var( \mbs{\beta}^{FD} ) &= 
\sigma^2_{e}
E \left[
( \Delta X' \Delta X )^{-1} 
\right]}
\end{align*}

\noindent 
com

\vspace{-1 em}
\begin{align*} 
\sigma^2_{e} = 
\left[ N ( T - 1 ) - K \right]^{-1}
\left[  
\sum_{i=1}^{N} 
\sum_{t=1}^{T}
\hat{e}_{it}^{2}
\right],
\end{align*}

\noindent
que é a média de todos $\hat{e}^{2}_{it}$ contando $K$ regressores.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Exogeneidade Estrita e FDIV}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\textbf{Questão 6:}
Explique a hipótese de exogeneidade estrita dos regressores.
Em seguida, argumente mostrando que a hipótese de exogeneidade estrita não se sustenta no seguinte modelo:

\vspace{-1.5 em}
\begin{align*}
y_{it} 
=
\mbs{z}_{it} \mbs{\gamma}
+
\rho y_{i t - 1}
+ 
c_{i} + u_{i t}.
\end{align*}
\vspace{-1.5 em}

Explique detalhadamente como esse modelo pode ser estimado a partir da combinação entre Variáveis Instrumentais e método da Primeira Diferença.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

No seguinte modelo

\vspace{-1 em}
\begin{align*} 
	y_{it} = \mbs{x}_{it} \mbs{\beta} + u_{it},
\end{align*}

\noindent
para
$t = 1, \dots, T$ e $i = 1, \dots, N$.

\begin{itemize}\itemsep0pt
\item
$y_{it}$ escalar;

\item
$\mbs{x}_{it}$  vetor $1 \times K$;

\item
$\mbs{\beta}$ vetor $K \times 1$;

\item
$u_{it}$ escalar.
\end{itemize}

\noindent
$\{x_{it}\}$ é estritamente \textbf{exógeno} se valer:

\vspace{-1 em}
\begin{align*}
	E( u_{it} \, | \, \mbs{x}_{i1}, \dots, \mbs{x}_{iT} ) = 0 \; , \qquad t = 1, \dots, T
\end{align*}

\noindent
ou seja:

\vspace{-1 em}
\begin{align*}
E( y_{it} \, | \, \mbs{x}_{i1}, \dots, \mbs{x}_{iT} ) = \mbs{x}_{it} \mbs{\beta} 
\; , \qquad t = 1, \dots, T
\end{align*}

\noindent
o que é equivalente a hipótese de que utilizamos o modelo linear correto.

Para o seguinte modelo:

\vspace{-1.5 em}
\begin{align*}
y_{it} = \mbs{z}_{it} \mbs{\gamma} + \rho y_{i t - 1} + c_{i} + u_{it}.
\; , \qquad t = 2, \dots, T
\end{align*}

\noindent
é \textbf{impossível} termos exogeneidade estrita.
Isso porque, nesse modelo, de efeitos não observados temos:

\vspace{-1.5 em}
\begin{align*}
	E( y_{it} \, | \, \mbs{z}_{i1}, \dots, \mbs{z}_{iT}, y_{it-1}, c_{i}) \neq 0.
\end{align*}

\noindent
Isso ocorre porque, $y_{it}$ é afetado por $y_{it-1}$ que contribui para $y_{it}$ com, pelo menos, $\rho c_{i}$.

\begin{equation*}
\left.
\begin{aligned}
y_{it} &= \mbs{z}_{it} \mbs{\gamma} + \rho y_{i t - 1} + c_{i} + u_{it}
\\
y_{it-1} &= \mbs{z}_{it-1} \mbs{\gamma} + \rho y_{i t - 2} + c_{i} + u_{it-1}
\end{aligned}
\right\} 
\implies
y_{it} = \mbs{z}_{it} \mbs{\gamma} +
\rho (\mbs{z}_{it-1} \mbs{\gamma} + \rho y_{i t - 2} + c_{i} + u_{it-1})
+ c_{i} + u_{it}.
\end{equation*}

Para eliminarmos este efeito, podemos tirar a primeira diferença do modelo:

\vspace{-1 em}
\begin{align}
\nonumber
y_{it} - y_{it-1} &= 
(\mbs{z}_{it} - \mbs{z}_{it-1}) \mbs{\gamma} +
\rho (y_{i t - 1} -  y_{i t - 2} ) +
(c_{i} - c_{i}) + (u_{it} - u_{it-1})
\\
\label{mod1:FDIV}
\Aboxed{
\Delta y_{it} &= 
\Delta \mbs{z}_{it} \mbs{\gamma} + \rho \Delta y_{i t - 1} + \Delta u_{it}
\, , \qquad t=3, \dots, T}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação}

Não podemos estimar o modelo \eqref{mod1:FDIV} por POLS, uma vez que $Cov(\Delta y_{it-1}, \Delta u_{it} ) \neq 0$.
Como saída, podemos estimar por P2SLS, usando instrumentos para $\Delta y_{it-1}$ (alguns intrumentos para $\Delta y_{it-1}$ são $y_{it-2}, y_{it-3}, \dots, y_{i1}$).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{P2SLS}

\vspace{-1 em}
\begin{align*}
	y_{it} = \mbs{x}_{it}' \mbs{\beta} + u_{it}
\end{align*}

\begin{itemize}\itemsep0pt
\item $i = 1, \dots, N$
\item $t = 1, \dots, T$
\item $y_{it}$ escalar;
\item $\mbs{x}_{it}$  vetor $K \times 1$;
\item $\mbs{\beta}$ vetor $K \times 1$;
\item $u_{it}$ escalar.
\end{itemize}

\vspace{-1 em}
\begin{align*}
\boxed{
\mbs{\beta}^{P2SLS} =  ( X' P_{Z} X )^{-1} ( X' P_{Z} \mbs{y} ) }
\end{align*}

\noindent
com

\vspace{-1 em}
\begin{align*}
\boxed{P_{Z} = Z'(Z'Z)^{-1}Z }
\end{align*}

\noindent
onde
$P_{Z}$ é a matriz de projeção em $Z$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{FDIV}

\vspace{-1 em}
\begin{align*}
y_{it} &= \mbs{x}_{it}' \mbs{\beta} + c_{i} + u_{it}
\; , \quad i = 1, \dots, N
\; , \quad t = 1, \dots, T
\\
\Delta y_{it} &= \Delta \mbs{x}_{it}' \mbs{\beta} + \Delta u_{it}
\; , \quad i = 1, \dots, N
\; , \quad t = 2, \dots, T
\end{align*}

Vamos supor $\Delta x_{it}'$ tem variável endógena ($y_{it}$, no caso).
$\mbs{w}_{it}$ é um vetor $1 \times L_{t}$ de instrumentos, onde $L_{t} \geq K$.
Se os instrumentos forem diferentes:

\vspace{-1 em}
\begin{align*}
	W_{i} = diag( \mbs{w}_{i2}', \mbs{w}_{i3}', \dots, \mbs{w}_{iT}')
\end{align*}

\noindent
onde $W_{i}$ é uma matriz $( T - 1 ) \times L$

\vspace{-1 em}
\begin{align*}
	L = L_{2} + L_{3} + \dots + L_{T}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Hipóteses}

\begin{description}
\item[FDIV.1:] $E( \mbs{w}_{it} \Delta u_{it}')$ para $i = 1, \dots, N$, $t = 2, \dots, T$.
\item[FDIV.2:] $Posto\left[ E( W_{i}' W_{i} ) \right] = L$
\item[FDIV.3:] $Posto\left[ E( W_{i}' \Delta X_{i} ) \right] = K$
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação FDIV}

\vspace{-1 em}
\begin{align*}
\boxed{
\mbs{\beta}^{FDIV} =  
\left(
\Delta X' P_{W} \Delta X 
\right)^{-1}
\left(
\Delta X' P_{W} \Delta \mbs{y}
\right)
}
\qquad
\boxed{
P_{W} = W(W'W)^{-1}W'}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Valor Esperado}

\vspace{-1 em}
\begin{align*}
E( \mbs{\beta}^{FDIV} ) =  
\beta + 
\left( \Delta X' P_{W} \Delta X \right)^{-1}
\left( \Delta X' P_{W} \mbs{e} \right)
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância}

\vspace{-1 em}
\begin{align*}
Var( \mbs{\beta}^{FDIV} ) &=
E\left\{  
\left[ E( \mbs{\beta}^{FDIV} ) - \beta \right] 
\left[ E( \mbs{\beta}^{FDIV} ) - \beta \right]'
\right\}
\\
&=
E\left\{  
\left[ \Delta X' P_{W} \Delta X \right]^{-1}
\left[ \Delta X' P_{W} \mbs{e} \right]
\left[ \Delta X' P_{W} \mbs{e} \right]'
\left[ \Delta X' P_{W} \Delta X \right]^{-1}
\right\}
\\
&=
E\left[
\left( \Delta X' P_{W} \Delta X \right)^{-1}
\left( \Delta X' P_{W} \mbs{e} \mbs{e}' P_{W} \Delta X \right)
\left( \Delta X' P_{W} \Delta X \right)^{-1}
\right]
\end{align*}

\noindent
$e_{i} = \Delta u_{it}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Latent Variables, Probit and Logit}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\textbf{Questão 7:}
Usando a motivação de uma \textbf{variável latente}, motive a construção do estimador \textbf{LOGIT/PROBIT}.
Explique o procedimento de estimação de verossimilhança que caracteriza o estimador.
Inclua em sua explicação o resultado da distribuição assintótica de 
$\sqrt{n}(\mbs{ \theta } - \mbs{\theta}_{0})$.
Ressalte a forma mais simples da variância assintótica desse estimador, devido ao fato de ser um estimador de máxima verossimilhança.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

Suponha $y^{*}$ não observável (\textbf{latente}) seguindo o seguinte modelo:

\vspace{-1 em}
\begin{align} \label{mod1:probit}
	y_{i}^{*} = \mbs{x}_{i}' \mbs{\beta} + \err_{i}.
\end{align}

\noindent
Defina $y$ como:

\vspace{-1 em}
\begin{align*}
y_{i} =
\begin{cases}
	1 \, , \quad y^{*}_{i} \geq 0
\\
	0 \, , \quad y^{*}_{i} < 0
\end{cases}
\end{align*}

\noindent
temos que:

\vspace{-1 em}
\begin{align*}
	P( y_{i} = 1 | \mbs{x} ) &= p( \mbs{x} )
	\\
	P( y_{i} = 0 | \mbs{x} ) &= 1 - p( \mbs{x} ).
\end{align*}

Além disso, pela definição de $y_{i}$, equação \eqref{mod1:probit}, temos:

\vspace{-1 em}
\begin{align*}
	P( y_{i} = 1 | \mbs{x} ) &= P(y_{i}^{*} \geq 0 \, | \mbs{x} )
\\
&= P( \mbs{x}_{i}' \mbs{\beta} + \err_{i} \geq 0 \, | \mbs{x} )
\\
&= P( \err_{i} \geq - \mbs{x}_{i}' \mbs{\beta}  \, | \mbs{x} ).
\end{align*}

\noindent
Agora, supondo que $\err_{i}$ tem FDA, $G$, tal que $G'=g$ é simétrica ao redor de zero:

\vspace{-1 em}
\begin{align*}
P( y_{i} = 1 | \mbs{x} ) 
&= 1 - P( \err_{i} < - \mbs{x}_{i}' \mbs{\beta}  \, | \mbs{x} )
\\
&= 1 - G( - \mbs{x}_{i}' \mbs{\beta}  \, | \mbs{x} )
\\
&= G( \mbs{x}_{i}' \mbs{\beta} ).
\end{align*}

Se $G(\cdot)$ for uma distribuição:

\begin{description}
	\item [Normal Padrão:] $\hat{\mbs{\beta}}$ é o estimador \textbf{probit}.
	\item [Logística:] $\hat{\mbs{\beta}}$ é o estimador \textbf{logit}.
\end{description}

Supondo $\mbs{y}_{i} \, | \, \mbs{x} \sim Bernoulli(p(\mbs{x}))$, sua fmp é dada por:

\vspace{-1 em}
\begin{align*}
f( y_{i} \, | \, \mbs{x}_{i} ; \mbs{\beta} ) 
&= 
\left[ G(\mbs{x}_{i}' \mbs{\beta} )  \right]^{y_{i}}
\left[ 1 - G(\mbs{x}_{i}' \mbs{\beta} )  \right]^{1 - y_{i}}
\; , \quad y=0,1.
\end{align*}

Para estimarmos $\hat{\mbs{\beta}}$ por máxima verossimilhança, temos de encontrar $\mbs{\beta} \in B$, onde $B$ é o espaço paramétrico, tal que $\mbs{\beta}$ maximize o valor da distribuição conjunta de $\mbs{y}$, ou seja:

\vspace{-1 em}
\begin{align*}
	\underset{\mbs{\beta} \in B}{\text{Max }} 
	\prod_{i=1}^{N}
	f( y_{i} \, | \, \mbs{x}_{i} ; \mbs{\beta} ).
\end{align*}

\noindent

Tirando o logaritmo e dividindo tudo por $N$ (podemos fazer isso pois são transformações monotônicas e não alteram o lugar onde $\mbs{\beta}$ ótimo irá parar):

\vspace{-1 em}
\begin{align*}
\underset{\mbs{\beta} \in B}{\text{Max }} 
\left\{ 
N^{-1} \sum_{i=1}^{N}
\ln \left[ f( y_{i} \, | \, \mbs{x}_{i} ; \mbs{\beta} ) \right]
\right\}.
\end{align*}

\noindent
Podemos definir
$\ell_{i}( \mbs{\beta} ) = \ln[ f( y_{i} \, | \, \mbs{x}_{i} ; \mbs{\beta} ) ]$
como sendo a verossimilhança condicional da observação $i$:

\vspace{-1 em}
\begin{align*}
\underset{\mbs{\beta} \in B}{\text{Max }} 
\left\{ 
N^{-1} \sum_{i=1}^{N} \ell_{i} (\mbs{\beta})
\right\}.
\end{align*}

Dessa forma, podemos ver que o problema acima é a analogia amostral de:

\vspace{-1 em}
\begin{align*}
\underset{\mbs{\beta} \in B}{\text{Max }} 
E \left[ 
\ell_{i} ( \mbs{\beta} )
\right].
\end{align*}

Definindo o \textit{vector score} da observação $i$:

\vspace{-1 em}
\begin{align*}
s_{i} (\mbs{\beta}) = 
\left[ \nabla_{\mbs{\beta}} \ell_{i} (\mbs{\beta}) \right]'
=
\begin{bmatrix}
	\dfrac{\partial{\ell_{i} (\mbs{\beta})}}{\partial{\beta_{1}}},
	\dots,
	\dfrac{\partial{\ell_{i} (\mbs{\beta})}}{\partial{\beta_{K}}}
\end{bmatrix}
\end{align*}

Definindo a \textbf{Matriz Hessiana} da observação $i$:

\vspace{-1 em}
\begin{align*}
H_{i} (\mbs{\beta}) = 
\nabla_{\mbs{\beta}} s_{i} (\mbs{\beta}) = 
\nabla_{\mbs{\beta}}^2 \ell_{i} (\mbs{\beta})
\end{align*}

Tendo essas definições, o \textbf{Teorema do Valor Médio} (TVM) nos diz que no intervalo $[a, b]$, existe um número, $c$, tal que:

\vspace{-1 em}
\begin{align*}
	f'(c) = \frac{f(b) - f(a)}{b - a}.
\end{align*}

\begin{center}
\red{FAZER DESENHO}
\end{center}

Trocando 
$f(\cdot)$ por $s_{i}(\cdot)$, 
$a$ por $\mbs{\beta}_{0}$, 
$b$ por $\widehat{\mbs{\beta}}$ e
$c$ por $\bar{\mbs{\beta}}$,
temos:

\vspace{-1 em}
\begin{align*}
H_{i} ( \bar{\mbs{\beta}} ) =
\frac{s_{i}( \widehat{\mbs{\beta}} ) - s_{i}( \mbs{\beta}_{0} )}{\widehat{\mbs{\beta}} - \mbs{\beta}_{0}},
\end{align*}

\noindent
tirando médias dos dois lados:

\vspace{-1 em}
\begin{align*}
N^{-1} \sum_{i=1}^{N} 
H_{i} ( \bar{\mbs{\beta}} ) 
=
\frac{1}{\widehat{\mbs{\beta}} - \mbs{\beta}_{0}}
N^{-1} \sum_{i=1}^{N} 
\left[ 
s_{i}( \widehat{\mbs{\beta}} ) - s_{i}( \mbs{\beta}_{0} )
\right]
\end{align*}

Supondo que
$\widehat{\mbs{\beta}}$
maximiza
$\ell (\mbs{\beta} \, | \, \mbs{y}, \mbs{x})$,
temos que:
$N^{-1} \sum_{i=1}^{N} s_{i}(\widehat{\mbs{\beta}}) = 0$.
E podemos reescrever a equação anterior como:

\vspace{-1 em}
\begin{align*}
\widehat{\mbs{\beta}} - \mbs{\beta}_{0}
&=
(-1)
\left[ N^{-1} \sum_{i=1}^{N} H_{i} ( \bar{\mbs{\beta}} ) \right]^{-1}
N^{-1} \sum_{i=1}^{N} s_{i}( \mbs{\beta}_{0} ) 
\\
\sqrt{N} ( \widehat{\mbs{\beta}} - \mbs{\beta}_{0} )
&=
\left[
- N^{-1} \sum_{i=1}^{N} H_{i} ( \bar{\mbs{\beta}} )
\right]^{-1}
\sqrt{N} \cdot N^{-1} \sum_{i=1}^{N} s_{i}( \mbs{\beta}_{0} ) 
\\
\Aboxed{
\sqrt{N} ( \widehat{\mbs{\beta}} - \mbs{\beta}_{0} )
&=
\left[
- N^{-1} \sum_{i=1}^{N} H_{i} ( \bar{\mbs{\beta}} )
\right]^{-1}
N^{-1/2} \sum_{i=1}^{N} s_{i}( \mbs{\beta}_{0} ) }.
\end{align*}

\noindent
Onde

\vspace{-1 em}
\begin{align*}
\left[ 
- N^{-1} \sum_{i=1}^{N}
H_{i} ( \bar{\mbs{\beta}} ) \right]^{-1}
\xrightarrow{p}
A_{0}^{-1} \, ,
&&
N^{-1/2} \sum_{i=1}^{N} s_{i}( \mbs{\beta}_{0} ) 
\xrightarrow{d}
N( 0, B_{0} ).
\end{align*}

\noindent
Assim, temos que:

\vspace{-1 em}
\begin{align*}
\Aboxed{
\sqrt{N} ( \widehat{\mbs{\beta}} - \mbs{\beta}_{0} )
\to
N ( 0, A_{0}^{-1} B_{0} A_{0}^{-1} )}.
\end{align*}

A forma mais simples de achar $Var ( {\widehat{\mbs{\beta}}} )$ é:

\vspace{-1 em}
\begin{align*}
	\Aboxed{
Var( \widehat{\mbs{\beta}} )
&=
- E[ H_{i} ( \widehat{\mbs{\beta}} ) ]^{-1} }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{ATT, ATE, Propensity Score}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\textbf{Questão 8:}
Explique como estimar o efeito médio do tratamente ($\tau_{ATE}$) e o efeito médio do tratamento sobre o tratado ($\tau_{ATT}$), considerando a hipótese de Ignorabilidade do Tratamento condicional a um conjunto de covariáveis.
Aborde o método \textit{Propensity Score}.
Discuta a importância do hipótese \textit{Overlap} para a aplicabilidae desse estimador.
Explique resumidamente como o \textit{Propensity Score} pode ser estimado.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

\begin{itemize}\itemsep0pt
\item
$y_{1}$ $\rightarrow$ variável de interesse com tratamento

\item
$y_{0}$ $\rightarrow$ variável de interesse sem tratamento
\end{itemize}

\vspace{-1 em}
\begin{align*}
w = 
\begin{cases}
1 & \text{se tratam}
\\
0 & \text{se não tratam}
\end{cases}
\end{align*}
 
Idealmente, para isolarmos completamente o efeito de $w=1$, gostaríamos de pode calcular:

\vspace{-1 em}
\begin{align*}
	N^{-1} \sum_{i=1}^{N}
	\left( y_{i1} - y_{i0} \right).
\end{align*}

Ou seja, o efeito que o tratamento causa sobre um indivíduo com todo o resto permanecendo constante.
Em outras palavras, queríamos que houvesse dois mundos paralelos observáveis onde seria possível observar o que acontece com $y_{i}$ com e sem tratamento.
Infelizmente, para ccada indivíduo $i$, observamos apenas $y_{i1}$ ou $y_{i0}$, nunca ambos.

Antes de continuarmos, faremos as seguintes definições:

\begin{description}
	\item[ATE:]  $E( y_{1} - y_{0} )$
	\item[ATT:]  $E( y_{1} - y_{0} \, | \, w = 1 )$ (ATE no tratado).
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{$ATE$ e $ATT$ condicional a variáveis $\mbs{x}$ }

\vspace{-1 em}
\begin{align*}
ATE( \mbs{x} ) &= E( y_1 - y_0 \, | \mbs{x})
\\
ATT( \mbs{x} ) &= E( y_1 - y_0 \, | \mbs{x}, w = 1)
\end{align*}

\noindent
\underline{OBS:}

\vspace{-1 em}
\begin{align*}
E( y_1 - y_0 ) &= E \left[ E( y_1 - y_0 \, | w) \right]
\\
E( y_1 - y_0 \, | w ) &=
E( y_1 - y_0 \, | w = 0 ) \cdot P(w=0)
+
E( y_1 - y_0 \, | w = 1 ) \cdot P(w=1).
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Métodos Assumindo Ignorabilidade do Tratamento}

\begin{description}
\item[ATE.1:] Ignorabilidade. 
\\
$w$ e $(y_{1}, y_{0})$ são independentes condicionais a $\mbs{x}$.

\item[ATE.1':] Ignorabilidade da Média. 

\vspace{-.75 em}
\begin{enumerate}[label =\alph*)] \itemsep0pt
\item $E( y_{0} \, | \, w, \mbs{x} ) = E( y_{0} \, | \, \mbs{x} )$
\item $E( y_{1} \, | \, w, \mbs{x} ) = E( y_{1} \, | \, \mbs{x} )$
\end{enumerate}

\end{description}

Vamos definir

\vspace{-1 em}
\begin{align*}
E( y_{0} \, | \, \mbs{x} ) &= \mu_{0}( \mbs{x} )
\\
E( y_{1} \, | \, \mbs{x} ) &= \mu_{1}( \mbs{x} ).
\end{align*}

Sob \textbf{ATE.1} e \textbf{ATE.1'}:

\vspace{-1 em}
\begin{align*}
	ATE( \mbs{x} ) &= E( y_{1} - y_{0} \, | \mbs{x} ) = \mu_{1}(\mbs{x}) - \mu_{0}(\mbs{x}) 
	\\
	ATT( \mbs{x} ) &= E( y_{1} - y_{0} \, | \mbs{x}, w=1 ) = \mu_{1}(\mbs{x}) - \mu_{0}(\mbs{x}) 
\end{align*}

\begin{description}
\item[ATE.2:] \textit{Overlap} \\
Para todo $\mbs{x}$, $P(w=1 \, | \, \mbs{x} ) \in ( 0, 1 )$, 
$p(\mbs{x}) = p(w=1 | \mbs{x})$.
\end{description}

$p(\mbs{x})$ é o \textit{Propensity Score}, ele representa a probabilidade de $y_{i}$ ser tratado dado o valor das covariáveis $\mbs{x}$.
Essa hipótese é importante visto que podemos expressar o $ATE$ em função de $p(\mbs{x})$.

\vspace{1 em}
Para o $ATT$ vamos supor:

\begin{description}
\item[ATT.1':] 
	$E( y_{0} \, | \mbs{x}, w ) = E( y_{0} \, | \, \mbs{x} )$

\item[ATT.2:] \textit{Overlap:} Para todo $\mbs{x}$, $P(w=1 | \mbs{x} ) < 1$.
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Propensity Score}

Como foi dito anteriormente, apenas observamos ou $y_{1}$ ou $y_{0}$ para a mesma pessoa, mas não ambos.
Mais precisamente, junto com $w$, o resultado observado é:

\vspace{-1 em}
\begin{align*}
	y = wy_{1} + (1 - w) y_{0}
\end{align*}

\noindent
como  $w$ é binário, $w^2 = w$, assim, temos:

\vspace{-1 em}
\begin{align*}
w y &= w^{2} y_{1} + (w - w^{2}) y_{0}
\implies
\boxed{w y = w y_{1} }
\\
( 1 - w ) y &= (w - w^{2}) y_{1} + ( w^{2} - 2w + 1 ) y_{0}
\implies
\boxed{( 1 - w ) y = (1 - w) y_{0}}.
\end{align*}

Fazemos isso para tentar isolar $\mu_{0}(\mbs{x})$ e $\mu_{1}(\mbs{x})$:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{$\mu_{1}( \mbs{x} )$}

\begin{align*}
E( w y | \mbs{x} ) &= E\left[  E \left( w y_{1} | \mbs{x}, w  \right) | \mbs{x} \right]
\\ &=
E \left[ w \mu_{1}(\mbs{x}) | \mbs{x} \right]
\\ &=
\mu_{1}(\mbs{x}) E(w | \mbs{x} ).
\end{align*}

\noindent
Como $w$ é binaria: $E(w| \mbs{x}) = P(w=1 | \mbs{x}) = p(\mbs{x})$.
Assim:

\vspace{-1 em}
\begin{align*}
E( w y | \mbs{x} ) &= \mu_{1}(\mbs{x}) p(\mbs{x})
\\
\Aboxed{ \mu_{1}(\mbs{x}) &= \frac{E(w y | \mbs{x})}{p(\mbs{x})} }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{$\mu_{0}( \mbs{x} )$}

\vspace{-1 em}
\begin{align*}
	E[ (1-w) y | \mbs{x} ] &= E\left[  E \left( (1 - w) y_{0} | \mbs{x}, w  \right) | \mbs{x} \right]
\\ &=
E \left[ (1 - w) \mu_{0}(\mbs{x}) | \mbs{x} \right]
\\ &=
\mu_{0}(\mbs{x}) E(w | \mbs{x} )
\\ 
E[ (1-w) y | \mbs{x} ] 
&=
\mu_{0}(\mbs{x}) [1 - p(\mbs{x})] \implies
\\ 
\Aboxed{
\mu_{0}(\mbs{x})
&=
\frac{E[ (1-w) y | \mbs{x} ] }{1 - p( \mbs{x} ) } }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{ATE:}

\begin{align*}
\mu_{1}(\mbs{x}) - \mu_{0}(\mbs{x}) =
E\left[ 
\frac{[w - p(\mbs{x})] y}{p(\mbs{x}) [1 - p(\mbs{x})]}
| \mbs{x}
\right]
\end{align*}

\begin{align*}
\Aboxed{
\widehat{ATE} =
N^{-1} \sum_{i=1}^{N}
\frac{[ w_{i} - p(\mbs{x}_{i} ) ] y_{i} }{ p( \mbs{x}_{i} ) [1 - p( \mbs{x}_{i} ) ] }
}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{ATT:}

\begin{align*}
E( y_{1} | \mbs{x}, w=1) - E(y_{0} | \mbs{x}) =
\frac{1}{\hat{P}(w=1)}
E\left[ 
\frac{[w - \hat{p}(\mbs{x})] y}{[ 1 - \hat{p}(\mbs{x}) ]}
| \mbs{x}
\right]
\end{align*}

\vspace{-1 em}
\begin{align*}
	\hat{P} (w = 1) = N^{-1} \sum_{i=1}^{N} w_{i}
\end{align*}

\vspace{-1.5 em}
\begin{align*}
\widehat{ATT} &=
\frac{N}{\sum_{i=1}^{N} w_{i} }
N^{-1} \sum_{i=1}^{N}
\frac{[ w_{i} - \hat{p}(\mbs{x}_{i} ) ] y_{i} }{[ 1 - \hat{p}( \mbs{x}_{i} ) ]}
\\
\Aboxed{
\widehat{ATT} &=
\frac{1}{\sum_{i=1}^{N} w_{i} }
\sum_{i=1}^{N}
\frac{[ w_{i} - \hat{p}(\mbs{x}_{i} ) ] y_{i} }{[1 - \hat{p}( \mbs{x}_{i} ) ]}
}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section*{Appêndice}

\subsection*{Sums of Values}
\noindent
\cite[p. 977, A.2.7]{greene-7ed}

\begin{align*}
\mbs{1}_{N}' \mbs{1}_{N} = N
\quad ; \qquad
\mbs{1}_{N} \mbs{1}_{N}' =
\begin{bmatrix}
	1 & \dots & 1	 \\
	\vdots & \ddots & \vdots \\
	1 & \dots & 1	
\end{bmatrix}_{N \times N}
\end{align*}

Defining $\mbs{x}$ with dimension $1 \times N$:

\begin{align*}
\mbs{x} = 
\begin{bmatrix}
x_{1} \\ \vdots \\ x_{N}	
\end{bmatrix}
\end{align*}

% SUM
\begin{align*}
\mbs{x}' \mbs{1}_{N} = 
\mbs{1}_{N}' \mbs{x} = 
(\mbs{x}' \mbs{1}_{N})' = 
\sum_{i=1}^{N} x_{i}
\end{align*}

% Matrix
\begin{align*}
\mbs{1}_{N} \mbs{x}' =
\begin{bmatrix}
	x_{1} & \dots & x_{N} \\
	\vdots & \ddots & \vdots \\
	x_{1} & \dots & x_{N}	
\end{bmatrix}_{N \times N}
\; ; \qquad
\mbs{x} \mbs{1}_{N}' =
\begin{bmatrix}
	x_{1} & \dots & x_{1} \\
	\vdots & \ddots & \vdots \\
	x_{N} & \dots & x_{N}	
\end{bmatrix}_{N \times N}
\end{align*}

\begin{align*}
E( \mbs{x} ) = \overline{\mbs{x}} = N^{-1} \sum_{i=1}^{N} x_{i} = N^{-1} \mbs{x}'\mbs{1}_{N}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Important Idempotent Matrices}
\noindent
\cite[p. 978, A.28]{greene-7ed}

Centering Matrix

\vspace{-1 em}
\begin{align*}
	M^{0} &= 
	I_{N} - \mbs{1}_{N} ( \mbs{1}_{N}' \mbs{1}_{N} )^{-1} \mbs{1}_{N}'
	= 
	I_{N} - N^{-1} \mbs{1}_{N} \mbs{1}_{N}' 
\end{align*}

A Matriz $M^{0}$ é \textbf{idempotente} e \textbf{simétrica}.

\begin{description}\itemsep0pt
\item [Idempotência:] $AA = A$
\item [Simetria:] $A'=A$
\end{description}


\vspace{-1 em}
\begin{align*}
M^{0} \mbs{x} &= 
( I_{N} - N^{-1} \mbs{1}_{N} \mbs{1}_{N}' ) \mbs{x} 
= 
\mbs{x} - N^{-1} \mbs{1}_{N} (\mbs{1}_{N}' \mbs{x}) 
=
\mbs{1}_{N} \overline{\mbs{x}}
=
\begin{bmatrix}
\overline{\mbs{x}} \\ \vdots \\ \overline{\mbs{x}}
\end{bmatrix}
\end{align*}

\vspace{-1 em}
\begin{align*}
M^{0} \mbs{1} &= 
( I_{N} - N^{-1} \mbs{1}_{N} \mbs{1}_{N}' ) \mbs{1}_{N}
= 
\mbs{1}_{N} - N^{-1} \mbs{1}_{N} (\mbs{1}_{N}' \mbs{1}_{N}) 
=
\mbs{0}_{N} 
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Bibliografia
\clearpage
\bibliographystyle{authordate3} % bibliography style
\renewcommand\bibname{REFERENCES} 
\bibliography{../refs.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}


