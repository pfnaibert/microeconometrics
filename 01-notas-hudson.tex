\documentclass[11pt, oneside, a4paper, article]{article}
% \documentclass[11pt, oneside, a4paper, article, ms]{memoir}
% \documentclass[11pt,oneside,a4paper]{letter}
\usepackage[a4paper, margin=1in]{geometry}   % MARGENS

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pacotes básicos 
\usepackage[T1]{fontenc}		% Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}		% Codificacao do documento (conversão automática dos acentos)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% languages
\usepackage[english]{babel}
% \selectlanguage{english}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AMS math
\usepackage{amsfonts, amssymb, amsthm}
\usepackage[fleqn]{amsmath}
\setlength{\mathindent}{0pt}

\usepackage{mathtools, latexsym}
% \usepackage{mathabx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pacotes de citações
\usepackage{natbib}	% Citações}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{indentfirst}		% Indenta o primeiro parágrafo de cada seção.
\usepackage{color}				% Controle das cores
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{microtype} 			% para melhorias de justificação
\usepackage{lscape}				% Gira a página em 90 graus
%\usepackage{listings}			% Formatação para inserir códigos
\usepackage[normalem]{ulem}
\usepackage[all]{xy}
\usepackage{xcolor}
\usepackage{ragged2e}           % formatação texto
\usepackage{bm}                 % bold symbols 
\usepackage[colorlinks, citecolor=blue, urlcolor=blue, linkcolor=red]{hyperref} % referencias dentro do texo (*QUEBRA MEMOIR*)
\usepackage{url}                % URL
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{subcaption}			% Faz subfiguras
%\usepackage{subfigure}         % subfigures
% \usepackage{setspace}         % Espaçamento
\usepackage{enumitem}
% \usepackage{enumerate}
\usepackage[sharp]{easylist}
% \usepackage{titlesec}		    % Customização de seçoes
\usepackage{titling}          % personalized other things
\usepackage{fancyhdr}         % personalized page style
\usepackage{lipsum}           % dummy text
% \usepackage{exercise}       % exercises
\usepackage[pagewise]{lineno}           % show line numbers
% \linenumbers

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{macros.tex}
% \pagestyle{headings}
\pagestyle{fancy}
\numberwithin{equation}{section}
% \setcounter{equation}{0}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METADATA
\title{Microeconometrics: Lecture Notes }
\author{Paulo F. Naibert}
% \date{25/06/2020}
% \date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}
\pagenumbering{gobble}

\begin{center}
\textbf{UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL}
\\
\textbf{PROGRAMA DE PÓS-GRADUAÇÃO EM ECONOMIA}
\\
\textbf{Microeconometria -- 2015/3}

\vfill
\textbf{\thetitle}

\vfill
\textbf{Autor: Paulo Ferreira Naibert } 
\\
\textbf{Professor: Hudson Torrent} 


\end{center}

\vfill

\begin{center}
\textbf{Porto Alegre \\ 30/06/2020 \\ Revisão: \today}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\pagenumbering{arabic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Regressão MQO Clássico}
\noindent
\citet[C.4 -- The Single-Equation Linear Model and OLS Estimation, p.49--76]{wool-2010}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelo de equações lineares} 

O modelo populacional que estudamos é linear em seus parâmetros,

\vspace{-1 em}
\begin{align} \label{ols:mod}
y &= \beta_{0} + \beta_{1} x_{1} + \dots + \beta_{K} x_{K} + u
\end{align}
onde:

\begin{description}[\noitemsep]
	\item [$y, x_{1}, \dots, x_{K}$]  são escalares aleatórios e observáveis (i.e., conseguimos observá-los em uma amostra aleatória da população);

	\item [$u$] é o \textit{random disturbance} não observável, ou erro; 

	\item [$\beta_{0}, \beta_{1}, \dots, \beta_{K}$] são parâmetros (constantes) que gostaríamos de estimar.
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Notação Vetorial} 
\noindent
\citet[Sec. 4.2 -- Asymptotic Properties of OLS; p.51]{wool-2010}

Por conveniência, escrevemos a equação populacional em forma de vetor:

\vspace{-1 em}
\begin{align} \label{ols:mod:vec}
	y &= \mbs{x} \mbs{\beta} + u
\end{align}

\noindent
onde,

\vspace{-1 em}
\begin{description}[noitemsep]
\item [$\mbs{x} \equiv (x_{1}, \dots, x_{K})$] é um vetor $1 \times K$ de regressores;

\item [$\mbs{\beta} \equiv (\beta_{1}, \dots, \beta_{K})'$] é um vetor $K \times 1$.
\end{description}

\noindent
Uma vez que a maioria das equações contém um intercepto, assumiremos que $x_{1} \equiv 1$, visto que essa hipótese deixa a interpretação mais fácil.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Amostra Aleatória} 

Assumimos que conseguimos obter uma amostra aleatória de tamanho $N$ da população para estimarmos $\mbs{\beta}$.
Dessa forma, $\{ (\mbs{x}_{i}, y_{i}); \, i = 1, 2, \dots, N \}$
são tratados como variáveis aleatória independentes, identicamente distribuídas, onde
$\mbs{x}_{i}$ é $1 \times K$ e $y_{i}$ é escalar.
Para cada observação $i$, temos:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1 em}
\begin{align} \label{ols:mod:vec:i}
	y_{i} &= \mbs{x}_{i} \mbs{\beta} + u_{i}.
\end{align}

\noindent
onde
$\mbs{x}_{i}$
é um vetor $1 \times K$ de regressores.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Notação Matricial [Meu]}
Empilhando as $N$ observações, obtemos a \textbf{Notação Matricial}:

\vspace{-1 em}
\begin{align} \label{ols:mod:mat}
	\mbs{y} &= \Xmat \mbs{\beta} + \mbs{u} 
\end{align}

\begin{description}[noitemsep]
\item [$\mbs{y}$]  é um vetor $N \times 1$;

\item [$\Xmat$]  é uma matriz $N \times K$ de regressores, com $N$ vetores, $\mbs{x}_{i}$, de dimensão $1 \times K$ empilhados;

\item [$\mbs{\beta}$] é um vetor $K \times 1$;

\item [$\mbs{u}$] é um vetor $N \times 1$;
\end{description}

\vspace{-1 em}
\begin{align*}
\mbs{y} = 
\begin{bmatrix}
	y_{1} \\ \vdots \\ y_{N}		
\end{bmatrix};
\quad
\Xmat = 
\begin{bmatrix}
	\mbs{x}_{1} \\ \vdots \\ \mbs{x}_{N}
\end{bmatrix} = 
\begin{bmatrix}
	x_{11}     & x_{12}     & \dots  & x_{1K} \\          
%	x_{21}     & x_{22}     & \dots  & x_{2K} \\         
	\vdots     & \vdots     & \ddots & \vdots \\        
	x_{N1} & x_{N2} & \dots  & x_{NK}		
\end{bmatrix};
\quad
\mbs{u} = 
\begin{bmatrix}
	u_{1} \\ \vdots \\ u_{N}		
\end{bmatrix}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{OLS.1} 
$y_{i} &= \mbs{x}_{i} \mbs{\beta} + u_{i} \, , \quad i = 1, \dots, N$;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{OLS.2}  $\Xmat$ é \textbf{não} estocástica;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{OLS.3} $\{ u_{i} \}_{i=1}^{N}$  é  $iid$ com e para cada $i = 1, \dots, N$:

\vspace{-1.5 em}
\begin{align*}
\E(u_{i}) &= 0
\\
\Var(u_{i}) &= \E(u_{i}^2) = \sigma^2
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{OLS.2'} $\Xmat$ é estocástica;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{OLS.3'} 

\begin{align*}
\E(u_{i} | \Xmat) &= 0, 
\\
\Var(u_{i} | \Xmat) &= 
E
\left\{ \left[ 
u_{i} - \E( u_{i} | \Xmat)
\right]^2 | \Xmat \right\}
=
\E(u_{i}^2 | \Xmat) = \sigma^2.
\end{align*}

\begin{remark}
$\E(u_{i} | \Xmat) = 0$ implica que $u_{i}$ é \textbf{não correlacionado} com todos os regressores $x_{k}$ para $k=1,\dots, K$. \red{Exogeneidade estrita}.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimação} 

\vspace{-2 em}
\begin{equation} \label{betahat:ols}
	\boxed{ \widehat{\mbs{\beta}} = (\Xmat'\Xmat)^{-1}\Xmat'\yvec }
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Valor Esperado} 

\vspace{-2 em}
\begin{align*} 
\E ( \widehat{\mbs{\beta}} ) 
&= \E \left[ (\Xmat'\Xmat)^{-1}\Xmat'\mbs{y} \right]
\\
&= \E \left[ (\Xmat'\Xmat)^{-1}\Xmat'(\Xmat \mbs{\beta} + \mbs{u}) \right]
\\
&= \E \left[ (\Xmat'\Xmat)^{-1}\Xmat'\Xmat \mbs{\beta} + (\Xmat'\Xmat)^{-1}\Xmat'\mbs{u} \right]
\\
&= \E (\mbs{\beta}) + \E[(\Xmat'\Xmat)^{-1}\Xmat'\mbs{u} ]
\\
\Aboxed{
\E ( \widehat{\mbs{\beta}} ) 
&= \mbs{\beta} + \E[(\Xmat'\Xmat)^{-1}\Xmat'\mbs{u} ] }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Viés} 

\vspace{-2 em}
\begin{align*} 
\Aboxed{ B( \widehat{\mbs{\beta}} ) &= \E ( \widehat{\mbs{\beta}} ) - \mbs{\beta} }
\\
\Aboxed{ B( \widehat{\mbs{\beta}} ) &= \E[ (\Xmat'\Xmat)^{-1}\Xmat'\mbs{u} ] }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
Sob \textbf{OLS.2'} e \textbf{OLS.3'}:

\vspace{-1 em}
\begin{align*}
\E[(\Xmat'\Xmat)^{-1}\Xmat'\mbs{u} ]
&= \E \left\{ \E\left[ (\Xmat'\Xmat)^{-1}\Xmat'\mbs{u} | \Xmat \right]  \right\}  
\\
&= \E \left\{  (\Xmat'\Xmat)^{-1}\Xmat'
\underbracket[.75pt]{\E( \mbs{u} | \Xmat )}_{= \mbs{0}}
\right\} = 0
\end{align*}

\noindent
ou seja, 
$B( \widehat{\mbs{\beta}} ) = 0$, logo $\widehat{\mbs{\beta}}$ é \textbf{não-viciado}.
O que também é equivalente a  $\E( \widehat{\mbs{\beta}} ) = \mbs{\beta}$.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variância} 
Supondo \textbf{OLS.2'} e \textbf{OLS.3'}:

\vspace{-1 em}
\begin{align*} 
\Var ( \widehat{\mbs{\beta}} | \Xmat) 
&= \E \left\{\left[ 
\widehat{\mbs{\beta}} - \E ( \widehat{\mbs{\beta}} | \Xmat )
\right]^2 | \Xmat \right\}
\\
&= \E \left\{ 
\left[ \widehat{\mbs{\beta}} - \E ( \widehat{\mbs{\beta}} | \Xmat ) \right]
\left[ \widehat{\mbs{\beta}} - \E ( \widehat{\mbs{\beta}} | \Xmat ) \right]'
| \Xmat \right\}
\\
&= \E \left\{ 
\left[ (\Xmat'\Xmat)^{-1}\Xmat' \mbs{u} \right]
\left[ (\Xmat'\Xmat)^{-1}\Xmat' \mbs{u} \right]'
| \Xmat \right\}
\\
&= \E \left[ (\Xmat'\Xmat)^{-1}\Xmat' \mbs{u} \mbs{u}' \Xmat (\Xmat'\Xmat)^{-1} | \Xmat \right]
\\
\Aboxed{
\Var ( \widehat{\mbs{\beta}} | \Xmat) 
&= 
(\Xmat'\Xmat)^{-1}\Xmat' 
\E \left[ \mbs{u} \mbs{u}'| \Xmat \right]
\Xmat (\Xmat'\Xmat)^{-1} }
\end{align*}

Supondo homocedasticidade e ausência de correlação serial: 
$\boxed{ \E \left[ \mbs{u} \mbs{u}'| \Xmat \right] = \sigma^2 I_{N} }$.
Assim, 

\vspace{-1 em}
\begin{align*} 
\Var ( \widehat{\mbs{\beta}} | \Xmat) 
&= \sigma^2 (\Xmat'\Xmat)^{-1}\Xmat' I_{N} \Xmat (\Xmat'\Xmat)^{-1}
= \sigma^2 (\Xmat'\Xmat)^{-1}\Xmat'\Xmat (\Xmat'\Xmat)^{-1}
\\
\Aboxed{ \Var ( \widehat{\mbs{\beta}} | \Xmat) &= \sigma^2 (\Xmat'\Xmat)^{-1} }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Ausência de Exogeneidade Estrita}
Nem sempre poderemos supor \textbf{exogeneidade estrita}.
Por exemplo, no modelo com variável defasada mostrado abaixo:

\vspace{-1 em}
\begin{align*}
%  \left.
%  \begin{aligned}
y_{t} &= \beta_{0} + \beta_{1} y_{t-1} + \beta_{2} x_{1t} + u_{t}
\\
y_{t-1} &= \beta_{0} + \beta_{1} y_{t-2} + \beta_{2} x_{1t-1} + u_{t-1}
%  \end{aligned}
%  \right\}
%  \implies
\\
y_{t} &=
\beta_{0}(1 + \beta_{1})
+
\beta_{1}^2 y_{t-2}
+
\beta_{1} 
\beta_{2} x_{1t-1} 
+
\beta_{2} x_{1t} 
+
u_{t}
+
\beta_{1} u_{t-1},
\end{align*}

\noindent
o erro é correlacionado com o regressor $y_{t-1}$.
Nesse caso, tentaremos obter apenas \textbf{consistência} e \textbf{variância assintótica} do estimador.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}
\red{Aqui comeceçaria a seção \ref{app1}}.
\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimação}

Lembrando que o estimador de OLS é:

\vspace{-1 em}
\begin{align*}
\betahatbold = (\Xmat'\Xmat)^{-1}\Xmat'\mbs{y},
\end{align*}

\noindent
onde usamos o modelo

\vspace{-1 em}
\begin{align*}
	y_{i} = \mbs{x}_{i} \mbs{\beta} + u_{i},
\end{align*}

\noindent
e definimos as variáveis

\vspace{-1 em}
\begin{align*}
\Xmat_{N \times K} =
\begin{bmatrix}
	\mbs{x}_{1} \\ \vdots \\	\mbs{x}_{N}	
\end{bmatrix},
\quad
\mbs{y} =
\begin{bmatrix}
	y_{1} \\ \vdots \\ y_{N}
\end{bmatrix},
\quad
\mbs{u} =
\begin{bmatrix}
	u_{1} \\ \vdots \\ u_{N}
\end{bmatrix}.
\end{align*}

\noindent
Assim, representamos $(\Xmat'\Xmat)^{-1}$ e $(\Xmat'\mbs{y})$ por meio dos seguintes somatórios

\vspace{-1 em}
\begin{align*}
\left( \Xmat'\Xmat \right)^{-1} = \left( \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1} ,
\quad
\left( \Xmat'\mbs{y} \right) = \sum_{i=1}^{N} \mbs{x}_{i}' y_{i}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Valor Esperado e Viés}

\vspace{-2 em}
\begin{align*}
\betahatbold &= 
\left( \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \mbs{x}_{i}' y_{i} \right)
\\
&=
\left( \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \mbs{x}_{i}' (\mbs{x}_{i} \mbs{\beta} + \mbs{u}_{i}) \right)
\\
&=
\left( \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \mbs{\beta} \right) +
\left( \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{u}_{i} \right)
\\
&=
\betavec +
\left( \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{u}_{i} \right).
\end{align*}

\noindent
Usando \textbf{LGN matricial} (lembrar que as dimensões dos vetores estão invertidas: $1 \times K$ e \textbf{não} $K \times 1$), temos:

\vspace{-1 em}
\begin{align} \label{eq:Q}
	\Aboxed{N^{-1} \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \arrowp Q }.
\end{align}

\noindent
Supondo $\E(\mbs{x}_{i}' \mbs{x}_{i}) = Q_{K \times K}$, finita e positiva definida, $posto(Q) = K$.
Supondo $\E(\mbs{x}_{i}' u_{i}) = 0$, o que corresponde a $Cov(\mbs{x}_{i}, u_{i}) = 0$, ou seja, o erro $u_{i}$ \textbf{não} é correlacionado com os regressores da própria equação.
Isso é bem menos que exogeneidade estrita.

Então, 

\vspace{-1 em}
\begin{align*}
	N^{-1} \sum_{i=1}^{N} \mbs{x}_{i}' u_{i} \arrowp \E(\mbs{x}_{i}' u_{i}) = \mbs{0}_{K}.
\end{align*}

Logo

\vspace{-1 em}
\begin{align*}
\betahatbold = 
\betavec +
\underbracket[1pt]{
\left( \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \mbs{x}_{i}' u_{i} \right)}_{\arrowp 0}
\end{align*}

Então, 
$(\betahatbold - \betavec) \arrowp 0$ 
que é equivalente a 
$\boxed{\betahatbold \arrowp \betavec}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Normalidade Assintótica} % do $\betahatbold^{OLS}$}

\vspace{-2 em}
\begin{align*}
\betahatbold 
&= 
\betavec +
\left( \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \mbs{x}_{i}' u_{i} \right)
\\
(\betahatbold - \betavec)
&=
\left( \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \mbs{x}_{i}' u_{i} \right)
\\
\sqrt{N} (\betahatbold - \betavec) &=
\left( N^{-1} \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}
\left( N^{-1/2} \sum_{i=1}^{N} \mbs{x}_{i}' u_{i} \right)
\end{align*}

\noindent
Supondo

\vspace{-1 em}
\begin{align*}
	\E( x_{ik}^{2} u_{i}^{2} ) < + \infty \, , \quad k=1, \dots, K, 
\end{align*}

\noindent
temos, pelo \textbf{TCL}, que

\vspace{-1 em}
\begin{align} \label{eq:limxu}
\Aboxed{ N^{-1/2} \sum_{i=1}^{N} \mbs{x}_{i}' u_{i} \arrowd N(0, B) }
\end{align}

\noindent
onde 

\vspace{-1 em}
\begin{align*}
B =
\E[\mbs{x}_{i}' u_{i}' u_{i} \mbs{x}_{i}] =
\E[ u_{i}^2 \mbs{x}_{i}' \mbs{x}_{i} ].
\end{align*}

E temos que $\boxed{N^{-1/2} \sum_{i=1}^{N} \mbs{x}_{i}' u_{i} = O_{p}(1)}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1 em}
Além disso, vamos utilizar a matriz \textbf{simétrica} e \textbf{não singular} $Q$ da equação \eqref{eq:Q}
Assim, temos 

\vspace{-1 em}
\begin{align*}
\sqrt{N} (\betahatbold - \betavec) &=
\left( N^{-1} \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}
\left( N^{-1/2} \sum_{i=1}^{N} \mbs{x}_{i}' u_{i} \right)
\\ &=
\left[ 
\left( N^{-1} \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1} 
+ Q^{-1} - Q^{-1}
\right]
\left( N^{-1/2} \sum_{i=1}^{N} \mbs{x}_{i}' u_{i} \right)
\\ &=
\left[ 
\left( N^{-1} \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1} 
- Q^{-1}
\right]
\left( N^{-1/2} \sum_{i=1}^{N} \mbs{x}_{i}' u_{i} \right)
+ Q^{-1} 
\left( N^{-1/2} \sum_{i=1}^{N} \mbs{x}_{i}' u_{i} \right),
\end{align*}

\noident
Podemos inverter $Q$ porque ela tem posto completo (não singular).
Pelas propriedades de $Q$, temos:

\vspace{-1 em}
\begin{align*}
N^{-1} \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \arrowp Q
\implies
\left( N^{-1} \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}  - Q^{-1} = o_{p}(1).
\end{align*}

Então,

\vspace{-1 em}
\begin{align*}
\sqrt{N} (\betahatbold - \betavec) &=
0_{p}(1) O_{p}(1)
+ Q^{-1} \left( N^{-1/2} \sum_{i=1}^{N} \mbs{x}_{i}' u_{i} \right),
\end{align*}

Usando \eqref{eq:limxu} e a definição \red{XX}

\begin{align*}
Q^{-1} \left( N^{-1/2} \sum_{i=1}^{N} \mbs{x}_{i}' u_{i} \right)
\arrowd 
N(\mbs{0}, Q^{-1} B Q^{-1}).
\end{align*}

Lembrando que $o_{p}(1) O_{p}(1) = o_{p}(1)$, temos:

\vspace{-1 em}
\begin{align*}
	\Aboxed{\sqrt{N} (\betahatbold - \betavec) \arrowd N(\mbs{0}, Q^{-1} B Q^{-1})}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variância}

Vamos definir $\boxed{V = Q^{-1} B Q^{-1} }$:

\begin{align*}
V =
\E[ (\mbs{x}_{i}' \mbs{x}_{i} ) ]^{-1}
\E[ (\mbs{x}_{i}' u_{i}' u_{i} \mbs{x}_{i} ) ]
\E[ (\mbs{x}_{i}' \mbs{x}_{i} ) ]^{-1}
\\
\Aboxed{
V =
\E[ (\mbs{x}_{i}' \mbs{x}_{i} ) ]^{-1}
\E[ (u_{i}^{2} \mbs{x}_{i}' \mbs{x}_{i} ) ]
\E[ (\mbs{x}_{i}' \mbs{x}_{i} ) ]^{-1} }.
\end{align*}

\noindent
Sob \textbf{Homocedasticidade}: 
$B = \E(u_{i}^2 \mbs{x}_{i}' \mbs{x}_{i}) &= \sigma^2 \E(\mbs{x}_{i}' \mbs{x}_{i})$

\begin{align*}
\Aboxed{
V &= \sigma^{2} \E[ (\mbs{x}_{i}' \mbs{x}_{i} ) ]^{-1} }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Estimador Amostral}

\begin{align*}
\widehat{V} &=
\left( N^{-1} \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}
\left( N^{-1} \sum_{i=1}^{N} u_{i}^2 \mbs{x}_{i}' \mbs{x}_{i} \right)
\left( N^{-1} \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}
\\ &=
N
\left( \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}
\left( \sum_{i=1}^{N} u_{i}^2 \mbs{x}_{i}' \mbs{x}_{i} \right)
\left( \sum_{i=1}^{N} \mbs{x}_{i}' \mbs{x}_{i} \right)^{-1}
\\
\Aboxed{
\widehat{V} &=
N
\left( \Xmat'\Xmat \right)^{-1}
\left( \sum_{i=1}^{N} u_{i}^2 \mbs{x}_{i}' \mbs{x}_{i} \right)
\left( \Xmat'\Xmat \right)^{-1} }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
$\Var(\betahatbold)$ 

\vspace{-1 em}
\begin{align*}
\Var(\sqrt{N} \betahatbold) &= V
\\
\Var(\betahatbold) &= N^{-1} V
\\
\Aboxed{
\Var(\betahatbold) &= 
\left( \Xmat'\Xmat \right)^{-1}
\left( \sum_{i=1}^{N} u_{i}^2 \mbs{x}_{i}' \mbs{x}_{i} \right)
\left( \Xmat'\Xmat \right)^{-1} }.
\end{align*}

A variância \textbf{Robusta} é:

\vspace{-1 em}
\begin{align*}
\Aboxed{
\widehat{\Var}(\betahatbold) &= 
(\Xmat'\Xmat)^{-1} 
\left( \sum_{i=1}^{N} \widehat{u}_{i}^{2} \mbs{x}_{i}' \mbs{x}_{i} \right)
(\Xmat'\Xmat)^{-1} }.
\end{align*}

A variância sob \textbf{Homocedasticidade} é:

\vspace{-1 em}
\begin{align*}
	\Aboxed{\widehat{\Var}(\betahatbold) &= \widehat{\sigma}^{2} (\Xmat' \Xmat)^{-1}} .
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{System OLS (SOLS)}

\noindent
\citet[C.7 -- Estimating Systems of Equations by OLS and GLS, p.143--179]{wool-2010}\\
\citet[Sec.7.3 -- System OLS Estimation of a Multivariate Linear System, p.147]{wool-2010} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Preliminares}
\noindent
\citet[Sec.7.3.1]{wool-2010}

Assumimos que temos as seguintes observações \textit{cross section} $iid$:
$\seq{ (\Xmat_{i}, \mbs{y}_{i}): i=1, \dots, N}$, onde:

\vspace{-1 em}
\begin{itemize}[itemsep = -1ex]
\item [$\Xmat_{i}$]  é uma matriz $G \times K$ e contém as variáveis explicativas que aparecem em qualquer lugar do sistema.
\item [$\mbs{y}_{i}$]  é um vetor $G \times 1$, que contém as variáveis dependentes para todas as equações $G$ (ou períodos de tempo, no caso de dados de painel).
\end{itemize}

O modelo linear multivariado para uma \red{observação (draw)} aleatória da população pode ser expresso como:

\vspace{-1 em}
\begin{align}\label{mod:SOLS}
	\mbs{y}_{i} = \Xmat_{i} \betavec + \mbs{u}_{i} \, , \quad i=1, \dots, N,
\end{align}

\noindent
onde:

\vspace{-1 em}
\begin{itemize}[itemsep = -1ex]
\item [$\betavec$] é um vetor $K \times 1$ de parâmetros de interesse; e
\item [$\mbs{u}_{i}$] é um vetor $G \times 1$ de não observáveis.
\end{itemize}

A equação \eqref{mod:SOLS} explica as $G$ variáveis $y_{i1}, \dots, y_{iG}$ em termos de $\Xmat_{i}$ e das não observáveis $\mbs{u}_{i}$.
Por causa da hipótese de amostra aleatória podemos escrever tudo em temos de uma observação genérica.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Propriedades Assintóticas do SOLS}
\noindent
\citet[Sec.7.3.1]{wool-2010}

\begin{description}[itemsep = 0ex]
\item [SOLS.1] $\E(\Xmat_{i}' \mbs{u}_{i}) = 0$.
		
\item [SOLS.2] $A \equiv \E( \Xmat_{i}' \Xmat_{i} )$ é não singular (tem posto pleno, posto igual a $K$). 
\end{description}

A hipótese \textbf{SOLS.1} é a mais fraca que podemos impor num aracabouço de regressão para conseguirmos um estimador de $\betavec$ consistente.
Essa hipótese permite que alguns elementos de $\Xmat_{i}$ sejam correlacionados com elementos de $\mbs{u}_{i}$.
Uma hipótese mais forte seria:

\vspace{-1 em}
\begin{align} \label{cond:mean:0}
	\E(\mbs{u}_{i} | \Xmat_{i} ) = \mbs{0}
\end{align}

Sob \textbf{SOLS.1}, temos:

\vspace{-1 em}
\begin{align*} 
\E[ \Xmat_{i}' ( \mbs{y}_{i} - \Xmat_{i} \betavec ) ] &= \mbs{0}
\\
\E( \Xmat_{i}' \Xmat_{i} ) \betavec &= \E( \Xmat_{i}' \mbs{y}_{i} )  
\end{align*}

Para cada $i$, $\Xmat_{i} \mbs{y}_{i}$ é um vetor aleatório $K \times 1$ e $\Xmat_{i}'\Xmat_{i}$ é uma matriz $K \times K$ aleatória simétrica, positiva semidefinida.
Então, $\E(\Xmat_{i}' \Xmat_{i})$ é sempre uma matriz $K \times K$ não aleatória simétrica, positiva semidefinida.
Para conseguirmos estimar $\betavec$ precisamos assumir que ele é o único vetor $K \times 1$ que satisfaz $\E( \Xmat_{i}' \Xmat_{i} ) \betavec = \E( \Xmat_{i}' \mbs{y}_{i} )$.
Por isso assumimos \textbf{SOLS.2} e sob \textbf{SOLS.1} e \textbf{SOLS.2}, podemos escrever $\betavec$ como:

\vspace{-1 em}
\begin{align} \label{beta:SOLS}
	\Aboxed{
\betavec &=
\left[ \E( \Xmat_{i}' \Xmat_{i} )  \right]^{-1}
\E( \Xmat_{i}' \mbs{y}_{i} )  }
\end{align}

\noindent
o que mostra que \textbf{SOLS.1} e \textbf{SOLS.2} identifica o vetor $\betavec$.
O\textbf{princípio da analogia} sugere que estimemos $\betavec$ pelas analogias amostrais de \eqref{beta:SOLS}.
Assim, definimos o estimador SOLS de $\betavec$ como:

\vspace{-1 em}
\begin{align} \label{betahat:SOLS}
\Aboxed{
\betahatbold^{SOLS} &=
\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \mbs{y}_{i}   \right)
}.
\end{align}

Para computar $\betahatbold$ usando linguagem de computação é mais fácil utilizar a notação matricial

\vspace{-1 em}
\begin{align} \label{betahat:SOLS:mat}
\Aboxed{
\betahatbold^{SOLS} &=
\left(  \Xmat' \Xmat   \right)^{-1} \left(  \Xmat' \mbs{y}   \right)
}
\end{align}

\noindent
onde

\vspace{-1 em}
\begin{description}[itemsep = -1ex]
\item [$\Xmat \equiv (\Xmat_{1}', \dots, \Xmat_{N}')$]  é uma matriz $NG \times K$ dos $\Xmat_{i}$ empilhados.

\item [$\mbs{y} \equiv (\mbs{y}_{1}', \dots, \mbs{y}_{N}')$] é um vetor $NG \times 1$ das observações $\mbs{y}_{i}$ empilhadas.
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{SOLS para SUR} Estimação SOLS para um modelo SUR é equivalente a OLS equação a equação.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Consistência}

Para provarmos a \textbf{consistência} do estimador, usamos a equação \eqref{betahat:SOLS}:

\vspace{-1 em}
\begin{align*}
\betahatbold^{SOLS} &=
\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \mbs{y}_{i}   \right)
\\ &=
\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
\left[ N^{-1}\sum_{i=1}^{N} \Xmat_{i}' (\Xmat_{i} \betavec + \mbs{u}_{i})   \right]
\\ &=
\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i} \betavec    \right)
+
\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \mbs{u}_{i}   \right)
\\ &=
\betavec
+
\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \mbs{u}_{i}   \right)
\end{align*}

Por \textbf{SOLS.1},
$N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \mbs{u}_{i} \arrowp \mbs{0}$;
e por \textbf{SOLS.2}
$\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i} \right)^{-1} \arrowp A^{-1}$.

Resumimos esse resultado pelo seguinte Teorema:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{teo1}[Consistência do SOLS]\label{SOLS:const}
Sob Hipóteses \textbf{SOLS.1} e \textbf{SOLS.2}, temos 
\begin{align*}
\Aboxed{
	\betahatbold^{SOLS} \arrowp \betavec
}.
\end{align*}
\end{teo1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Normalidade Assintótica}

Para fazermos \textbf{Inferência}, precisamos achar a variância assintótica do estimador de OLS sob, essencialmente, as mesmas duas hipóteses. 
Tecnicamente, a seguinte derivação exige os elementos de
$\Xmat_{i}' \mbs{u}_{i} \mbs{u}_{i}' \Xmat_{i}$
tenham \textit{finite expected absolute value}.
De \eqref{betahat:SOLS} e \eqref{mod:SOLS}, escrevemos:

\vspace{-1 em}
\begin{align*} 
\betahatbold  &=
\betavec +
\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \mbs{u}_{i}   \right)
\\ 
(\betahatbold - \betavec) &= 
\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \mbs{u}_{i}   \right)
\\ 
\Aboxed{
\sqrt{N}(\betahatbold - \betavec) &= 
\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
\left( N^{-1/2} \sum_{i=1}^{N} \Xmat_{i}' \mbs{u}_{i}   \right)
}.
\end{align*}

Uma vez que $\E(\Xmat_{i}' \mbs{u}_{i})=0$, sob a hipótese \textbf{SOLS.1}, o CLT implica que:

\vspace{-1 em}
\begin{align*} 
N^{-1/2} \sum_{i=1}^{N} \Xmat_{i} \mbs{u}_{i} \arrowd N(\mbs{0}, B),
\end{align*}

\noindent
onde

\vspace{-1 em}
\begin{align*} 
B \equiv \E(\Xmat_{i}' \mbs{u}_{i} \mbs{u}_{i}' \Xmat_{i}) \equiv \Var(\Xmat_{i} \mbs{u}_{i}).
\end{align*}

\noindent
Em particular,

\vspace{-1 em}
\begin{align*} 
N^{-1/2} \sum_{i=1}^{N} \Xmat_{i} \mbs{u}_{i} = O_{p}(1).
\end{align*}

Porém,

\vspace{-1 em}
\begin{align*} 
	\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i} \right)^{-1} = (\Xmat'\Xmat/N)^{-1} = A^{-1} + o_{p}(1).
\end{align*}

\noindent
Sendo Assim,

\vspace{-1 em}
\begin{align*} 
\sqrt{N}(\betahatbold - \betavec) &= 
\left[ 
A^{-1} +
\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
- A^{-1}
\right]
\left( N^{-1/2} \sum_{i=1}^{N} \Xmat_{i}' \mbs{u}_{i}   \right)
\\ &=
A^{-1}\left( N^{-1/2} \sum_{i=1}^{N} \Xmat_{i}' \mbs{u}_{i}   \right)
+
[(\Xmat'\Xmat/N)^{-1} - A^{-1}]
\left( N^{-1/2} \sum_{i=1}^{N} \Xmat_{i}' \mbs{u}_{i}   \right)
\\&=
A^{-1}\left( N^{-1/2} \sum_{i=1}^{N} \Xmat_{i}' \mbs{u}_{i}   \right)
+ o_{p}(1) O_{p}(1)
\\&=
A^{-1}\left( N^{-1/2} \sum_{i=1}^{N} \Xmat_{i}' \mbs{u}_{i}   \right)
+ o_{p}(1)
\end{align*}

Portanto, com apenas \textit{single-equation OLS and 2SLS}, obtemos a representação assintótica para $\sqrt{N}(\betahatbold - \betavec)$ que é uma combinação linear não aleatória de somas parciais que satisfazem o CLT.
Usando o \red{lema de equivalência assintótica}, temos:

\begin{align*} 
\sqrt{N}(\betahatbold - \betavec)
\arrowd
N(\mbs{0}, A^{-1} B A^{-1})
\end{align*}

Resumimos esse resultado com o seguinte Teorema:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{teo1}[Normalidade Assintótica do SOLS]\label{teo:sols:norm}
Sob Hipóteses \textbf{SOLS.1} e \textbf{SOLS.2}, temos que a seguinte equação vale:

\begin{align} \label{eq:sols:norm}
	\Aboxed{
\sqrt{N}(\betahatbold - \betavec)
\arrowd
N(\mbs{0}, A^{-1} B A^{-1})
}.
\end{align}
\end{teo1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Variância Assintótica}

A variância assintótica de $\betahatbold^{SOLS}$ é:

\vspace{-1 em}
\begin{align}\label{eq:avar:sols}
	\Avar(\betahatbold^{SOLS}) = A^{-1} B A^{-1}/N.
\end{align}

Assim, $\Avar(\betahatbold^{SOLS})$ tende a zero a uma taxa $1/N$, como esperado.
Estimação consistente de $A$ é:

\vspace{-1 em}
\begin{align*}
	\widehat{A} \equiv \Xmat'\Xmat/N = N^{-1} \sum_{i=1}^{N} \Xmat_{i}'\Xmat_{i}
\end{align*}

Um estimador consistente para $B$ pode ser achado usando o princípio da analogia.

\vspace{-1 em}
\begin{align*}
B = \E(\Xmat_{i}' \mbs{u}_{i} \mbs{u}_{i}' \Xmat_{i}), 
\quad
N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \mbs{u}_{i} \mbs{u}_{i}' \Xmat_{i} \arrowp B.
\end{align*}

Uma vez que não podemos observar $\mbs{u}_{i}$, usamos os resíduos da estimação de SOLS:

\vspace{-1 em}
\begin{align*}
\widehat{\mbs{u}}_{i} \equiv \mbs{y}_{i} - \Xmat_{i} \betahatbold 
=
\ubold_{i} - \Xmat_{i} (\betahatbold - \betavec).
\end{align*}

Assim, definimos $\Bhat$ e usando LGN, podemos mostrar que:

\vspace{-1 em}
\begin{align*}
\Bhat \equiv N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \uhatbold_{i} \uhatbold_{i}' \Xmat_{i} 
\arrowp B.
\end{align*}

\noindent
onde supomos que certos momentos envolvendo $\Xmat_{i}$ e $\ubold_{i}$ são finitos.

Portanto, $\Avar[\sqrt{N}(\betahatbold - \betavec)]$ é \textbf{consistentemente} estimado por $\Ahat^{-1} \Bhat \Ahat^{-1}$, e $\Avar(\betahatbold)$ é estimado como:

\vspace{-1 em}
\begin{align*}
\Vhat \equiv 
\left( \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}  \right)^{-1}
\left( \sum_{i=1}^{N} \Xmat_{i}' \uhatbold_{i} \uhatbold_{i}'  \Xmat_{i}  \right)
\left( \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}  \right)^{-1}.
\end{align*}

Sob as hipóteses \textbf{SOLS.1} e \textbf{SOLS.2}, nós fazemos inferência em $\betavec$ como $\betahatbold$ fosse normalmente distribuído com média $\betavec$ e variância $\Vhat$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{SOLS para Dados de Painel}
\noindent
\citet[Sec.7.8 -- The Linar Panel Data Model, Revisited. p.169]{wool-2010} 

No caso de dados de painel:

\vspace{-1 em}
\begin{align*}
\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}
=
\sum_{i=1}^{N} \sum_{t=1}^{T} \mbs{x}_{it}' \mbs{x}_{it};
\quad
\sum_{i=1}^{N} \Xmat_{i}' \mbs{y}_{i}
=
\sum_{i=1}^{N} \sum_{t=1}^{T} \mbs{x}_{it}' y_{it}.
\end{align*}

Portanto, podemos escrever $\betahatbold$ como:

\vspace{-1 em}
\begin{align} \label{betahat:POLS}
\Aboxed{
\betahatbold^{POLS} =
\left( \sum_{i=1}^{N} \sum_{t=1}^{T} \mbs{x}_{it}' \mbs{x}_{it} \right)^{-1}
\left( \sum_{i=1}^{N} \sum_{t=1}^{T} \mbs{x}_{it}' y_{it} \right)
}.
\end{align}

Este estimador é chamado \textbf{estimador de Mínimos Quadrados Agrupados (POLS)} porque ele corresponde a rodar uma regressão OLS nas observações agrupadas através de $i$ e $t$. 
% This estimator is called the \textbf{pooled ordinary least squares (POLS) estimator} because it corresponds to running OLS oin the observation pooled across $i$ and $t$.
O estimador da equação \eqref{betahat:POLS} é o mesmo para unidades de \textit{cross section} amostradas em diferentes pontos do tempo.
O Teorema \ref{SOLS:const}, abaixo, mostraa que o estimador POLS é consistente sob as condições de ortogonalidade na hipótese \red{XX} e uma hipótese de posto completo.

$\E(\mbs{x}_{it})$

$\Xmat_{i}' \mbs{u}_{i} = \sum_{t=1}^{T} \mbs{x}_{it}' u_{it}$




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{System GLS (SGLS)}

\noindent
\citet[Sec.7.4 -- Consistency and Asymptotic Normality of Generalized Least Squares, p.153]{wool-2010} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Hipóteses}

Para implementarmos o estimador de \textbf{GLS} precisamos das seguintes hipótese:

\begin{enumerate}
\item %1

$\E(\Xmat_{i} \otimes \mbs{u}_{i}) = 0$.

Para SGLS ser consistente, precisamos que $\mbs{u}_{i}$ não seja correlacionada com nenhum elemento de $\Xmat_{i}$.

\item %2

$\Omega$ é positiva definida (para ter inversa).
$\E(\Xmat_{i}^{\prime} \Omega^{-1} \Xmat_{i})$ é \textbf{não} singular (para ter invesa).

Onde, $\Omega$ é a seguinte matriz \textbf{simétrica}, positiva-definida:

\vspace{-1.5 em}
\begin{align*}
\Omega = \E(\mbs{u}_{i} \mbs{u}_{i}^{\prime}).
\end{align*}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação}

Agora, transformamos o sistema de equações ao realizarmos a pré-multiplicação do sistema por $\Omega^{-1/2}$:

\vspace{-1.5 em}
\begin{align*}
\Omega^{-1/2} \mbs{y}_{i} 
&=
\Omega^{-1/2} \Xmat_{i} \mbs{\beta}
+
\Omega^{-1/2} \mbs{u}_{i}
\\
\mbs{y}_{i}^{*}
&=
\Xmat_{i}^{*} \mbs{\beta}
+
\mbs{u}^{*}_{i}
\end{align*}

Estimando a equação acima por \textbf{SOLS}:

\vspace{-1.5 em}
\begin{align*}
\beta^{SOLS}
&=
\left( \sum_{i=1} \Xmat_{i}^{*'} \Xmat_{i}^{*} \right)^{-1}
\left( \sum_{i=1} \Xmat_{i}^{*'} \mbs{y}_{i}^{*} \right)
\\
&=
\left( \sum_{i=1} \Xmat_{i}^{'} \Omega^{-1/2} \Omega^{-1/2} \Xmat_{i} \right)^{-1}
\left( \sum_{i=1} \Xmat_{i}^{'} \Omega^{-1/2} \Omega^{-1/2} \mbs{y}_{i} \right)
\\
&=
\left( \sum_{i=1} \Xmat_{i}^{'} \Omega^{-1} \Xmat_{i} \right)^{-1}
\left( \sum_{i=1} \Xmat_{i}^{'} \Omega^{-1} \mbs{y}_{i} \right)
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{GLS Factível}

\noindent
\citet[Sec.7.5 -- Feasible GLS, p.153]{wool-2010} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{FSGLS: SGLS Factível}

Para obtermos $\beta^{SGLS}$ precisamos conhecer $\Omega$, o que não ocorre na prática.
Então, precisamos estimar $\Omega$ com um estimador consistente.
Para tanto usamos um procedimento de dois passos:

\begin{enumerate}
\item  % Passo 1
Estimar $\mbs{y}_{i} = \Xmat_{i} \mbs{\beta} + \mbs{u}_{i}$ via \textbf{SOLS} e guardar o resíduo estimado $\widehat{\mbs{u}_{i}}$.

\item  %Passo 2
Estimar $\Omega$ com o seguinte estimador $\widehat{\Omega}$:

\vspace{-1.5 em}
\begin{align*}
	\widehat{\Omega} 
	= 
	N^{-1} \sum_{i=1}^{N} \mbs{u}_{i} \mbs{u}_{i}'
\end{align*}
\end{enumerate}

Com a estimativa $\widehat{\Omega}$ feita, podemos obter $\beta^{FSGLS}$ pela fórmula do $\beta^{SGLS}$:

\vspace{-1.5 em}
\begin{align*}
	\beta^{FGLS}
	= 
	\left[ 
		\sum_{i} \Xmat_{i}' \widehat{\Omega}^{-1} \Xmat_{i}
	\right]^{-1}
	\left[ 
		\sum_{i} \Xmat_{i}' \widehat{\Omega}^{-1} \mbs{y}_{i}
	\right]
\end{align*}

Empilhando as $N$ observações:

\vspace{-1.5 em}
\begin{align*}
\beta^{FGLS}
= 
\left[ \Xmat \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) \Xmat \right]^{-1}
\left[ \Xmat' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) \mbs{y} \right]
\end{align*}

Reescrevendo a equação acima:

\vspace{-1.5 em}
\begin{align*}
\beta^{FGLS}
&= 
\left[  \Xmat \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) \Xmat \right]^{-1}
\left[  \Xmat' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) (\Xmat \beta + u) \right]
\\
&= 
\left[ \Xmat \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) \Xmat \right]^{-1}
\left\{ 
\left[ \Xmat' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) \Xmat \beta \right]
% \\
% &
\; +
\left[ \Xmat' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) u \right]
\right\}
\\
&= 
\beta +
\left[ \Xmat \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) \Xmat \right]^{-1}
\left[ \Xmat' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) u \right]
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Valor Esperado}

\vspace{-1 em}
\begin{align*}
\E(\beta^{FGLS})
= 
\beta +
\left[ \Xmat \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) \Xmat \right]^{-1}
\left[ \Xmat' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) u \right]
\end{align*}

Concluímos que, se 
$\widehat{\Omega} \xrightarrow{\enskip p \enskip} \Omega$,
então,
$\beta^{FSGLS} \xrightarrow{\enskip p \enskip} \beta$,

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância}

\vspace{-1 em}
\begin{align*}
\Var(\beta^{FGLS})
&= 
\left[ \Xmat \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) \Xmat \right]^{-1}
\left[ \Xmat' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) u \right]
\left\{ 
\left[ \Xmat \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) \Xmat \right]^{-1}
\left[ \Xmat' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) u \right]
\right\}^{\prime}
\\
&=
\left[ \Xmat \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) \Xmat \right]^{-1}
\left[
\Xmat' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) 
u u'
\left( I_{N} \otimes \widehat{\Omega}^{-1} \right) \Xmat
\right]
\left[ \Xmat \left( I_{N} \otimes \widehat{\Omega}^{-1} \right) \Xmat \right]^{-1}
\end{align*}

Tirando o valor Esperado e supondo que:

\vspace{-1.5 em}
\begin{align*}
\E(\Xmat_{i} \Omega^{-1} u_{i} u_{i}' \Xmat_{i}) = \E(\Xmat_{i} \Omega^{-1})
\end{align*}
temos:

\vspace{-1.5 em}
\begin{align*}
\E\left[ \Xmat' \left( I_{N} \otimes \widehat{\Omega}^{-1} \right)
	u u'
\left( I_{N} \otimes \widehat{\Omega}^{-1} \right)' \Xmat \right]
=
\E(\Xmat' \Omega^{-1} \Xmat)
\end{align*}
e temos:

\vspace{-1.5 em}
\begin{align*}
	\Var(\beta^{FSGLS}) = \left[ \E(\Xmat' \Omega^{-1} \Xmat \right]^{-1}.
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Modelo de Efeitos Não Observados}
\citet[C.10 -- Basic Linear Unobserved Effects Panel Data Models]{wool-2010}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Endogeneity and GMM}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

No seguinte modelo \textit{cross-section}:

\vspace{-1 em}
\begin{align} \label{mod1}
	y_{i} = \beta_{0} + \beta_{1} x_{1i} + \beta_{2} x_{2i} + \err_{i}
	\; ; \quad i = 1, \dots, N.
\end{align}

\noindent
A variável explicativa $x_{k}$ é dita \textbf{endógena} se ela for correlacionada com erro.
Se $x_{k}$ for não correlacionada com o erro, então $x_{k}$ é dita \textbf{exógena}.

Endogeneidade surge, normalmente, de três maneiras diferentes:

\begin{enumerate}\itemsep0pt
	\item Variável Omitida;
	\item Simultaneidade;
	\item Erro de Medida.
\end{enumerate}

No modelo \eqref{mod1} vamos supor:

\begin{itemize}\itemsep0pt
	\item $x_{1}$ é exógena.
	\item $x_{2}$ é endógena.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Hipóteses}

Assim, precisamos encontrar um instrumento $z_{i}$ para $x_{2}$, uma vez que queremos estimar $\beta_{0}$, $\beta_{1}$ e $\beta_{2}$ de maneira consistente.
Para $z_{i}$ ser um bom instrumento precisamos que $z$ tenha:

\begin{enumerate}\itemsep0pt
\item $Cov(z, \err) = 0$ $\implies$  $z$ é exógena em \eqref{mod1}.
\item $Cov(z, x_{2}) \neq 0$ $\implies$  correlação com $x_{2}$ após controlar para outras vaariáveis.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação}

Indo para o problema de dados de painel, temos:

\vspace{-1 em}
\begin{align} \label{mod2}
	\mbs{y}_{i} = \Xmat_{i} \mbs{\beta} + \mbs{u}_{i}
	\; ; \quad i = 1, \dots, N.
\end{align}

\noindent
onde 
$\mbs{y}_{i}$ é um vetor $T \times 1$,
$\Xmat_{i}$ é uma matriz $T \times K$,
$\mbs{\beta}$ é o vetor de coeficientes $K \times 1$,
$\mbs{u}_{i}$ é o vetor de erros $T \times 1$.

Se é verdade que há endogeneidade em \eqref{mod2}, então:

\vspace{-1 em}
\begin{align*}
	\E(\Xmat_{i}^{\prime} \mbs{u}_{i}) \neq 0
\end{align*}

Definimos $Z_{i}$ como uma matriz $T \times L$ com $L \geq K$ de variáveis exógenas (incluindo o instrumento).
Queremos acabar com a endogeneidade, ou seja:

\vspace{-1 em}
\begin{align*}
	\E(Z_{i}^{\prime} \mbs{u}_{i}) = 0
\end{align*}

Supondo $L = K$ (apenas substituímos a variável endógena por um instrumento).

\vspace{-1 em}
\begin{align*}
\E[ Z_{i}^{\prime} ( \mbs{y}_{i} - \Xmat_{i} \mbs{\beta} ) ] &= 0
\\
\E( Z_{i}^{\prime} \mbs{y}_{i} ) - \E( Z_{i}^{\prime} \Xmat_{i} ) \mbs{\beta} &= 0
\\
\E( Z_{i}^{\prime} \mbs{y}_{i} ) &= \E( Z_{i}^{\prime} \Xmat_{i} ) \mbs{\beta}
\\
\Aboxed{
\mbs{\beta} &=
\left[ \E( Z_{i}^{\prime} \Xmat_{i} ) \right]^{-1}
\left[ \E( Z_{i}^{\prime} \mbs{y}_{i} ) \right]
}
\end{align*}

Se Usarmos estimadores amostrais:

\vspace{-1 em}
\begin{align*}
\mbs{\hat{\beta}} &=
\left[ N^{-1} \sum_{i=1}^{N} Z_{i}^{\prime} \Xmat_{i} \right]^{-1}
\left[ N^{-1} \sum_{i=1}^{N} Z_{i}^{\prime} \mbs{y}_{i} \right]
\\
\Aboxed{
\mbs{\hat{\beta}} &=
( Z^{\prime} \Xmat )^{-1} ( Z^{\prime} \mbs{y} ) }
\end{align*}

\vspace{1 em}
Se $L > K$, vamos considerar:

\vspace{-1 em}
\begin{align*}
\underset{\mbs{\beta}}{\text{Min}} \;
\E( Z_{i}\mbs{u}_{i} )^2
\end{align*}
\noindent onde:

\vspace{-1 em}
\begin{align*}
\E( Z_{i}\mbs{u}_{i} )^2 
&=
\E[ ( Z_{i}\mbs{u}_{i} )' ( Z_{i}\mbs{u}_{i} ) ]
=
( Z' \mbs{y} - Z' \Xmat \mbs{\beta} )' ( Z' \mbs{y} - Z' \Xmat \mbs{\beta} )
\\
&=
\mbs{y}' ZZ' \mbs{y}
-
\mbs{y}' ZZ' \Xmat \mbs{\beta}
-
\mbs{\beta}' \Xmat' ZZ' \mbs{y}
+
\mbs{\beta}' \Xmat' ZZ' \Xmat \mbs{\beta}
\end{align*}

Derivando em relação em $\mbs{\beta}$ e igualando a zero:

\vspace{-1 em}
\begin{align*}
-2 \mbs{y}' ZZ' \Xmat + 2 \mbs{\beta}'\Xmat' ZZ' \Xmat &= 0
\\
\mbs{\beta}'\Xmat' ZZ' \Xmat &= \mbs{y}' ZZ' \Xmat 
\\
\mbs{\beta}' &= ( \mbs{y}' ZZ' \Xmat ) ( \Xmat' ZZ' \Xmat )^{-1}
\\
\Aboxed{
\mbs{\beta} &= ( \Xmat' ZZ' \Xmat )^{-1} ( \Xmat' ZZ' \mbs{y} ) }
\end{align*}

Um estimador mais eficiente pode ser encontrado fazendo:

\vspace{-1 em}
\begin{align*}
\underset{\mbs{\beta}}{\text{Min}} \;
\E[ ( Z_{i}' \mbs{y} - Z' \Xmat \mbs{\beta} )' W ( Z_{i}' \mbs{y} - Z' \Xmat \mbs{\beta} ) ].
\end{align*}

\noindent
Escolhendo $\widehat{W}$, a priori, temos:

\vspace{-1 em}
\begin{align*}
\underset{\mbs{\beta}}{\text{Min}} \;
\left\{ 
\mbs{y}' Z \widehat{W} Z' \mbs{y}
-
\mbs{y}' Z \widehat{W} Z' \Xmat \mbs{\beta}
-
\mbs{\beta}' \Xmat'  Z \widehat{W} Z' \mbs{y}
+
\mbs{\beta}' \Xmat'  Z \widehat{W} Z' \Xmat \mbs{\beta}
\right\}
\end{align*}

Derivando em relação em $\mbs{\beta}$ e igualando a zero:

\vspace{-1 em}
\begin{align*}
-2 \mbs{y}' Z \widehat{W} Z' \Xmat + 2 \mbs{\beta}'\Xmat' Z \widehat{W} Z' \Xmat &= 0
\\
\mbs{\beta}'\Xmat' Z \widehat{W} Z' \Xmat &= \mbs{y}' Z \widehat{W} Z' \Xmat 
\\
\mbs{\beta}' &= ( \mbs{y}' Z \widehat{W} Z' \Xmat ) ( \Xmat' Z \widehat{W} Z' \Xmat )^{-1}
\\
\Aboxed{
\mbs{\beta}^{GMM} &= ( \Xmat' Z \widehat{W}' Z' \Xmat )^{-1} ( \Xmat' Z \widehat{W}' Z' \mbs{y} ) }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Valor Esperado} 

\vspace{-1 em}
\begin{align*}
\Aboxed{
\E( \mbs{\beta}^{GMM} ) &=
\mbs{\beta} +
\E[ ( \Xmat' Z \widehat{W}' Z' \Xmat )^{-1} ( \Xmat' Z \widehat{W}' Z' \mbs{u} ) ] }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância} 

\vspace{-1 em}
\begin{align*}
\Var( \mbs{\beta}^{GMM} ) &=
\E \left\{ 
\left[ ( X' Z \widehat{W}' Z' X )^{-1} ( X' Z \widehat{W}' Z' \mbs{u} ) \right]
\left[ ( X' Z \widehat{W}' Z' X )^{-1} ( X' Z \widehat{W}' Z' \mbs{u} ) \right]'
\right\}
\\ &=
\E \left\{ 
( X' Z \widehat{W}' Z' X )^{-1}
X' Z \widehat{W}' Z' \mbs{u} \mbs{u}' Z \widehat{W} Z' X 
( X' Z \widehat{W} Z' X )^{-1}
\right\}.
\end{align*}

\noindent
Definindo $\Delta = \E(Z' \mbs{u}\mbs{u}' Z)$ com $\Delta = W^{-1}$:

\vspace{-1 em}
\begin{align*}
\Var( \mbs{\beta}^{GMM} ) &=
\E \left\{ 
( X' Z \widehat{W}' Z' X )^{-1}
X' Z \widehat{W}' W^{-1} \widehat{W} Z' X 
( X' Z \widehat{W} Z' X )^{-1}
\right\}
\\ &=
\E \left\{ 
( X' Z \widehat{W}' Z' X )^{-1}
( X' Z \widehat{W}' Z' X )
( X' Z \widehat{W} Z' X )^{-1}
\right\}.
\\
\Aboxed{
\Var( \mbs{\beta}^{GMM} ) &=
\E \left[
( X' Z \widehat{W} Z' X )^{-1}
\right] }.
\end{align*}

\noindent
Se tivéssemos definido $W = (Z'Z)^{-1}$, teríamos $\beta^{2SLS}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Random Effects (RE, EA)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

O modelo linear de \textbf{efeitos não observados}:

\vspace{-1 em}
\begin{align} \label{mod1:EA}
	y_{it} = \mbs{x}_{it} \mbs{\beta} + c_{i} + u_{it},
\end{align}

\noindent
onde
$t = 1, \dots, T$ e $i = 1, \dots, N$.

O modelo contém explicitamente um componente não observado que não varia no tempo $c_{i}$.
Abordamos esse componente como parte do erro, não como parâmetro a ser estimado.
Para a análise de \textbf{Efeitos Aleatórios, (EA) ou (RE)}, supomos que os regressões $\mbs{x}_{it}$ são \textbf{não correlacionados} com $c_{i}$, mas fazemos hipóteses mais restritas que o \textbf{POLS}; pois assim exploramos a presença de \textbf{correlação serial} do erro composto por GLS e garantimos a consitência do estimador de FGLS.

Podemos reescrever \eqref{mod1:EA} como:

\vspace{-1 em}
\begin{align} \label{mod2:EA}
	y_{it} = \mbs{x}_{it} \mbs{\beta} + v_{it},
\end{align}

\noindent
onde
$t = 1, \dots, T$, $i = 1, \dots, N$ e $\boxed{v_{it} = c_{i} + u_{it}}$ é o erro composto.

Agora, vamos empilhar os $t$'s e reescrever \eqref{mod2:EA} como:

\vspace{-1 em}
\begin{align} \label{mod3:EA}
	\mbs{y}_{i} = X_{i} \mbs{\beta} + \mbs{v}_{i},
\end{align}

\noindent
onde
$i = 1, \dots, N$ e $\boxed{\mbs{v}_{i} = c_{i} \mbs{1}_{T} + \mbs{u}_{i}}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Hipóteses de $\mbs{\widehat{\beta}}^{RE}$}

As Hipóteses que usamos para $\mbs{\widehat{\beta}}^{RE}$ são:

\begin{enumerate} \itemsep0pt
\item  
Usamos o modelo correto e $c_{i}$ não é endógeno.

\begin{enumerate}[label =\alph*)]
\item 
	$\E( u_{it} \, | \,  x_{i1}, \dots, x_{iT}, c_{i} ) = 0$,
	$i = 1, \dots, N$.
\item        
	$\E( c_{it} \, | \, x_{i1}, \dots, x_{iT} ) = \E( c_{i} ) = 0$,
	$i = 1, \dots, N$.
\end{enumerate}

\item  Posto completo de $\E( X_{i}' \Omega^{-1} X_{i} )$.

Definindo a matriz $T \times T$, $\boxed{\Omega \equiv \E(\mbs{v}_{i} \mbs{v}_{i}')}$, queremos que $\E( X_{i} \Omega^{-1} X_{i} )$ tenha posto completo (posto = $K$).
\end{enumerate}

A matriz $\Omega$ é simétrica $\Omega' = \Omega$ e positiva definida $\det(\Omega) > 0$.
Assim podemos achar $\Omega^{1/2}$ e $\Omega^{-1/2}$ com $\Omega = \Omega^{1/2} \Omega^{1/2}$ e $\Omega^{-1} = \Omega^{-1/2} \Omega^{-1/2}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação}

Premultiplicando \eqref{mod3:EA} port $\Omega^{-1/2}$ do dois lados, temos:

\vspace{-1 em}
\begin{align} 
\notag
\Omega^{-1/2}\mbs{y}_{i} &= \Omega^{-1/2}X_{i} \mbs{\beta} + \Omega^{-1/2}\mbs{v}_{i}
\\
\label{mod4:EA}
\mbs{y}_{i}^{*} &= X_{i}^{*} \mbs{\beta} + \mbs{v}_{i}^{*},
\end{align}

Estimando o modelo acima por POLS:

\vspace{-1 em}
\begin{align} 
\notag
\mbs{\beta}^{POLS} &= 
\left( \sum_{i=1}^{N} X_{i}^{*}' X_{i}^{*} \right)^{-1}
\left( \sum_{i=1}^{N} X_{i}^{*}' \mbs{y}_{i}^{*} \right)
\\ \notag
&=
\left( \sum_{i=1}^{N} X_{i}' \Omega^{-1} X_{i} \right)^{-1}
\left( \sum_{i=1}^{N} X_{i}' \Omega^{-1} \mbs{y}_{i} \right)
\\ \label{beta:RE:1}
&=
\left( X' (I_{N} \otimes \Omega^{-1}) X \right)^{-1}
\left( X' (I_{N} \otimes \Omega^{-1}) \mbs{y} \right).
\end{align}

O problema, agora, é estimar $\Omega$.
Supondo:
\begin{itemize}\itemsep0pt
\item $\E(u_{it}u_{it}) = \sigma_{u}^{2}$;
\item $\E(u_{it}u_{is}) = 0$.
\end{itemize}
Como $\Omega = \E(\mbs{v}_{i} \mbs{v}_{i}') = \E[ ( c_{i} \mbs{1}_{T} + \mbs{u}_{i} ) ( c_{i} \mbs{1}_{T} + \mbs{u}_{i} )' ]$, temos que:

\vspace{-1 em}
\begin{align*} 
\E(v_{it}v_{it}) &=
	\E( c_{i}^{2} + 2c_{i} u_{it} + u_{it}^{2}) 
	=
	\sigma_{c}^{2} + \sigma_{u}^{2}
\\
\E(v_{it}v_{is})	&=
	\E[ ( c_{i} + u_{it} ) ( c_{i} + u_{is} ) ]
	=
	\E( c_{i}^{2} + c_{i} u_{is} + u_{it} c_{i} + u_{it} u_{is} )
	=
	\sigma_{c}^{2}.
\end{align*}

Assim, 

\vspace{-1 em}
\begin{align*}
\Omega 
= 
\E(\mbs{v}_{i} \mbs{v}_{i}') = \sigma^{2}_{u} I_{T} + \sigma_{c}^{2} \mbs{1}_{T} \mbs{1}_{T}'
\end{align*}

\noindent
onde
$\sigma^{2}_{u} I_{T}$ 
é uma matriz diagonal, e 
$\sigma_{c}^{2} \mbs{1}_{T} \mbs{1}_{T}'$ é uma matriz com todos os elementos iguais a $\sigma_{c}^{2}$.

Agora, rodando POLS em \eqref{mod3:EA} e guardando os resíduos, temos:

\vspace{-1 em}
\begin{align*}
\hat{v}_{it}^{POLS}
= 
\hat{y}_{it}^{POLS} - \mbs{x}_{it} \mbs{\hat{\beta}}^{POLS}
\end{align*}

\noindent
e conseguimos estimar $\sigma_{v}^{2}$ e $\sigma_{c}^{2}$ por estimadores amostrais:

\begin{itemize}\itemsep0pt
\item 
como $\sigma_{v}^{2} = \E(v_{it}^{2})$:

\vspace{-1.5 em}
\begin{align*}
\hat{\sigma}_{v}^{2} =
(NT - K)^{-1} 
\sum_{i=1}^{N}
\sum_{t=1}^{T}
\hat{v}_{it}^2
\end{align*}
\vspace{-1.5 em}

\item 
como $\sigma_{c}^{2} = \E(v_{it} v_{is})$:

\vspace{-1.5 em}
\begin{align*}
\hat{\sigma}_{c}^{2} =
\left[ N \frac{T ( T-1 )}{2} - K  \right]^{-1}
\sum_{i=1}^{N}
\sum_{t=1}^{T-1}
\sum_{s=t+1}^{T}
\hat{v}_{it} \hat{v}_{is}
\end{align*}
\vspace{-1.5 em}

\item $N$ indivíduos;

\item $T$ elementos da diagonal principal de $\Omega$

\item $\frac{T ( T - 1)}{2}$ elementos da matriz triangular superior dos elementos fora da diagonal.

\item $K$ regressores.
\end{itemize}

Agora que temos $\hat{\sigma}^2_{v}$ e $\hat{\sigma}^2_{c}$ podemos achar $\hat{\sigma}^{2}_{u}$ pela equação $\boxed{\hat{\sigma}_{u}^{2} = \hat{\sigma}_{v}^{2} - \hat{\sigma}_{c}^{2}}$.
Dessa forma, achamos os $T^2$ elementos de $\widehat{\Omega}$, e podemos escrever:

\vspace{-1 em}
\begin{align*}
\widehat{\Omega}
= 
\hat{\sigma}^{2}_{u} I_{T} + \hat{\sigma}_{c}^{2} \mbs{1}_{T} \mbs{1}_{T}'
\end{align*}

Com $\widehat{\Omega}$ estimado, reescrevemos \eqref{beta:RE:1} como:

\vspace{-1 em}
\begin{align} \label{beta:RE:2}
\mbs{\beta}^{RE} = 
\left[ X' (I_{N} \otimes \widehat{\Omega}^{-1}) X \right]^{-1}
\left[ X' (I_{N} \otimes \widehat{\Omega}^{-1}) \mbs{y} \right].
\end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Valor Esperado}

\vspace{-1 em}
\begin{align*}
	\Aboxed{
\E( \mbs{\beta}^{RE} ) = 
\mbs{\beta} +
\left[ X' (I_{N} \otimes \widehat{\Omega}^{-1}) X \right]^{-1}
\left[ X' (I_{N} \otimes \widehat{\Omega}^{-1}) \mbs{v} \right] }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância}

\vspace{-1 em}
\begin{align*} 
\Var( \mbs{\beta}^{RE} ) = 
E
\left\{ 
\left[ X' ( I_{N} \otimes \widehat{\Omega}^{-1} ) X \right]^{-1}
\left[
X' ( I_{N} \otimes \widehat{\Omega}^{-1} )
\mbs{v} \mbs{v}'
( I_{N} \otimes \widehat{\Omega}^{-1} )' X
\right]
\left[ X' ( I_{N} \otimes \widehat{\Omega}^{-1} ) X \right]
\right\},
\end{align*}

\noindent
como $\E( \mbs{v}_{i} \mbs{v}_{i}' ) =\Omega$,

\vspace{-1 em}
\begin{align*} 
	\Aboxed{
\Var( \mbs{\beta}^{RE} ) = 
E
\left[ X' ( I_{N} \otimes \widehat{\Omega}^{-1} ) X \right] }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Fixed Effects (EF, FE)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

O modelo linear de \textbf{efeitos não observados}:

\vspace{-1 em}
\begin{align} \label{mod1:FE}
	y_{it} = \mbs{x}_{it} \mbs{\beta} + c_{i} + u_{it},
\end{align}

\noindent
onde
$t = 1, \dots, T$ e $i = 1, \dots, N$.

O modelo contém explicitamente um componente não observado que não varia no tempo $c_{i}$.
Abordamos esse componente como parte do erro, não como parâmetro a não observado.
No caso da análise de \textbf{Efeitos Fixos (EF, FE)}, permitimos que esse componente $c_{i}$ seja correlacionado com $\mbs{x}_{it}$.
Assim, se decidíssemos estimar o modelo \eqref{mod1:FE} por POLS, ignorando $c_{i}$, teríamos problemas de inconsistência devido a \textbf{endogeneidade}.

As $T$ equações do modelo \eqref{mod1:FE} podem ser reescritas como:

\vspace{-1 em}
\begin{align} \label{mod2:FE}
	\mbs{y}_{i} = X_{i} \mbs{\beta} + c_{1} \mbs{1}_{T} + \mbs{u}_{i},
\end{align}

\noindent
com
$\mbs{v}_{i} = c_{i} \mbs{1}_{T} + \mbs{u}_{i}$ sendo os erros compostos.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Matriz $M^{0}$}

Definimos a matriz $M^{0}$ como:

\vspace{-1 em}
\begin{align*}
	M^{0} &=
	I_{T} - T^{-1} \mbs{1}_{T} \mbs{1}_{T}'
	=
	I_{T} - \mbs{1}_{T} (\mbs{1}_{T}' \mbs{1}_{T})^{-1} \mbs{1}_{T}'.
\end{align*}

\noindent
A matriz $M^{0}$ é idempotente e simétrica.

\begin{align*}
	M^{0} \mbs{x} &= \mbs{x} - \overline{\mbs{x}} \mbs{1}_{T}
	= \ddot{\mbs{x}}.
\end{align*}

Podemos transformar o modelo \eqref{mod2:FE} ao premultiplicarmos todo o modelo por $M^{0}$.

\vspace{-1 em}
\begin{align*} 
M^{0} \mbs{y}_{i} &= M^{0} X_{i} \mbs{\beta} + M^{0} ( c_{1} \mbs{1}_{T} ) + M^{0} \mbs{u}_{i},
\quad i = 1, \dots, N.
\end{align*}

\vspace{-1 em}
\begin{align*} 
M^{0} ( c_{1} \mbs{1}_{T} ) = 
( I_{T} - T^{-1} \mbs{1}_{T} \mbs{1}_{T}' ) c_{i} \mbs{1}_{T} 
=
c_{i} \mbs{1}_{T} - T^{-1} c_{i} \mbs{1}_{T} \mbs{1}_{T}' \mbs{1}_{T} 
=
c_{i} \mbs{1}_{T} - c_{i} \mbs{1}_{T} 
\implies
\boxed{ M^{0} ( c_{1} \mbs{1}_{T} ) = 0 }
\end{align*}

\vspace{-1 em}
\begin{align} \label{mod2:FE}
\ddot{\mbs{y}}_{i} &= \ddot{X}_{i} \mbs{\beta} + \ddot{\mbs{u}_{i}},
\quad i = 1, \dots, N.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação POLS}

Aplicando POLS no modelo \eqref{mod2:FE}

\vspace{-1 em}
\begin{align} \label{beta:pols:FE}
\Aboxed{
\mbs{\beta}^{FE} =
\left[ \sum_{i=1}^{N} \ddot{X}_{i}' \ddot{X}_{i} \right]^{-1}
\left[ \sum_{i=1}^{N} \ddot{X}_{i}' \ddot{\mbs{y}}_{i} \right]
}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Hipóteses}

As Hipóteses que usamos para $\mbs{\widehat{\beta}}^{FE}$ são:

\begin{description}
\setlength\itemsep{.5 em}

\item [FE.1:] Exogeneidade Estrita:
$\E( u _{it} \, | \, \mbs{x}_{i1}, \dots, \mbs{x}_{iT}, c_{i}) = 0$, para $t=1, \dots, T$ e $i = 1, \dots, N$.

\item  [FE.2:] Posto completo de $\E( X_{i}' \Omega^{-1} X_{i} )$ (para inverter a matriz).
$posto[ \E( X_{i}' \Omega^{-1} X_{i} ) ]  = K$.

\item [FE.3:] Homoscedasticidade:
	$\E(\mbs{u}_{i} \mbs{u}_{i}' \,|\, X_{i}, c_{i}) = \sigma_{u}^{2} I_{T}$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Valor Esperado}
Usando \textbf{FE.1} e \textbf{FE.2}, apenas.

\vspace{-1 em}
\begin{align*}
\E( \mbs{\beta}^{FE} ) &=
\mbs{\beta} +
\E\left[
\left( \sum_{i=1}^{N} \ddot{X}_{i}' \ddot{X}_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \ddot{X}_{i}' \ddot{\mbs{u}}_{i} \right)
\right]
\\
\Aboxed{
\E( \mbs{\beta}^{FE} ) &=
\mbs{\beta} +
\E \left[ ( \ddot{X}' \ddot{X} )^{-1} (\ddot{X}' \ddot{\mbs{u}}) \right]
}
\end{align*}

\noindent
Sabendo que 
$\ddot{X} = ( I_{N} \otimes M^{0} ) X$
e
$\ddot{\mbs{u}} = ( I_{N} \otimes M^{0} ) \mbs{u}$,
definimos:

\vspace{-1 em}
\begin{align*}
\E( \mbs{\beta}^{FE} ) &=
\mbs{\beta} +
\E \left\{
\left[  
X' ( I_{N} \otimes M^{0} )( I_{N} \otimes M^{0} ) X 
\right]^{-1}
\left[ 
X' ( I_{N} \otimes M^{0} )( I_{N} \otimes M^{0} ) \mbs{u}
\right]
\right\}
\\
\Aboxed{
\E( \mbs{\beta}^{FE} ) &=
\mbs{\beta} +
\E \left\{
\left[  
X' ( I_{N} \otimes M^{0} ) X 
\right]^{-1}
\left[ 
X' ( I_{N} \otimes M^{0} ) \mbs{u}
\right]
\right\} }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância}

Usamos a variância do estimador para inferência.
Usando \textbf{FE.1} e \textbf{FE.2}, apenas:

\vspace{-1 em}
\begin{align*} 
	\Aboxed{
\Var( \mbs{\beta}^{FE} ) = 
\E \left[
( \ddot{X}' \ddot{X} )^{-1}
(\ddot{X}' \ddot{\mbs{u}}) (\ddot{\mbs{u}}' \ddot{X} )
( \ddot{X}' \ddot{X} )^{-1} 
\right]}
\end{align*}


\begin{description}
\item [Pão:]
\begin{align*}
\E\left[ ( \ddot{X}' \ddot{X} )^{-1} \right] &=
\E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
\\ &=
\E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
\end{align*}

\item [Recheio:]
\begin{align*}
\E\left[
(\ddot{X}' \ddot{\mbs{u}}) (\ddot{\mbs{u}}' \ddot{X} ) 
\right] 
&=
\E \left[
X' ( I_{N} \otimes M^{0} ) ( I_{N} \otimes M^{0} ) 
\mbs{u} \mbs{u}'
( I_{N} \otimes M^{0} ) ( I_{N} \otimes M^{0} ) X
\right]
\\ &=
\E \left[
X' ( I_{N} \otimes M^{0} ) \mbs{u} \mbs{u}' ( I_{N} \otimes M^{0} ) X
\right]
\end{align*}
\end{description}

\vspace{-1 em}
\begin{align*} 
\Var( \mbs{\beta}^{FE} ) &= \text{Pão Recheio Pão}
\\ 
\Aboxed{
\Var( \mbs{\beta}^{FE} ) &= 
\E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
\E \left[
X' ( I_{N} \otimes M^{0} ) \mbs{u} \mbs{u}' ( I_{N} \otimes M^{0} ) X
\right]
\E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância sob Homocedasticidade}

Usando \textbf{FE.3}, temos

\begin{description}
\item [Recheio':]
\begin{align*}
\E \left[ X' ( I_{N} \otimes M^{0} ) \right]
\sigma^2_{u} I_{NT}
\E \left[ ( I_{N} \otimes M^{0} ) X \right]
=
\sigma^2_{u}
\E \left[ X' ( I_{N} \otimes M^{0} ) X \right]
\end{align*}
\end{description}

\noindent
\red{ $( I_{N} \otimes M^{0} )$ é uma matrix de dimensão $NT \times NT$, visto que $I_{N}$ é $N\times N$ e $M^{0}$ é $T \times T$.}

\vspace{-2 em}
\begin{align*}
\Var( \mbs{\beta}^{FE} ) &= \text{Pão Recheio' Pão} 
\\ &=
\E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
\sigma_{u}^{2} \E\left[ X' ( I_{N} \otimes M^{0} ) X \right]
\E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
\\  &=
\E \left\{ \left[
X' ( I_{N} \otimes M^{0} ) X
\right]^{-1} \right\}
\sigma_{u}^{2} I_{NT}
\\
\Aboxed{ \Var( \mbs{\beta}^{FE} ) &= \sigma_{u}^{2} \cdot  \E\left[ X' ( I_{N} \otimes M^{0} ) X \right] }
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{First Difference (FD, PD)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

O modelo linear de \textbf{efeitos não observados}:

\vspace{-1 em}
\begin{align} \label{mod1:FD}
	y_{it} = \mbs{x}_{it} \mbs{\beta} + c_{i} + u_{it},
\end{align}

\noindent
para
$t = 1, \dots, T$ e $i = 1, \dots, N$.

O modelo contém explicitamente um componente não observado, $c_{i}$, que não varia no tempo.
Tratamos o componente não observado como parte do erro, não como parâmetro a ser estimado.
Aqui permitimos que $c_{i}$ seja correlacionado com $\mbs{x}_{it}$.
Deste modo, \textbf{não} podemos ignorar a sua presença e estimar \eqref{mod1:FD} por POLS, visto que isso resultaria num estimador inconsistente devido a \textbf{endogeneidade}.

Assim, transformamos o modelo para eliminar $c_{i}$ e conseguirmos fazer uma estimação consistente de $\mbs{\beta}$.
A trasnformação a ser feita é a primeira diferença.
Para tanto, seguimos os seguintes passos:

\begin{itemize}\itemsep0pt
\item Reescrevemos \eqref{mod1:FD} defasado:

\vspace{-1 em}
\begin{align}  \label{mod2:FD}
	y_{it-1} = \mbs{x}_{it-1} \mbs{\beta} + c_{i} + u_{it-1}
\end{align}

\item Tiramos a diferença entre \eqref{mod2:FD} e \eqref{mod1:FD}:

\vspace{-1 em}
\begin{align}
\nonumber
y_{it} - y_{it-1} &=
(\mbs{x}_{it} - \mbs{x}_{it-1}) \mbs{\beta} +
c_{i} - c_{i} +
u_{it} - u_{it-1}
\\
\label{mod3:FD}
\Delta y_{it} &=
\Delta \mbs{x}_{it} \mbs{\beta} +
\Delta u_{it}. 
\end{align}

\noindent
para
$t = 2, \dots, T$ e $i = 1, \dots, N$.
\end{itemize}

Reescrevendo \eqref{mod3:FD} no formato matricial empilhando $T$:

\vspace{-1 em}
\begin{align} \label{mod4:FD}
	\Delta \mbs{y}_{i} = \Delta X_{i} \mbs{\beta} + \mbs{e}_{i}
\end{align}

\noindent
com 
$\boxed{e_{it} = \Delta u_{it}}$.

\begin{itemize}\itemsep0pt
\item
$\Delta \mbs{y}_{i}$ vetor $( T - 1 ) \times 1$ 
\item
$\Delta X_{i}$  matriz  $( T - 1 ) \times K$
\item
$\mbs{\beta}$ vetor $K \times 1$
\item
$\mbs{e}_{i}$ vetor $(T - 1 ) \times 1$
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação POLS}

O estimador $\widehat{\mbs{\beta}}^{FD}$ é o POLS da regressão no modelo \eqref{mod4:FD}, assim:

\vspace{-1 em}
\begin{align} \label{beta:pols:FD}
\Aboxed{
\mbs{\beta}^{FD} =
\left[ \sum_{i=1}^{N} \Delta X_{i}' \Delta X_{i} \right]^{-1}
\left[ \sum_{i=1}^{N} \Delta X_{i}' \Delta \mbs{y}_{i} \right]
}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Hipóteses}

As Hipóteses que usamos para $\mbs{\widehat{\beta}}^{FD}$ são:

\begin{description}
\setlength\itemsep{.5 em}

\item [FD.1:] Exogeneidade Estrita:
$\E( u _{it} \, | \, \mbs{x}_{i1}, \dots, \mbs{x}_{iT}, c_{i}) = 0$, para $t=1, \dots, T$ e $i = 1, \dots, N$.

\item  [FD.2:] Posto completo de $\E( \Delta X_{i}' \Delta X_{i} )$ (para inverter a matriz).
$posto[ \E( \Delta X_{i} ' \Delta X_{i} ) ]  = K$.

\item [FD.3:] Homoscedasticidade:
	$\E(\mbs{e}_{i} \mbs{e}_{i}' \,|\, X_{i}, c_{i}) = \sigma_{e}^{2} I_{T-1}$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Valor Esperado}
Usando apenas \textbf{FD.1} e \textbf{FD.2}:

\vspace{-1 em}
\begin{align*}
\E( \mbs{\beta}^{FD} ) &=
\mbs{\beta} +
\E\left[
\left( \sum_{i=1}^{N} \Delta X_{i}' \Delta X_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \Delta X_{i}' \mbs{e}_{i} \right)
\right]
\\
\Aboxed{
\E( \mbs{\beta}^{FD} ) &=
\mbs{\beta} +
\E \left[ ( \Delta X' \Delta X )^{-1} ( \Delta X' \mbs{e} \right]
}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância}

Usando apenas \textbf{FD.1} e \textbf{FD.2}:

\vspace{-1 em}
\begin{align*} 
\Aboxed{
\Var( \mbs{\beta}^{FD} ) = 
\E \left[
( \Delta X' \Delta X )^{-1}
( \Delta X' \mbs{e}  \mbs{e}' \Delta X )
( \Delta X' \Delta X )^{-1} 
\right]}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância sob Homocedasticidade}

Usando \textbf{FD.3}, temos

\vspace{-1 em}
\begin{align*} 
\Var( \mbs{\beta}^{FD} ) &= 
\sigma_{e}^{2}
\E \left[
( \Delta X' \Delta X )^{-1}
( \Delta X' \Delta X )
( \Delta X' \Delta X )^{-1} 
\right]
\\
\Aboxed{
\Var( \mbs{\beta}^{FD} ) &= 
\sigma^2_{e}
\E \left[
( \Delta X' \Delta X )^{-1} 
\right]}
\end{align*}

\noindent 
com

\vspace{-1 em}
\begin{align*} 
\sigma^2_{e} = 
\left[ N ( T - 1 ) - K \right]^{-1}
\left[  
\sum_{i=1}^{N} 
\sum_{t=1}^{T}
\hat{e}_{it}^{2}
\right],
\end{align*}

\noindent
que é a média de todos $\hat{e}^{2}_{it}$ contando $K$ regressores.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Exogeneidade Estrita e FDIV}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

No seguinte modelo

\vspace{-1 em}
\begin{align*} 
	y_{it} = \mbs{x}_{it} \mbs{\beta} + u_{it},
\end{align*}

\noindent
para
$t = 1, \dots, T$ e $i = 1, \dots, N$.

\begin{itemize}\itemsep0pt
\item
$y_{it}$ escalar;

\item
$\mbs{x}_{it}$  vetor $1 \times K$;

\item
$\mbs{\beta}$ vetor $K \times 1$;

\item
$u_{it}$ escalar.
\end{itemize}

\noindent
$\{x_{it}\}$ é estritamente \textbf{exógeno} se valer:

\vspace{-1 em}
\begin{align*}
	\E( u_{it} \, | \, \mbs{x}_{i1}, \dots, \mbs{x}_{iT} ) = 0 \; , \qquad t = 1, \dots, T
\end{align*}

\noindent
ou seja:

\vspace{-1 em}
\begin{align*}
\E( y_{it} \, | \, \mbs{x}_{i1}, \dots, \mbs{x}_{iT} ) = \mbs{x}_{it} \mbs{\beta} 
\; , \qquad t = 1, \dots, T
\end{align*}

\noindent
o que é equivalente a hipótese de que utilizamos o modelo linear correto.

Para o seguinte modelo:

\vspace{-1.5 em}
\begin{align*}
y_{it} = \mbs{z}_{it} \mbs{\gamma} + \rho y_{i t - 1} + c_{i} + u_{it}.
\; , \qquad t = 2, \dots, T
\end{align*}

\noindent
é \textbf{impossível} termos exogeneidade estrita.
Isso porque, nesse modelo, de efeitos não observados temos:

\vspace{-1.5 em}
\begin{align*}
	\E( y_{it} \, | \, \mbs{z}_{i1}, \dots, \mbs{z}_{iT}, y_{it-1}, c_{i}) \neq 0.
\end{align*}

\noindent
Isso ocorre porque, $y_{it}$ é afetado por $y_{it-1}$ que contribui para $y_{it}$ com, pelo menos, $\rho c_{i}$.

\begin{equation*}
\left.
\begin{aligned}
y_{it} &= \mbs{z}_{it} \mbs{\gamma} + \rho y_{i t - 1} + c_{i} + u_{it}
\\
y_{it-1} &= \mbs{z}_{it-1} \mbs{\gamma} + \rho y_{i t - 2} + c_{i} + u_{it-1}
\end{aligned}
\right\} 
\implies
y_{it} = \mbs{z}_{it} \mbs{\gamma} +
\rho (\mbs{z}_{it-1} \mbs{\gamma} + \rho y_{i t - 2} + c_{i} + u_{it-1})
+ c_{i} + u_{it}.
\end{equation*}

Para eliminarmos este efeito, podemos tirar a primeira diferença do modelo:

\vspace{-1 em}
\begin{align}
\nonumber
y_{it} - y_{it-1} &= 
(\mbs{z}_{it} - \mbs{z}_{it-1}) \mbs{\gamma} +
\rho (y_{i t - 1} -  y_{i t - 2} ) +
(c_{i} - c_{i}) + (u_{it} - u_{it-1})
\\
\label{mod1:FDIV}
\Aboxed{
\Delta y_{it} &= 
\Delta \mbs{z}_{it} \mbs{\gamma} + \rho \Delta y_{i t - 1} + \Delta u_{it}
\, , \qquad t=3, \dots, T}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação}

Não podemos estimar o modelo \eqref{mod1:FDIV} por POLS, uma vez que $Cov(\Delta y_{it-1}, \Delta u_{it} ) \neq 0$.
Como saída, podemos estimar por P2SLS, usando instrumentos para $\Delta y_{it-1}$ (alguns intrumentos para $\Delta y_{it-1}$ são $y_{it-2}, y_{it-3}, \dots, y_{i1}$).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{P2SLS}

\vspace{-1 em}
\begin{align*}
	y_{it} = \mbs{x}_{it}' \mbs{\beta} + u_{it}
\end{align*}

\begin{itemize}\itemsep0pt
\item $i = 1, \dots, N$
\item $t = 1, \dots, T$
\item $y_{it}$ escalar;
\item $\mbs{x}_{it}$  vetor $K \times 1$;
\item $\mbs{\beta}$ vetor $K \times 1$;
\item $u_{it}$ escalar.
\end{itemize}

\vspace{-1 em}
\begin{align*}
\boxed{
\mbs{\beta}^{P2SLS} =  ( X' P_{Z} X )^{-1} ( X' P_{Z} \mbs{y} ) }
\end{align*}

\noindent
com

\vspace{-1 em}
\begin{align*}
\boxed{P_{Z} = Z'(Z'Z)^{-1}Z }
\end{align*}

\noindent
onde
$P_{Z}$ é a matriz de projeção em $Z$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{FDIV}

\vspace{-1 em}
\begin{align*}
y_{it} &= \mbs{x}_{it}' \mbs{\beta} + c_{i} + u_{it}
\; , \quad i = 1, \dots, N
\; , \quad t = 1, \dots, T
\\
\Delta y_{it} &= \Delta \mbs{x}_{it}' \mbs{\beta} + \Delta u_{it}
\; , \quad i = 1, \dots, N
\; , \quad t = 2, \dots, T
\end{align*}

Vamos supor $\Delta x_{it}'$ tem variável endógena ($y_{it}$, no caso).
$\mbs{w}_{it}$ é um vetor $1 \times L_{t}$ de instrumentos, onde $L_{t} \geq K$.
Se os instrumentos forem diferentes:

\vspace{-1 em}
\begin{align*}
	W_{i} = diag( \mbs{w}_{i2}', \mbs{w}_{i3}', \dots, \mbs{w}_{iT}')
\end{align*}

\noindent
onde $W_{i}$ é uma matriz $( T - 1 ) \times L$

\vspace{-1 em}
\begin{align*}
	L = L_{2} + L_{3} + \dots + L_{T}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Hipóteses}

\begin{description}
\item[FDIV.1:] $\E( \mbs{w}_{it} \Delta u_{it}')$ para $i = 1, \dots, N$, $t = 2, \dots, T$.
\item[FDIV.2:] $Posto\left[ \E( W_{i}' W_{i} ) \right] = L$
\item[FDIV.3:] $Posto\left[ \E( W_{i}' \Delta X_{i} ) \right] = K$
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Estimação FDIV}

\vspace{-1 em}
\begin{align*}
\boxed{
\mbs{\beta}^{FDIV} =  
\left(
\Delta X' P_{W} \Delta X 
\right)^{-1}
\left(
\Delta X' P_{W} \Delta \mbs{y}
\right)
}
\qquad
\boxed{
P_{W} = W(W'W)^{-1}W'}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Valor Esperado}

\vspace{-1 em}
\begin{align*}
\E( \mbs{\beta}^{FDIV} ) =  
\beta + 
\left( \Delta X' P_{W} \Delta X \right)^{-1}
\left( \Delta X' P_{W} \mbs{e} \right)
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Variância}

\vspace{-1 em}
\begin{align*}
\Var( \mbs{\beta}^{FDIV} ) &=
\E\left\{  
\left[ \E( \mbs{\beta}^{FDIV} ) - \beta \right] 
\left[ \E( \mbs{\beta}^{FDIV} ) - \beta \right]'
\right\}
\\
&=
\E\left\{  
\left[ \Delta X' P_{W} \Delta X \right]^{-1}
\left[ \Delta X' P_{W} \mbs{e} \right]
\left[ \Delta X' P_{W} \mbs{e} \right]'
\left[ \Delta X' P_{W} \Delta X \right]^{-1}
\right\}
\\
&=
\E\left[
\left( \Delta X' P_{W} \Delta X \right)^{-1}
\left( \Delta X' P_{W} \mbs{e} \mbs{e}' P_{W} \Delta X \right)
\left( \Delta X' P_{W} \Delta X \right)^{-1}
\right]
\end{align*}

\noindent
$e_{i} = \Delta u_{it}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Latent Variables, Probit and Logit}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

Suponha $y^{*}$ não observável (\textbf{latente}) seguindo o seguinte modelo:

\vspace{-1 em}
\begin{align} \label{mod1:probit}
	y_{i}^{*} = \mbs{x}_{i}' \mbs{\beta} + \err_{i}.
\end{align}

\noindent
Defina $y$ como:

\vspace{-1 em}
\begin{align*}
y_{i} =
\begin{cases}
	1 \, , \quad y^{*}_{i} \geq 0
\\
	0 \, , \quad y^{*}_{i} < 0
\end{cases}
\end{align*}

\noindent
temos que:

\vspace{-1 em}
\begin{align*}
	P( y_{i} = 1 | \mbs{x} ) &= p( \mbs{x} )
	\\
	P( y_{i} = 0 | \mbs{x} ) &= 1 - p( \mbs{x} ).
\end{align*}

Além disso, pela definição de $y_{i}$, equação \eqref{mod1:probit}, temos:

\vspace{-1 em}
\begin{align*}
	P( y_{i} = 1 | \mbs{x} ) &= P(y_{i}^{*} \geq 0 \, | \mbs{x} )
\\
&= P( \mbs{x}_{i}' \mbs{\beta} + \err_{i} \geq 0 \, | \mbs{x} )
\\
&= P( \err_{i} \geq - \mbs{x}_{i}' \mbs{\beta}  \, | \mbs{x} ).
\end{align*}

\noindent
Agora, supondo que $\err_{i}$ tem FDA, $G$, tal que $G'=g$ é simétrica ao redor de zero:

\vspace{-1 em}
\begin{align*}
P( y_{i} = 1 | \mbs{x} ) 
&= 1 - P( \err_{i} < - \mbs{x}_{i}' \mbs{\beta}  \, | \mbs{x} )
\\
&= 1 - G( - \mbs{x}_{i}' \mbs{\beta}  \, | \mbs{x} )
\\
&= G( \mbs{x}_{i}' \mbs{\beta} ).
\end{align*}

Se $G(\cdot)$ for uma distribuição:

\begin{description}
	\item [Normal Padrão:] $\hat{\mbs{\beta}}$ é o estimador \textbf{probit}.
	\item [Logística:] $\hat{\mbs{\beta}}$ é o estimador \textbf{logit}.
\end{description}

Supondo $\mbs{y}_{i} \, | \, \mbs{x} \sim Bernoulli(p(\mbs{x}))$, sua fmp é dada por:

\vspace{-1 em}
\begin{align*}
f( y_{i} \, | \, \mbs{x}_{i} ; \mbs{\beta} ) 
&= 
\left[ G(\mbs{x}_{i}' \mbs{\beta} )  \right]^{y_{i}}
\left[ 1 - G(\mbs{x}_{i}' \mbs{\beta} )  \right]^{1 - y_{i}}
\; , \quad y=0,1.
\end{align*}

Para estimarmos $\hat{\mbs{\beta}}$ por máxima verossimilhança, temos de encontrar $\mbs{\beta} \in B$, onde $B$ é o espaço paramétrico, tal que $\mbs{\beta}$ maximize o valor da distribuição conjunta de $\mbs{y}$, ou seja:

\vspace{-1 em}
\begin{align*}
	\underset{\mbs{\beta} \in B}{\text{Max }} 
	\prod_{i=1}^{N}
	f( y_{i} \, | \, \mbs{x}_{i} ; \mbs{\beta} ).
\end{align*}

\noindent

Tirando o logaritmo e dividindo tudo por $N$ (podemos fazer isso pois são transformações monotônicas e não alteram o lugar onde $\mbs{\beta}$ ótimo irá parar):

\vspace{-1 em}
\begin{align*}
\underset{\mbs{\beta} \in B}{\text{Max }} 
\left\{ 
N^{-1} \sum_{i=1}^{N}
\ln \left[ f( y_{i} \, | \, \mbs{x}_{i} ; \mbs{\beta} ) \right]
\right\}.
\end{align*}

\noindent
Podemos definir
$\ell_{i}( \mbs{\beta} ) = \ln[ f( y_{i} \, | \, \mbs{x}_{i} ; \mbs{\beta} ) ]$
como sendo a verossimilhança condicional da observação $i$:

\vspace{-1 em}
\begin{align*}
\underset{\mbs{\beta} \in B}{\text{Max }} 
\left\{ 
N^{-1} \sum_{i=1}^{N} \ell_{i} (\mbs{\beta})
\right\}.
\end{align*}

Dessa forma, podemos ver que o problema acima é a analogia amostral de:

\vspace{-1 em}
\begin{align*}
\underset{\mbs{\beta} \in B}{\text{Max }} 
\E \left[ 
\ell_{i} ( \mbs{\beta} )
\right].
\end{align*}

Definindo o \textit{vector score} da observação $i$:

\vspace{-1 em}
\begin{align*}
s_{i} (\mbs{\beta}) = 
\left[ \nabla_{\mbs{\beta}} \ell_{i} (\mbs{\beta}) \right]'
=
\begin{bmatrix}
	\dfrac{\partial{\ell_{i} (\mbs{\beta})}}{\partial{\beta_{1}}},
	\dots,
	\dfrac{\partial{\ell_{i} (\mbs{\beta})}}{\partial{\beta_{K}}}
\end{bmatrix}
\end{align*}

Definindo a \textbf{Matriz Hessiana} da observação $i$:

\vspace{-1 em}
\begin{align*}
H_{i} (\mbs{\beta}) = 
\nabla_{\mbs{\beta}} s_{i} (\mbs{\beta}) = 
\nabla_{\mbs{\beta}}^2 \ell_{i} (\mbs{\beta})
\end{align*}

Tendo essas definições, o \textbf{Teorema do Valor Médio} (TVM) nos diz que no intervalo $[a, b]$, existe um número, $c$, tal que:

\vspace{-1 em}
\begin{align*}
	f'(c) = \frac{f(b) - f(a)}{b - a}.
\end{align*}

\begin{center}
\red{FAZER DESENHO}
\end{center}

Trocando 
$f(\cdot)$ por $s_{i}(\cdot)$, 
$a$ por $\mbs{\beta}_{0}$, 
$b$ por $\widehat{\mbs{\beta}}$ e
$c$ por $\bar{\mbs{\beta}}$,
temos:

\vspace{-1 em}
\begin{align*}
H_{i} ( \bar{\mbs{\beta}} ) =
\frac{s_{i}( \widehat{\mbs{\beta}} ) - s_{i}( \mbs{\beta}_{0} )}{\widehat{\mbs{\beta}} - \mbs{\beta}_{0}},
\end{align*}

\noindent
tirando médias dos dois lados:

\vspace{-1 em}
\begin{align*}
N^{-1} \sum_{i=1}^{N} 
H_{i} ( \bar{\mbs{\beta}} ) 
=
\frac{1}{\widehat{\mbs{\beta}} - \mbs{\beta}_{0}}
N^{-1} \sum_{i=1}^{N} 
\left[ 
s_{i}( \widehat{\mbs{\beta}} ) - s_{i}( \mbs{\beta}_{0} )
\right]
\end{align*}

Supondo que
$\widehat{\mbs{\beta}}$
maximiza
$\ell (\mbs{\beta} \, | \, \mbs{y}, \mbs{x})$,
temos que:
$N^{-1} \sum_{i=1}^{N} s_{i}(\widehat{\mbs{\beta}}) = 0$.
E podemos reescrever a equação anterior como:

\vspace{-1 em}
\begin{align*}
\widehat{\mbs{\beta}} - \mbs{\beta}_{0}
&=
(-1)
\left[ N^{-1} \sum_{i=1}^{N} H_{i} ( \bar{\mbs{\beta}} ) \right]^{-1}
N^{-1} \sum_{i=1}^{N} s_{i}( \mbs{\beta}_{0} ) 
\\
\sqrt{N} ( \widehat{\mbs{\beta}} - \mbs{\beta}_{0} )
&=
\left[
- N^{-1} \sum_{i=1}^{N} H_{i} ( \bar{\mbs{\beta}} )
\right]^{-1}
\sqrt{N} \cdot N^{-1} \sum_{i=1}^{N} s_{i}( \mbs{\beta}_{0} ) 
\\
\Aboxed{
\sqrt{N} ( \widehat{\mbs{\beta}} - \mbs{\beta}_{0} )
&=
\left[
- N^{-1} \sum_{i=1}^{N} H_{i} ( \bar{\mbs{\beta}} )
\right]^{-1}
N^{-1/2} \sum_{i=1}^{N} s_{i}( \mbs{\beta}_{0} ) }.
\end{align*}

\noindent
Onde

\vspace{-1 em}
\begin{align*}
\left[ 
- N^{-1} \sum_{i=1}^{N}
H_{i} ( \bar{\mbs{\beta}} ) \right]^{-1}
\xrightarrow{p}
A_{0}^{-1} \, ,
&&
N^{-1/2} \sum_{i=1}^{N} s_{i}( \mbs{\beta}_{0} ) 
\xrightarrow{d}
N( 0, B_{0} ).
\end{align*}

\noindent
Assim, temos que:

\vspace{-1 em}
\begin{align*}
\Aboxed{
\sqrt{N} ( \widehat{\mbs{\beta}} - \mbs{\beta}_{0} )
\to
N ( 0, A_{0}^{-1} B_{0} A_{0}^{-1} )}.
\end{align*}

A forma mais simples de achar $\Var ( {\widehat{\mbs{\beta}}} )$ é:

\vspace{-1 em}
\begin{align*}
	\Aboxed{
\Var( \widehat{\mbs{\beta}} )
&=
- \E[ H_{i} ( \widehat{\mbs{\beta}} ) ]^{-1} }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{ATT, ATE, Propensity Score}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Modelo}

\begin{itemize}\itemsep0pt
\item
$y_{1}$ $\rightarrow$ variável de interesse com tratamento

\item
$y_{0}$ $\rightarrow$ variável de interesse sem tratamento
\end{itemize}

\vspace{-1 em}
\begin{align*}
w = 
\begin{cases}
1 & \text{se tratam}
\\
0 & \text{se não tratam}
\end{cases}
\end{align*}
 
Idealmente, para isolarmos completamente o efeito de $w=1$, gostaríamos de pode calcular:

\vspace{-1 em}
\begin{align*}
	N^{-1} \sum_{i=1}^{N}
	\left( y_{i1} - y_{i0} \right).
\end{align*}

Ou seja, o efeito que o tratamento causa sobre um indivíduo com todo o resto permanecendo constante.
Em outras palavras, queríamos que houvesse dois mundos paralelos observáveis onde seria possível observar o que acontece com $y_{i}$ com e sem tratamento.
Infelizmente, para ccada indivíduo $i$, observamos apenas $y_{i1}$ ou $y_{i0}$, nunca ambos.

Antes de continuarmos, faremos as seguintes definições:

\begin{description}
	\item[ATE:]  $\E( y_{1} - y_{0} )$
	\item[ATT:]  $\E( y_{1} - y_{0} \, | \, w = 1 )$ (ATE no tratado).
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{$ATE$ e $ATT$ condicional a variáveis $\mbs{x}$ }

\vspace{-1 em}
\begin{align*}
ATE( \mbs{x} ) &= \E( y_1 - y_0 \, | \mbs{x})
\\
ATT( \mbs{x} ) &= \E( y_1 - y_0 \, | \mbs{x}, w = 1)
\end{align*}

\noindent
\underline{OBS:}

\vspace{-1 em}
\begin{align*}
\E( y_1 - y_0 ) &= \E \left[ \E( y_1 - y_0 \, | w) \right]
\\
\E( y_1 - y_0 \, | w ) &=
\E( y_1 - y_0 \, | w = 0 ) \cdot P(w=0)
+
\E( y_1 - y_0 \, | w = 1 ) \cdot P(w=1).
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Métodos Assumindo Ignorabilidade do Tratamento}

\begin{description}
\item[ATE.1:] Ignorabilidade. 
\\
$w$ e $(y_{1}, y_{0})$ são independentes condicionais a $\mbs{x}$.

\item[ATE.1':] Ignorabilidade da Média. 

\vspace{-.75 em}
\begin{enumerate}[label =\alph*)] \itemsep0pt
\item $\E( y_{0} \, | \, w, \mbs{x} ) = \E( y_{0} \, | \, \mbs{x} )$
\item $\E( y_{1} \, | \, w, \mbs{x} ) = \E( y_{1} \, | \, \mbs{x} )$
\end{enumerate}

\end{description}

Vamos definir

\vspace{-1 em}
\begin{align*}
\E( y_{0} \, | \, \mbs{x} ) &= \mu_{0}( \mbs{x} )
\\
\E( y_{1} \, | \, \mbs{x} ) &= \mu_{1}( \mbs{x} ).
\end{align*}

Sob \textbf{ATE.1} e \textbf{ATE.1'}:

\vspace{-1 em}
\begin{align*}
	ATE( \mbs{x} ) &= \E( y_{1} - y_{0} \, | \mbs{x} ) = \mu_{1}(\mbs{x}) - \mu_{0}(\mbs{x}) 
	\\
	ATT( \mbs{x} ) &= \E( y_{1} - y_{0} \, | \mbs{x}, w=1 ) = \mu_{1}(\mbs{x}) - \mu_{0}(\mbs{x}) 
\end{align*}

\begin{description}
\item[ATE.2:] \textit{Overlap} \\
Para todo $\mbs{x}$, $P(w=1 \, | \, \mbs{x} ) \in ( 0, 1 )$, 
$p(\mbs{x}) = p(w=1 | \mbs{x})$.
\end{description}

$p(\mbs{x})$ é o \textit{Propensity Score}, ele representa a probabilidade de $y_{i}$ ser tratado dado o valor das covariáveis $\mbs{x}$.
Essa hipótese é importante visto que podemos expressar o $ATE$ em função de $p(\mbs{x})$.

\vspace{1 em}
Para o $ATT$ vamos supor:

\begin{description}
\item[ATT.1':] 
	$\E( y_{0} \, | \mbs{x}, w ) = \E( y_{0} \, | \, \mbs{x} )$

\item[ATT.2:] \textit{Overlap:} Para todo $\mbs{x}$, $P(w=1 | \mbs{x} ) < 1$.
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Propensity Score}

Como foi dito anteriormente, apenas observamos ou $y_{1}$ ou $y_{0}$ para a mesma pessoa, mas não ambos.
Mais precisamente, junto com $w$, o resultado observado é:

\vspace{-1 em}
\begin{align*}
	y = wy_{1} + (1 - w) y_{0}
\end{align*}

\noindent
como  $w$ é binário, $w^2 = w$, assim, temos:

\vspace{-1 em}
\begin{align*}
w y &= w^{2} y_{1} + (w - w^{2}) y_{0}
\implies
\boxed{w y = w y_{1} }
\\
( 1 - w ) y &= (w - w^{2}) y_{1} + ( w^{2} - 2w + 1 ) y_{0}
\implies
\boxed{( 1 - w ) y = (1 - w) y_{0}}.
\end{align*}

Fazemos isso para tentar isolar $\mu_{0}(\mbs{x})$ e $\mu_{1}(\mbs{x})$:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{$\mu_{1}( \mbs{x} )$}

\begin{align*}
\E( w y | \mbs{x} ) &= \E\left[  \E \left( w y_{1} | \mbs{x}, w  \right) | \mbs{x} \right]
\\ &=
\E \left[ w \mu_{1}(\mbs{x}) | \mbs{x} \right]
\\ &=
\mu_{1}(\mbs{x}) \E(w | \mbs{x} ).
\end{align*}

\noindent
Como $w$ é binaria: $\E(w| \mbs{x}) = P(w=1 | \mbs{x}) = p(\mbs{x})$.
Assim:

\vspace{-1 em}
\begin{align*}
\E( w y | \mbs{x} ) &= \mu_{1}(\mbs{x}) p(\mbs{x})
\\
\Aboxed{ \mu_{1}(\mbs{x}) &= \frac{\E(w y | \mbs{x})}{p(\mbs{x})} }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{$\mu_{0}( \mbs{x} )$}

\vspace{-1 em}
\begin{align*}
	\E[ (1-w) y | \mbs{x} ] &= \E\left[  \E \left( (1 - w) y_{0} | \mbs{x}, w  \right) | \mbs{x} \right]
\\ &=
\E \left[ (1 - w) \mu_{0}(\mbs{x}) | \mbs{x} \right]
\\ &=
\mu_{0}(\mbs{x}) \E(w | \mbs{x} )
\\ 
\E[ (1-w) y | \mbs{x} ] 
&=
\mu_{0}(\mbs{x}) [1 - p(\mbs{x})] \implies
\\ 
\Aboxed{
\mu_{0}(\mbs{x})
&=
\frac{\E[ (1-w) y | \mbs{x} ] }{1 - p( \mbs{x} ) } }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{ATE:}

\begin{align*}
\mu_{1}(\mbs{x}) - \mu_{0}(\mbs{x}) =
\E\left[ 
\frac{[w - p(\mbs{x})] y}{p(\mbs{x}) [1 - p(\mbs{x})]}
| \mbs{x}
\right]
\end{align*}

\begin{align*}
\Aboxed{
\widehat{ATE} =
N^{-1} \sum_{i=1}^{N}
\frac{[ w_{i} - p(\mbs{x}_{i} ) ] y_{i} }{ p( \mbs{x}_{i} ) [1 - p( \mbs{x}_{i} ) ] }
}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{ATT:}

\begin{align*}
\E( y_{1} | \mbs{x}, w=1) - \E(y_{0} | \mbs{x}) =
\frac{1}{\hat{P}(w=1)}
\E\left[ 
\frac{[w - \hat{p}(\mbs{x})] y}{[ 1 - \hat{p}(\mbs{x}) ]}
| \mbs{x}
\right]
\end{align*}

\vspace{-1 em}
\begin{align*}
	\hat{P} (w = 1) = N^{-1} \sum_{i=1}^{N} w_{i}
\end{align*}

\vspace{-1.5 em}
\begin{align*}
\widehat{ATT} &=
\frac{N}{\sum_{i=1}^{N} w_{i} }
N^{-1} \sum_{i=1}^{N}
\frac{[ w_{i} - \hat{p}(\mbs{x}_{i} ) ] y_{i} }{[ 1 - \hat{p}( \mbs{x}_{i} ) ]}
\\
\Aboxed{
\widehat{ATT} &=
\frac{1}{\sum_{i=1}^{N} w_{i} }
\sum_{i=1}^{N}
\frac{[ w_{i} - \hat{p}(\mbs{x}_{i} ) ] y_{i} }{[1 - \hat{p}( \mbs{x}_{i} ) ]}
}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section*{Appêndice}

\subsection*{Sums of Values}
\noindent
\cite[p. 977, A.2.7]{greene-7ed}

\begin{align*}
\mbs{1}_{N}' \mbs{1}_{N} = N
\quad ; \qquad
\mbs{1}_{N} \mbs{1}_{N}' =
\begin{bmatrix}
	1 & \dots & 1	 \\
	\vdots & \ddots & \vdots \\
	1 & \dots & 1	
\end{bmatrix}_{N \times N}
\end{align*}

Defining $\mbs{x}$ with dimension $1 \times N$:

\begin{align*}
\mbs{x} = 
\begin{bmatrix}
x_{1} \\ \vdots \\ x_{N}	
\end{bmatrix}
\end{align*}

% SUM
\begin{align*}
\mbs{x}' \mbs{1}_{N} = 
\mbs{1}_{N}' \mbs{x} = 
(\mbs{x}' \mbs{1}_{N})' = 
\sum_{i=1}^{N} x_{i}
\end{align*}

% Matrix
\begin{align*}
\mbs{1}_{N} \mbs{x}' =
\begin{bmatrix}
	x_{1} & \dots & x_{N} \\
	\vdots & \ddots & \vdots \\
	x_{1} & \dots & x_{N}	
\end{bmatrix}_{N \times N}
\; ; \qquad
\mbs{x} \mbs{1}_{N}' =
\begin{bmatrix}
	x_{1} & \dots & x_{1} \\
	\vdots & \ddots & \vdots \\
	x_{N} & \dots & x_{N}	
\end{bmatrix}_{N \times N}
\end{align*}

\begin{align*}
\E( \mbs{x} ) = \overline{\mbs{x}} = N^{-1} \sum_{i=1}^{N} x_{i} = N^{-1} \mbs{x}'\mbs{1}_{N}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Important Idempotent Matrices}
\noindent
\cite[p. 978, A.28]{greene-7ed}

Centering Matrix

\vspace{-1 em}
\begin{align*}
	M^{0} &= 
	I_{N} - \mbs{1}_{N} ( \mbs{1}_{N}' \mbs{1}_{N} )^{-1} \mbs{1}_{N}'
	= 
	I_{N} - N^{-1} \mbs{1}_{N} \mbs{1}_{N}' 
\end{align*}

A Matriz $M^{0}$ é \textbf{idempotente} e \textbf{simétrica}.

\begin{description}\itemsep0pt
\item [Idempotência:] $AA = A$
\item [Simetria:] $A'=A$
\end{description}


\vspace{-1 em}
\begin{align*}
M^{0} \mbs{x} &= 
( I_{N} - N^{-1} \mbs{1}_{N} \mbs{1}_{N}' ) \mbs{x} 
= 
\mbs{x} - N^{-1} \mbs{1}_{N} (\mbs{1}_{N}' \mbs{x}) 
=
\mbs{1}_{N} \overline{\mbs{x}}
=
\begin{bmatrix}
\overline{\mbs{x}} \\ \vdots \\ \overline{\mbs{x}}
\end{bmatrix}
\end{align*}

\vspace{-1 em}
\begin{align*}
M^{0} \mbs{1} &= 
( I_{N} - N^{-1} \mbs{1}_{N} \mbs{1}_{N}' ) \mbs{1}_{N}
= 
\mbs{1}_{N} - N^{-1} \mbs{1}_{N} (\mbs{1}_{N}' \mbs{1}_{N}) 
=
\mbs{0}_{N} 
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Conceitos Básicos de Convergência Estatística} \label{app1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[\textbf{Estimador Consistente}]
Um estimador $\hat{\theta}$ é \textbf{consistente} para um parâmetro $\theta$ se 

\vspace{-1 em}
\begin{align*}
\hat{\theta} \overset{p}{\longrightarrow} \theta.
 %  \\
 %  \hat{\theta} \arrowp \theta.
\end{align*}
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[\textbf{Convergência em Probabilidade}]
Uma sequência de variáveis aleatórias:
$\{ X_{n} \}_{n \geq 1}$ 
\textbf{converge em probabilidade} para uma variável aleatória $X$ se, dado $\err > 0$, 

\vspace{-1 em}
\begin{align*}
	P(| X_{n} - X | > \err ) \to 0,
\end{align*}

\noindent
quando $n \to + \infty$.
E denotamos

\vspace{-1 em}
\begin{align*}
	X_{n} \arrowp X.
\end{align*}
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[\textbf{Desigualdade de Markov}]
Seja
$\{ X_{n} \}_{n \geq 1}$ 
uma sequência de variáveis aleatórias com
$E|X_{n}|^{K} < +\infty$, $K>0$. 
Então, dado $\err >0$

\vspace{-1 em}
\begin{align*}
	P(| X_{n} | > \err ) \leq \frac{E|X_{n}|^{K}}{\err^{K}}
\end{align*}
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1} %[\textbf{}]
\begin{align*}
0 \leq P(| \thetahat - \theta | > \err ) \leq \frac{E|X_{n}|^{2}}{\err^{2}}
\end{align*}
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[\textbf{Erro Quadrático Médio}]
\begin{align*}
EQM(\thetahat) 
=
\E\left[ \left( \thetahat - \theta \right)^2 \right] 
=
\left[ Bias(\thetahat)^{2} + \Var(\thetahat) \right]
\end{align*}
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Então, se $Bias(\thetahat) \to 0$ e $\Var(\thetahat) \to 0$, temos que $EQM(\thetahat) \to 0$.
Pelo \textbf{Teorema do Sanduíche}, $P(|\thetahat - \theta| > \err) \to 0$; logo, $\thetahat \arrowp \theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[\textbf{LGN -- Lei dos Grandes Números}]
Seja $\seq{X_{i}}_{i \geq 1}$ uma sequência de variáveis aleatórias $iid$ com $\E(X_{i}) = \mu$.
Então, 

\vspace{-1 em}
\begin{align*}
	N^{-1} \sum_{i=1}^{N} X_{i} \arrowp \mu.
\end{align*}
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[\textbf{LGN -- Caso Matricial}]
Seja $\seq{\mbs{x}_{i}}_{i=1}^{N}$, uma sequência $iid$ de vetores aleatórios $K \times 1$ com $\E(\mbs{x}_{i} \mbs{x}_{i}') = Q_{K \times K}$ finita.
Então, 

\vspace{-1 em}
\begin{align*}
N^{-1} \sum_{i=1}^{N}
\mbs{x}_{i} \mbs{x}_{i}'
\arrowp Q.
\end{align*}

Se $Q$ for positiva definida, $Q$ terá inversa.
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Multiplicação de Matriz}

\begin{align*}
A_{2 \times 2} =
\begin{bmatrix}
	a_{11} & a_{12} \\
	a_{21} & a_{22}		
\end{bmatrix}
\quad
B_{2 \times 3} =
\begin{bmatrix}
	b_{11} & b_{12} & b_{13} \\
	b_{21} & b_{22} & b_{23}		
\end{bmatrix}
\end{align*}

\begin{align*}
[AB]_{2 \times 3} &=
\begin{bmatrix}
	a_{11} \\ a_{21}
\end{bmatrix}
\begin{bmatrix}
	b_{11} & b_{12} & b_{13}
\end{bmatrix}
+
\begin{bmatrix}
	a_{12} \\ a_{22}
\end{bmatrix}
\begin{bmatrix}
	b_{21} & b_{22} & b_{23}
\end{bmatrix}
\implies
AB = \sum_{i=1}^{2} a_{i}b_{i}
\end{align*}

\noindent
onde
$a_{i}$ é a $i$-ésima \textbf{coluna} da matriz $A$.
$b_{i}$ é a $i$-ésima \textbf{linha} da matriz $B$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}
\begin{align*}
P( | X_{n} - X | > \err ) & \to 0
\\
X_{n} - X & \arrowp 0
\\
X_{n} & \arrowp X
\end{align*}
\end{def1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[$o_{p}$]
\begin{align*}
X_{n} = o_{p}(1) & \implies X_{n} \arrowp 0
\\
X_{n} = o_{p}(Y_{n}) & \implies
\frac{X_{n}}{Y_{n}} = o_{p}(1) \implies
\frac{X_{n}}{Y_{n}} \arrowp 0
\\
X_{n} = W_{n} + o_{p}(1) & \implies
(X_{n} - W_{n}) = o_{p}(1) \implies
(X_{n} - W_{n}) \arrowp 0
\end{align*}
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[Limitação em Probabilidade: $O_{p}$]
Dizemos que $X_{n}$ é \textbf{limitado em probabilidade} e denotado por 
$X_{n} = O_{p}(1)$,
se existe $M$ maior que zero, tal que para todo $\err$ maior que zero, $P( | X_{n} | > 0) < \err$.

\vspace{-1 em}
\begin{align*}
X_{n} = O_{p}(1) \implies \exists M > 0 \, ; \;
\forall \err > 0 \, , \;
P( | X_{n} | > 0) < \err.
\end{align*}
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}
Dizemos que
$X_{n} = O_{p}(Y_{n})$ se existe $M$ maior que zero, tal que para todo $\err$ maior que zero, $P( | X_{n}/Y_{n} | > 0) < \err$.

\vspace{-1 em}
\begin{align*}
X_{n} = O_{p}(Y_{n}) \implies \exists M > 0 \, ; \;
\forall \err > 0 \, , \;
P( | X_{n}/Y_{n} | > 0) < \err.
\end{align*}
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}
Se
$X_{n} = O_{p}(1)$ e $Y_{n} = o_{p}(1)$, então

\vspace{-1 em}
\begin{align*}
	X_{n} Y_{n} = O_{p}(1) o_{p}(1) = o_{p}(1).
\end{align*}
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[Equivalência Assintótica]
Seja
$\seq{\mbs{x}_{n}}$ e $\seq{\mbs{z}_{n}}$
sequências de vetores aleatórios $K \times 1$.
Se $\mbs{z}_{n} \arrowd \mbs{z}$ e 
$\mbs{x}_{n} - \mbs{z}_{n} \arrowp \mbs{0}_{K}$.
Então, 

\vspace{-1 em}
\begin{align*}
\mbs{x}_{n} \arrowd \mbs{z}.
\end{align*}

\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[Convergência em Distribuição]
Seja
$\seq{X_{n}}_{n \geq 1}$  uma sequência de variáveis aleatórias e $X$ uma variável aleatória com $F_{n}$ e $F$ suas respectivas FDAs, então

\begin{align*}
	X_{n} \arrowd X, \text{ se } F_{n}(X) \to F(X)
\end{align*}
\noindent
para todo $X$ onde $F$ é contínuo.
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[Convergência em Distribuição e Limitação em Probabilidade]
Se $X_{n} \arrowd X$, $X$ um variável aleatória qualquer; então $X_{n} = O_{p}(1)$.
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[TCL -- Teorema Central do Limite]
Seja $\seq{X_{n}}_{n = 1}^{N}$ $iid$ com $\E(X_{n}) = \mu$ e $\Var(X_{n}) = \sigma^{2} < + \infty$.
Então, para $S_{N} = \sum_{n=1}^{N} X_{n}$:

\begin{align*}
\frac{S_{N} - N \mu }{ \sqrt{N} \sigma}
=
\frac{N ( \overline{X} - \mu ) }{ \sqrt{N} \sigma}
=
\frac{N ( \overline{X} - \mu ) }{ \sqrt{N} \sigma}
=
\boxed{
\frac{\sqrt{N} ( \overline{X} - \mu ) }{\sigma}
\arrowd Z \sim N(0,1)}.
\end{align*}
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[TCL -- Caso Vetorial]
Seja $\seq{\mbs{w}_{i}}_{i=1}^{n}$ uma sequência $iid$ de vetores aleatórios $K \times 1$ com
$\E( w_{ik}^{2} ) < + \infty$, $k= 1, \dots K$ e $\E( \mbs{w}_{i} ) = \mbs{0}_{K}$.
Então,

\vspace{-1 em}
\begin{align*}
	N^{-1/2} \sum_{i=1}^{N} \mbs{w}_{i} \arrowd N(\mbs{0}, B),
\end{align*}

\noindent
onde, $B=\Var(\mbs{w}_{i}) = \E( \mbs{w}_{i} \mbs{w}_{i}')$.
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{def1}[]
Seja $\seq{\mbs{z}_{n}}$ uma sequência de vetores $K \times 1$ aleatórios com 

\vspace{-1 em}
\begin{align*}
	\mbs{z}_{n} \arrowd N(\mbs{0}, V).
\end{align*}

Então, para qualquer matriz $A$ de dimensão $K \times M$ \textbf{não} estocástica,

\vspace{-1 em}
\begin{align*}
	A' \mbs{z}_{n} \arrowd N(\mbs{0}, A'VA).
\end{align*}
\end{def1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section*{TODO}

\begin{enumerate}[noitemsep]
	\item Acabar Aula 2
	\item Revisar Aula 1 com C.4
	\item Revisar Conceitos Estatísticos com C.3
	\item Fazer POLS com Sec 7.8
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Bibliografia
\clearpage
\bibliographystyle{authordate3} % bibliography style
\renewcommand\bibname{REFERENCES} 
\bibliography{./refs.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}


