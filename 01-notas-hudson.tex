\documentclass[11pt, oneside, a4paper, article]{article}
\usepackage[a4paper, margin=1in]{geometry}   % MARGENS
% \usepackage[a4paper, margin=2cm]{geometry}   % MARGENS

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pacotes básicos 
\usepackage[T1]{fontenc}		% Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}		% Codificacao do documento (conversão automática dos acentos)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% languages
\usepackage[english]{babel}
% \selectlanguage{english}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AMS math
\usepackage{amsfonts, amssymb, amsthm}
\usepackage[fleqn]{amsmath}
\setlength{\mathindent}{0pt}

\usepackage{mathtools, latexsym}
% \usepackage{mathabx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Pacotes de citações
\usepackage{natbib}	% Citações}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{indentfirst}		% Indenta o primeiro parágrafo de cada seção.
\usepackage{color}				% Controle das cores
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{microtype} 			% para melhorias de justificação
%\usepackage{listings}			% Formatação para inserir códigos
\usepackage[normalem]{ulem}
\usepackage[all]{xy}
\usepackage{xcolor}
\usepackage{ragged2e}           % formatação texto
\usepackage{bm}                 % bold symbols 
\usepackage[colorlinks, citecolor=blue, urlcolor=blue, linkcolor=magenta]{hyperref} % referencias dentro do texo (*QUEBRA MEMOIR*)
\usepackage{url}                % URL
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{subcaption}			% Faz subfiguras
%\usepackage{subfigure}         % subfigures
% \usepackage{setspace}         % Espaçamento
\usepackage{enumitem}
% \usepackage{enumerate}
\usepackage[sharp]{easylist}
% \usepackage{titlesec}		    % Customização de seçoes
\usepackage{titling}          % personalized other things
\usepackage{fancyhdr}         % personalized page style
\usepackage{lipsum}           % dummy text
% \usepackage{exercise}       % exercises
\usepackage[flushleft]{threeparttable} % notas nas tabelas

% \usepackage{lscape}				% Gira a página em 90 graus
\usepackage{pdflscape} % páginas em formato paisagem
\usepackage{multirow} % permite fazer tabelas com multirows
\usepackage{tabularx} % 
\usepackage{tikz} % desenhos
\tikzset{>=latex}

\usepackage[pagewise]{lineno}           % show line numbers
% \linenumbers

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{../macros.tex}
% \pagestyle{headings}
\pagestyle{fancy}
\numberwithin{equation}{section}
% \setcounter{equation}{0}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METADATA
\title{Microeconometrics: Lecture Notes }
\author{Paulo F. Naibert}
% \date{25/06/2020}
% \date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}
\pagenumbering{gobble}

\begin{center}
\textbf{UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL}
\\
\textbf{PROGRAMA DE PÓS-GRADUAÇÃO EM ECONOMIA}
\\
\textbf{Microeconometria -- 2015/3}

\vfill
\textbf{\thetitle}

\vfill
\textbf{Autor: Paulo Ferreira Naibert } 
\\
\textbf{Professor: Hudson Torrent} 
\end{center}

\vfill

\begin{center}
	\textbf{Porto Alegre \\ 30/06/2020 \\ Revisão: \today}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\pagenumbering{arabic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Regressão MQO Clássico}
\noindent
\citet[C.4 -- The Single-Equation Linear Model and OLS Estimation, p.49--76]{wool-2010}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelo de equações lineares} 

O modelo populacional que estudamos é linear em seus parâmetros,

\vspace{-1 em}
\begin{align} \label{ols:mod}
	y &= \beta_{0} + \beta_{1} x_{1} + \dots + \beta_{K} x_{K} + u
\end{align}
onde:

\begin{description}[\noitemsep]
	\item [$y, x_{1}, \dots, x_{K}$]  são escalares aleatórios e observáveis (i.e., conseguimos observá-los em uma amostra aleatória da população);

	\item [$u$] é o \textit{random disturbance} não observável, ou erro; 

	\item [$\beta_{0}, \beta_{1}, \dots, \beta_{K}$] são parâmetros (constantes) que gostaríamos de estimar.
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Notação Vetorial} 
\noindent
\citet[Sec. 4.2 -- Asymptotic Properties of OLS; p.51]{wool-2010}

Por conveniência, escrevemos a equação populacional em forma de vetor:

\vspace{-1 em}
\begin{align} \label{ols:mod:vec}
	y &= \xvec \betavec + u
\end{align}

\noindent
onde,

\vspace{-1 em}
\begin{description}[noitemsep]
\item [$\xvec \equiv (x_{1}, \dots, x_{K})$] é um vetor $1 \times K$ de regressores;

\item [$\betavec \equiv (\beta_{1}, \dots, \beta_{K})'$] é um vetor $K \times 1$.
\end{description}

\noindent
Uma vez que a maioria das equações contém um intercepto, assumiremos que $x_{1} \equiv 1$, visto que essa hipótese deixa a interpretação mais fácil.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Amostra Aleatória} 

Assumimos que conseguimos obter uma amostra aleatória de tamanho $N$ da população para estimarmos $\betavec$.
Dessa forma, $\{ (\xvec_{i}, y_{i}); \, i = 1, 2, \dots, N \}$
são tratados como variáveis aleatória independentes, identicamente distribuídas, onde
$\xvec_{i}$ é $1 \times K$ e $y_{i}$ é escalar.
Para cada observação $i$, temos:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1 em}
\begin{align} \label{ols:mod:vec:i}
	y_{i} &= \xvec_{i} \betavec + u_{i}.
\end{align}

\noindent
onde
$\xvec_{i}$
é um vetor $1 \times K$ de regressores.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hipóteses} 

\paragraph{OLS.1} 
$y_{i} &= \xvec_{i} \betavec + u_{i} \, , \quad i = 1, \dots, N$;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{OLS.2}  $\Xmat$ é \textbf{não} estocástica;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{OLS.3} $\{ u_{i} \}_{i=1}^{N}$  é  $iid$ com e para cada $i = 1, \dots, N$:

\vspace{-1.5 em}
\begin{align*}
	\E(u_{i}) &= 0
	\\
	\Var(u_{i}) &= \E(u_{i}^2) = \sigma^2
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{OLS.2'} $\Xmat$ é estocástica;

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{OLS.3'} 

\begin{align*}
	\E(u_{i} | \Xmat) &= 0, 
	\\
	\Var(u_{i} | \Xmat) &= 
	E
	\left\{ \left[ 
			u_{i} - \E( u_{i} | \Xmat)
	\right]^2 | \Xmat \right\}
	=
	\E(u_{i}^2 | \Xmat) = \sigma^2.
\end{align*}

\begin{remark}
	$\E(u_{i} | \Xmat) = 0$ implica que $u_{i}$ é \textbf{não correlacionado} com todos os regressores $x_{k}$ para $k=1,\dots, K$. \red{Exogeneidade estrita}.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimação} 

Usando \textbf{OLS.1:}

\vspace{-1.5 em}
\begin{align*}
	y_{i} &= \xvec_{i} \betavec + u_{i} 
	\\
	\xvec_{i}' y_{i} &= \xvec_{i}' \xvec_{i} \betavec + \xvec_{i}' u_{i} 
	\\
	\E( \xvec_{i}' y_{i}) &= \E( \xvec_{i}' \xvec_{i} ) \betavec + \E( \xvec_{i}' u_{i} ) 
\end{align*}
Usando $\boxed{\E( \xvec_{i}' u_{i} ) = 0}$ 
\red{[Qual seria essa hipótese?]}

\vspace{-1 em}
\begin{align}
	\notag
	\E( \xvec_{i}' y_{i}) &= \E( \xvec_{i}' \xvec_{i} ) \betavec
	\\
	\label{ols:beta}
	\Aboxed{\betavec &= [\E( \xvec_{i}' \xvec_{i} )]^{-1} \E( \xvec_{i}' y_{i}) }.
\end{align}

Agora, usando o \textbf{princípio da analogia} e utilizando \textbf{estimadores amostrais}:

\vspace{-1 em}
\begin{align}\label{ols:betahat:sums}
	\Aboxed{
		\betahatbold &= 
		\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' y_{i} \right) }.
\end{align}

Podemos desenvolver essa equação para:

\vspace{-1 em}
\begin{align}
	\notag
	\betahatbold &= 
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' (\xvec_{i} \betavec + \uvec_{i}) \right)
	\\
	\notag
	&=
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \betavec \right) +
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \uvec_{i} \right)
	\\
	\label{ols:betahat:u}
	\Aboxed{
		\betahatbold &= 
		\betavec +
		\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \uvec_{i} \right) }.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Notação Matricial}
Empilhando as $N$ observações, obtemos a \textbf{Notação Matricial}:

\vspace{-1 em}
\begin{align} \label{ols:mod:mat}
	\yvec &= \Xmat \betavec + \uvec 
\end{align}

\begin{description}[noitemsep]
	\item [$\yvec$]  é um vetor $N \times 1$;

	\item [$\Xmat$]  é uma matriz $N \times K$ de regressores, com $N$ vetores, $\xvec_{i}$, de dimensão $1 \times K$ empilhados;

	\item [$\betavec$] é um vetor $K \times 1$;

	\item [$\uvec$] é um vetor $N \times 1$;
\end{description}

\vspace{-1 em}
\begin{align*}
	\yvec = 
	\begin{bmatrix}
		y_{1} \\ \vdots \\ y_{N}		
	\end{bmatrix};
	\quad
	\Xmat = 
	\begin{bmatrix}
		\xvec_{1} \\ \vdots \\ \xvec_{N}
	\end{bmatrix} = 
	\begin{bmatrix}
		x_{11}     & x_{12}     & \dots  & x_{1K} \\          
%	x_{21}     & x_{22}     & \dots  & x_{2K} \\         
		\vdots     & \vdots     & \ddots & \vdots \\        
		x_{N1} & x_{N2} & \dots  & x_{NK}		
	\end{bmatrix};
	\quad
	\uvec = 
	\begin{bmatrix}
		u_{1} \\ \vdots \\ u_{N}		
	\end{bmatrix}.
\end{align*}

As somas de vetores viram simples multiplicações de matrizes e a equação \eqref{ols:betahat:sums}, vira:

\vspace{-1 em}
\begin{align} 
	\label{ols:betahat:mat}
	\widehat{\betavec} = (N^{-1} \Xmat' \Xmat)^{-1} (N^{-1} \Xmat'\yvec)
	\implies
	\Aboxed{ \widehat{\betavec} = (\Xmat'\Xmat)^{-1}(\Xmat'\yvec)}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Valor Esperado} 

\vspace{-2 em}
\begin{align*} 
	\E ( \widehat{\betavec} ) 
	&= \E \left[ (\Xmat'\Xmat)^{-1}\Xmat'\yvec \right]
	= \E \left[ (\Xmat'\Xmat)^{-1}\Xmat'(\Xmat \betavec + \uvec) \right]
	= \E \left[ (\Xmat'\Xmat)^{-1}\Xmat'\Xmat \betavec + (\Xmat'\Xmat)^{-1}\Xmat'\uvec \right]
	\\
	&= \E (\betavec) + \E[(\Xmat'\Xmat)^{-1}\Xmat'\uvec ]
	\implies
	\boxed{
		\E ( \widehat{\betavec} ) 
	= \betavec + \E[(\Xmat'\Xmat)^{-1}\Xmat'\uvec ] }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Viés} 

\vspace{-2 em}
\begin{align*} 
	\B( \widehat{\betavec} ) = \E ( \widehat{\betavec} ) - \betavec
	\implies
	\Aboxed{ \B( \widehat{\betavec} ) = \E[ (\Xmat'\Xmat)^{-1}\Xmat'\uvec ] }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
	Sob \textbf{OLS.2'} e \textbf{OLS.3'}:

	\vspace{-2 em}
	\begin{align*}
		\E[(\Xmat'\Xmat)^{-1}\Xmat'\uvec ]
		= \E \left\{ \E\left[ (\Xmat'\Xmat)^{-1}\Xmat'\uvec | \Xmat \right]  \right\}  
		= \E \left\{  (\Xmat'\Xmat)^{-1}\Xmat'
			\underbracket[.75pt]{\E( \uvec | \Xmat )}_{= \zerovec}
		\right\} = 0
	\end{align*}

	\noindent
	ou seja, 
	$\B( \widehat{\betavec} ) = 0$, logo $\widehat{\betavec}$ é \textbf{não-viciado}.
	O que também é equivalente a  $\E( \widehat{\betavec} ) = \betavec$.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variância} 
Supondo \textbf{OLS.2'} e \textbf{OLS.3'}:

\vspace{-1 em}
\begin{align*} 
	\Var ( \widehat{\betavec} | \Xmat) 
	&= \E \left\{\left[ 
			\widehat{\betavec} - \E ( \widehat{\betavec} | \Xmat )
	\right]^2 | \Xmat \right\}
	\\
	&= \E \left\{ 
		\left[ \widehat{\betavec} - \E ( \widehat{\betavec} | \Xmat ) \right]
		\left[ \widehat{\betavec} - \E ( \widehat{\betavec} | \Xmat ) \right]'
	| \Xmat \right\}
	\\
	&= \E \left\{ 
		\left[ (\Xmat'\Xmat)^{-1}\Xmat' \uvec \right]
		\left[ (\Xmat'\Xmat)^{-1}\Xmat' \uvec \right]'
	| \Xmat \right\}
	\\
	&= \E \left[ (\Xmat'\Xmat)^{-1}\Xmat' \uvec \uvec' \Xmat (\Xmat'\Xmat)^{-1} | \Xmat \right]
	\\
	\Aboxed{
		\Var ( \widehat{\betavec} | \Xmat) 
		&= 
		(\Xmat'\Xmat)^{-1}\Xmat' 
		\E \left[ \uvec \uvec'| \Xmat \right]
	\Xmat (\Xmat'\Xmat)^{-1} }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Homocedasticidade}
Supondo \textbf{homocedasticidade} e ausência de correlação serial: 
$\boxed{ \E \left[ \uvec \uvec'| \Xmat \right] = \sigma^2 \Imat_{N} }$.
Assim, 

\vspace{-1 em}
\begin{align*} 
	\Var ( \widehat{\betavec} | \Xmat) 
	= \sigma^2 (\Xmat'\Xmat)^{-1}\Xmat' \Imat_{N} \Xmat (\Xmat'\Xmat)^{-1}
	= \sigma^2 (\Xmat'\Xmat)^{-1}\Xmat'\Xmat (\Xmat'\Xmat)^{-1}
	\implies
	\Aboxed{ \Var ( \widehat{\betavec} | \Xmat) &= \sigma^2 (\Xmat'\Xmat)^{-1} }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Ausência de Exogeneidade Estrita}
Nem sempre poderemos supor \textbf{exogeneidade estrita}.
Por exemplo, no modelo com variável defasada mostrado abaixo:

\vspace{-1 em}
\begin{align*}
%  \left.
%  \begin{aligned}
	y_{t} &= \beta_{0} + \beta_{1} y_{t-1} + \beta_{2} x_{1t} + u_{t}
	\\
	y_{t-1} &= \beta_{0} + \beta_{1} y_{t-2} + \beta_{2} x_{1t-1} + u_{t-1}
%  \end{aligned}
%  \right\}
%  \implies
	\\
	y_{t} &=
	\beta_{0}(1 + \beta_{1})
	+
	\beta_{1}^2 y_{t-2}
	+
	\beta_{1} 
	\beta_{2} x_{1t-1} 
	+
	\beta_{2} x_{1t} 
	+
	u_{t}
	+
	\beta_{1} u_{t-1},
\end{align*}

\noindent
o erro é correlacionado com o regressor $y_{t-1}$.
Nesse caso, tentaremos obter apenas \textbf{consistência} e \textbf{variância assintótica} do estimador.
Para tanto, utilizaremos a equação \eqref{ols:betahat:u}: 

\vspace{-1 em}
\begin{align*}
	\betahatbold &= 
	\betavec +
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \uvec_{i} \right).
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1 em}
\begin{center}
	\Large{\red{Aqui comeceçaria a seção \ref{app:est}}.}
\end{center}
\vspace{1 em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Consistência}
Vamos definir a matriz $K \times K$, $\Amat \equiv \E(\xvec_{i}' \xvec_{i})$.
Supondo $\Amat$, finita e positiva definida, $\posto(\Amat) = K$.
Usando \textbf{LGN matricial} (Definição \ref{teo:lgn:mat:1} na página \pageref{teo:lgn:mat:1}), temos: 

\noindent
\red{[lembrar que as dimensões dos vetores estão invertidas: $1 \times K$ e \textbf{não} $K \times 1$]}

\vspace{-1 em}
\begin{align} \label{eq:Amat}
	N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i}
	\arrowp \Amat
	\implies
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\arrowp \Amat^{-1}.
\end{align}

Além disso, vamos supor $\E(\xvec_{i}' u_{i}) = 0$, o que corresponde a $\Cov(\xvec_{i}, u_{i}) = 0$, ou seja, o erro $u_{i}$ \textbf{não} é correlacionado com os regressores da própria equação.
Isso é bem menos que exogeneidade estrita.
Então, 

\vspace{-1 em}
\begin{align*}
	N^{-1} \sum_{i=1}^{N} \xvec_{i}' u_{i} \arrowp \E(\xvec_{i}' u_{i}) = \zerovec_{K}.
\end{align*}

Logo,

\vspace{-1 em}
\begin{align*}
	\boxed{
		\betahatbold = 
		\betavec +
		\underbracket[1pt]{
			\left( \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
		\left( \sum_{i=1}^{N} \xvec_{i}' u_{i} \right)}_{\arrowp 0}
	}
\end{align*}

Então, 
$(\betahatbold - \betavec) \arrowp 0$ 
que é equivalente a 
$\betahatbold \arrowp \betavec$ e 
$\plim\betahatbold = \betavec$,
ou seja, $\betahatbold$ é \textbf{consistente} para $\betavec$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Normalidade Assintótica} % do $\betahatbold^{OLS}$}

\vspace{-2 em}
\begin{align*}
	\betahatbold 
	&= 
	\betavec +
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' u_{i} \right)
	\\
	(\betahatbold - \betavec)
	&=
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' u_{i} \right)
	\\
	\sqrt{N} (\betahatbold - \betavec) &=
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\left( N^{-1/2} \sum_{i=1}^{N} \xvec_{i}' u_{i} \right)
\end{align*}

\noindent
Supondo
$\E( x_{ik}^{2} u_{i}^{2} ) < + \infty$,
$k=1, \dots, K$, e definindo
$\Bmat = \E[\xvec_{i}' u_{i}' u_{i} \xvec_{i}] = \E[ u_{i}^2 \xvec_{i}' \xvec_{i} ]$.
Temos, pela Definição \ref{def:tcl} (\textbf{TCL}), que

\vspace{-1 em}
\begin{align} \label{eq:limxu}
	N^{-1/2} \sum_{i=1}^{N} \xvec_{i}' u_{i} \arrowd N(\zerovec, \Bmat)
	\implies
	N^{-1/2} \sum_{i=1}^{N} \xvec_{i}' u_{i} = \bigOp(1)
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1 em}
Além disso, vamos utilizar a matriz \textbf{simétrica} e \textbf{não singular} $\Amat$ da equação \eqref{eq:Amat}
Assim, temos 

\vspace{-1 em}
\begin{align*}
	\sqrt{N} (\betahatbold - \betavec) &=
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\left( N^{-1/2} \sum_{i=1}^{N} \xvec_{i}' u_{i} \right)
	\\ &=
	\left[ 
		\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1} 
		+ \Amat^{-1} - \Amat^{-1}
	\right]
	\left( N^{-1/2} \sum_{i=1}^{N} \xvec_{i}' u_{i} \right)
	\\ &=
	\left[ 
		\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1} 
		- \Amat^{-1}
	\right]
	\left( N^{-1/2} \sum_{i=1}^{N} \xvec_{i}' u_{i} \right)
	+ \Amat^{-1} 
	\left( N^{-1/2} \sum_{i=1}^{N} \xvec_{i}' u_{i} \right),
\end{align*}

\noident
Podemos inverter $\Amat$ porque ela tem posto completo (não singular).
Pelas propriedades de $\Amat$, temos:

\vspace{-1 em}
\begin{align*}
	N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \arrowp \Amat
	\implies
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}  - \Amat^{-1} = \litop(1).
\end{align*}

Então,

\vspace{-1 em}
\begin{align*}
	\sqrt{N} (\betahatbold - \betavec) &=
	\litop(1) \bigOp(1)
	+ \Amat^{-1} \left( N^{-1/2} \sum_{i=1}^{N} \xvec_{i}' u_{i} \right),
\end{align*}

Usando \eqref{eq:limxu} e o Lema \ref{lem:equiv:assin}.

\begin{align*}
	\Amat^{-1} \left( N^{-1/2} \sum_{i=1}^{N} \xvec_{i}' u_{i} \right)
	\arrowd 
	N(\zerovec, \Amat^{-1} \Bmat \Amat^{-1}).
\end{align*}

Lembrando que $\litop(1) \bigOp(1) = \litop(1)$, temos:

\vspace{-1 em}
\begin{align*}
	\sqrt{N} (\betahatbold - \betavec) \arrowd \mathcal{N}(\zerovec, \Amat^{-1} \Bmat \Amat^{-1})
	\implies
	\sqrt{N} (\betahatbold - \betavec) \asim \mathcal{N}(\zerovec, \Amat^{-1} \Bmat \Amat^{-1})
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variância}

\vspace{-2 em}
\begin{align*}
	\Vmat &= \Amat^{-1} \Bmat \Amat^{-1} 
	\\
	\Vmat &=
	\E[ (\xvec_{i}' \xvec_{i} ) ]^{-1}
	\E[ (\xvec_{i}' u_{i}' u_{i} \xvec_{i} ) ]
	\E[ (\xvec_{i}' \xvec_{i} ) ]^{-1}
	\\
	\Vmat &=
	\E[ (\xvec_{i}' \xvec_{i} ) ]^{-1}
	\E[ (u_{i}^{2} \xvec_{i}' \xvec_{i} ) ]
	\E[ (\xvec_{i}' \xvec_{i} ) ]^{-1}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Homocedasticidade}

Sob \textbf{Homocedasticidade}, temos
$\Bmat = \E(u_{i}^2 \xvec_{i}' \xvec_{i}) &= \sigma^2 \E(\xvec_{i}' \xvec_{i})$, 
logo

\vspace{-1 em}
\begin{align*}
	\Aboxed{
	\Vmat &= \sigma^{2} \E[ (\xvec_{i}' \xvec_{i} ) ]^{-1} }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Estimador Amostral}

\vspace{-2 em}
\begin{align*}
	\widehat{\Vmat} &=
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\left( N^{-1} \sum_{i=1}^{N} u_{i}^2 \xvec_{i}' \xvec_{i} \right)
	\left( N^{-1} \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\\ &=
	N
	\left( \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\left( \sum_{i=1}^{N} u_{i}^2 \xvec_{i}' \xvec_{i} \right)
	\left( \sum_{i=1}^{N} \xvec_{i}' \xvec_{i} \right)^{-1}
	\\
	\Aboxed{
		\widehat{\Vmat} &=
		N
		\left( \Xmat'\Xmat \right)^{-1}
		\left( \sum_{i=1}^{N} u_{i}^2 \xvec_{i}' \xvec_{i} \right)
	\left( \Xmat'\Xmat \right)^{-1} }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Variância do estimador de OLS }

\vspace{-1 em}
\begin{align*}
	\Var(\sqrt{N} \betahatbold) &= \Vmat
	\\
	\Var(\betahatbold) &= N^{-1} \Vmat
	\\
	\Aboxed{
		\Var(\betahatbold) &= 
		\left( \Xmat'\Xmat \right)^{-1}
		\left( \sum_{i=1}^{N} u_{i}^2 \xvec_{i}' \xvec_{i} \right)
	\left( \Xmat'\Xmat \right)^{-1} }.
\end{align*}

\noindent
A variância \textbf{Robusta} é:

\vspace{-1 em}
\begin{align*}
	\Aboxed{
		\widehat{\Var}(\betahatbold) &= 
		(\Xmat'\Xmat)^{-1} 
		\left( \sum_{i=1}^{N} \widehat{u}_{i}^{2} \xvec_{i}' \xvec_{i} \right)
	(\Xmat'\Xmat)^{-1} }.
\end{align*}

\noindent
A variância sob \textbf{Homocedasticidade} é:

\vspace{-1 em}
\begin{align*}
	\Aboxed{\widehat{\Var}(\betahatbold) &= \widehat{\sigma}^{2} (\Xmat' \Xmat)^{-1}} .
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{System OLS (SOLS)}

\noindent
\citet[C.7 -- Estimating Systems of Equations by OLS and GLS, p.143--179]{wool-2010}\\
\citet[Sec.7.3 -- System OLS Estimation of a Multivariate Linear System, p.147]{wool-2010} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelo Linear}

Assumimos que temos as seguintes observações \textit{cross section} $iid$:
$\seq{ (\Xmat_{i}, \yvec_{i}): i=1, \dots, N}$, onde:

\vspace{-1 em}
\begin{itemize}[itemsep = -1ex]
	\item [$\Xmat_{i}$]  é uma matriz $G \times K$ e contém as variáveis explicativas que aparecem em qualquer lugar do sistema.
	\item [$\yvec_{i}$]  é um vetor $G \times 1$, que contém as variáveis dependentes para todas as equações $G$ (ou períodos de tempo, no caso de dados de painel).
\end{itemize}

O modelo linear multivariado para uma \red{observação (draw)} aleatória da população pode ser expresso como:

\vspace{-1 em}
\begin{align}\label{sols:mod}
	\Aboxed{
		\yvec_{i} = \Xmat_{i} \betavec + \uvec_{i} \, , \quad i=1, \dots, N,
	}
\end{align}

\noindent
onde:

\vspace{-1 em}
\begin{itemize}[itemsep = -1ex]
	\item [$\betavec$] é um vetor $K \times 1$ de parâmetros de interesse; e
	\item [$\uvec_{i}$] é um vetor $G \times 1$ de não observáveis.
\end{itemize}

A equação \eqref{sols:mod} explica as $G$ variáveis $y_{i1}, \dots, y_{iG}$ em termos de $\Xmat_{i}$ e das não observáveis $\uvec_{i}$.
Por causa da hipótese de amostra aleatória podemos escrever tudo em temos de uma observação genérica.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hipóteses}
\noindent
\citet[Sec.7.3.1]{wool-2010}

\paragraph{SOLS.1} $\E(\Xmat_{i}' \uvec_{i}) = \zerovec_{K \times 1}$.

\paragraph{SOLS.2} $\Amat \equiv \E( \Xmat_{i}' \Xmat_{i} )$ é não singular (tem posto pleno, posto igual a $K$). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimação}
Note que, sob \textbf{SOLS.1}, temos:

\vspace{-1 em}
\begin{align} 
	\notag
	\E[ \Xmat_{i}' ( \yvec_{i} - \Xmat_{i} \betavec ) ] &= \zerovec
	\\
	\notag
	\E( \Xmat_{i}' \Xmat_{i} ) \betavec &= \E( \Xmat_{i}' \yvec_{i} )  
	\\
	\label{sols:beta}
	\Aboxed{
		\betavec &=
		\left[ \E( \Xmat_{i}' \Xmat_{i} )  \right]^{-1}
	\E( \Xmat_{i}' \yvec_{i} )  }
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Usando estimadores amostrais:

\vspace{-1 em}
\begin{align} \label{sols:betahat}
	\Aboxed{
		\betahatbold^{SOLS} &=
		\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
		\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \yvec_{i}   \right)
	}.
\end{align}

Para computar $\betahatbold$ usando linguagem de computação é mais fácil utilizar a notação matricial.
Para tanto, cortamos os $N^{-1}$ e substituímos os somatórios por multiplicações de matrizes.

\vspace{-1 em}
\begin{align} 
	\label{sols:betahat:mat}
	\Aboxed{
		\betahatbold^{SOLS} &=
		\left(  \Xmat' \Xmat   \right)^{-1} \left(  \Xmat' \yvec   \right)
	}
\end{align}

\noindent
onde
\begin{description}[noitemsep]
	\item [$\Xmat \equiv (\Xmat_{1}', \dots, \Xmat_{N}')$]  é uma matriz $NG \times K$ dos $\Xmat_{i}$ empilhados.

	\item [$\yvec \equiv (\yvec_{1}', \dots, \yvec_{N}')$] é um vetor $NG \times 1$ das observações $\yvec_{i}$ empilhadas.
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Consistência}

Para provarmos a \textbf{consistência} do estimador, usamos as equações \eqref{sols:betahat} e \eqref{sols:mod}:

\vspace{-1 em}
\begin{align*}
	\betahatbold^{SOLS} &=
	\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
	\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \yvec_{i}   \right)
	\\ &=
	\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
	\left[ N^{-1}\sum_{i=1}^{N} \Xmat_{i}' (\Xmat_{i} \betavec + \uvec_{i})   \right]
	\\ &=
	\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
	\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i} \betavec    \right)
	+
	\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
	\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \uvec_{i}   \right).
\end{align*}

\noindent
E chegamos em:

\vspace{-1 em}
\begin{align}\label{sols:betahat:u}
	\Aboxed{
		\betahatbold^{SOLS} &=
		\betavec
		+
		\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
		\left( N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \uvec_{i}   \right)
	}.
\end{align}

Por \textbf{SOLS.1}:

\vspace{-1 em}
\begin{align*}
	N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \uvec_{i} \arrowp \zerovec;
\end{align*}

\noindent e por \textbf{SOLS.2}

\vspace{-1 em}
\begin{align*}
	\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i} \right)^{-1} \arrowp \Amat^{-1}.
\end{align*}

Resumimos esse resultado pelo seguinte Teorema:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{teo}[Consistência do SOLS]\label{SOLS:const}
	Sob Hipóteses \textbf{SOLS.1} e \textbf{SOLS.2}, temos 
	\begin{align*}
		\Aboxed{
			\betahatbold^{SOLS} \arrowp \betavec
		}.
	\end{align*}
\end{teo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Normalidade Assintótica}

De \eqref{sols:betahat:u}:

\vspace{-2 em}
\begin{align*} 
	\betahatbold  &=
	\betavec +
	\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
	\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \uvec_{i}   \right)
	\\ 
	(\betahatbold - \betavec) &= 
	\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
	\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \uvec_{i}   \right).
\end{align*}

\noindent
E chegamos em:

\vspace{-1 em}
\begin{align} \label{sols:betahat:rootn}
	\Aboxed{
		\sqrt{N}(\betahatbold - \betavec) &= 
		\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
		\left( N^{-1/2} \sum_{i=1}^{N} \Xmat_{i}' \uvec_{i}   \right)
	}.
\end{align}

Uma vez que $\E(\Xmat_{i}' \uvec_{i})=0$, sob a hipótese \textbf{SOLS.1}, a definição \ref{def:tcl} (\textbf{TCL}) implica que:

\vspace{-1 em}
\begin{align*} 
	N^{-1/2} \sum_{i=1}^{N} \Xmat_{i} \uvec_{i} \arrowd N(\zerovec, \Bmat),
\end{align*}

\noindent
onde

\vspace{-1 em}
\begin{align*} 
	\Bmat \equiv \E(\Xmat_{i}' \uvec_{i} \uvec_{i}' \Xmat_{i}) \equiv \Var(\Xmat_{i} \uvec_{i}).
\end{align*}

\noindent
Em particular,

\vspace{-1 em}
\begin{align*} 
	N^{-1/2} \sum_{i=1}^{N} \Xmat_{i} \uvec_{i} = \bigOp(1).
\end{align*}

Porém,

\vspace{-1 em}
\begin{align*} 
	\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i} \right)^{-1} = (\Xmat'\Xmat/N)^{-1} 
	=
	\Amat^{-1} + \litop(1).
\end{align*}

\noindent
Sendo assim,

\vspace{-1 em}
\begin{align*} 
	\sqrt{N}(\betahatbold - \betavec) &= 
	\left[ 
		\Amat^{-1} +
		\left( N^{-1} \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}   \right)^{-1}
		- \Amat^{-1}
	\right]
	\left( N^{-1/2} \sum_{i=1}^{N} \Xmat_{i}' \uvec_{i}   \right)
	\\ &=
	\Amat^{-1}\left( N^{-1/2} \sum_{i=1}^{N} \Xmat_{i}' \uvec_{i}   \right)
	+
	[(\Xmat'\Xmat/N)^{-1} - \Amat^{-1}]
	\left( N^{-1/2} \sum_{i=1}^{N} \Xmat_{i}' \uvec_{i}   \right)
	\\&=
	\Amat^{-1}\left( N^{-1/2} \sum_{i=1}^{N} \Xmat_{i}' \uvec_{i}   \right)
	+ \litop(1) \bigOp(1)
	\\&=
	\Amat^{-1}\left( N^{-1/2} \sum_{i=1}^{N} \Xmat_{i}' \uvec_{i}   \right)
	+ \litop(1)
\end{align*}

\vspace{-1 em}
\begin{align}\label{sols:betahat:asim}
	\Aboxed{
		\sqrt{N}(\betahatbold - \betavec)
		\arrowd
		N(\zerovec, \Amat^{-1} \Bmat \Amat^{-1})
	}
\end{align}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variância Assintótica}

\paragraph{SOLS.3: Homocedasticidade}
$\E(\Xmat_{i}' \uvec_{i} \uvec_{i}' \Xmat_{i}) = \sigma^{2} \E(\Xmat_{i}' \Xmat_{i})$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1 em} 
De \eqref{sols:betahat:asim}, vamos definir $\Vmat = \Amat^{-1} \Bmat \Amat^{-1}$.
Sob \textbf{SOLS.3},
$\Vmat = \sigma^{2} \left[ \E(\Xmat_{i}' \Xmat_{i}) \right]^{-1}$.
Estimando:

\vspace{-1 em}
\begin{align*}
	\sigmahat^{2}
	=
	\frac{1}{NG - K}
	\sum_{i =1}^{N} \sum_{g=1}^{G} \uhat^2_{ig}
\end{align*}

\noindent onde $\uhat_{ig} = y_{ig} - \xvec_{ig} \betavechat^{SOLS}$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{A Matriz Robusta}

\vspace{-2 em}
\begin{align*}
	\Vmathat =
	\left( \Xmat' \Xmat \right)^{-1}
	\left( \sum_{i=1}^{N} \Xmat_{i}' \uvechat_{i}' \uvechat_{i} \Xmat \right)
	\left( \Xmat' \Xmat \right)^{-1}
\end{align*}

\vspace{-1 em}
\begin{align*}
	\sum_{i=1}^{N} \Xmat_{i}' \Omegamathat \Xmat_{i} \arrowp 
	\E(\Xmat_{i} \Omegamat \Xmat_{i})
\end{align*}
Mas \textbf{não} é verdade que $\Omegamathat \arrowp \Omegamat$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}[noitemsep]
	\item
		Havendo constante, \textbf{SOLS.1} $\implies \E(\uvec_{i})=0$
	\item
		Ausência de correlação entre os regressores de uma equação e o erro da própria equação $\implies$ \textbf{SOLS.1}.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Variância Asstintótica}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}
	\red{REVER}
\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1 em}
\begin{align}\label{eq:avar:sols}
	\Avar(\betahatbold^{SOLS}) = \Amat^{-1} \Bmat \Amat^{-1}/N.
\end{align}

Assim, $\Avar(\betahatbold^{SOLS})$ tende a zero a uma taxa $1/N$, como esperado.
Estimação consistente de $\Amat$ é:

\vspace{-1 em}
\begin{align*}
	\widehat{\Amat} \equiv \Xmat'\Xmat/N = N^{-1} \sum_{i=1}^{N} \Xmat_{i}'\Xmat_{i}
\end{align*}

Um estimador consistente para $\Bmat$ pode ser achado usando o princípio da analogia.

\vspace{-1 em}
\begin{align*}
	\Bmat = \E(\Xmat_{i}' \uvec_{i} \uvec_{i}' \Xmat_{i}), 
	\quad
	N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \uvec_{i} \uvec_{i}' \Xmat_{i} \arrowp \Bmat.
\end{align*}

Uma vez que não podemos observar $\uvec_{i}$, usamos os resíduos da estimação de SOLS:

\vspace{-1 em}
\begin{align*}
	\widehat{\uvec}_{i} \equiv \yvec_{i} - \Xmat_{i} \betahatbold 
	=
	\uvec_{i} - \Xmat_{i} (\betahatbold - \betavec).
\end{align*}

Assim, definimos $\Bmathat$ e usando LGN, podemos mostrar que:

\vspace{-1 em}
\begin{align*}
	\Bmathat \equiv N^{-1}\sum_{i=1}^{N} \Xmat_{i}' \uvechat_{i} \uvechat_{i}' \Xmat_{i} 
	\arrowp \Bmat.
\end{align*}

\noindent
onde supomos que certos momentos envolvendo $\Xmat_{i}$ e $\uvec_{i}$ são finitos.

Portanto, $\Avar[\sqrt{N}(\betahatbold - \betavec)]$ é \textbf{consistentemente} estimado por $\Amathat^{-1} \Bmathat \Amathat^{-1}$, e $\Avar(\betahatbold)$ é estimado como:

\vspace{-1 em}
\begin{align*}
	\Vmathat \equiv 
	\left( \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}  \right)^{-1}
	\left( \sum_{i=1}^{N} \Xmat_{i}' \uvechat_{i} \uvechat_{i}'  \Xmat_{i}  \right)
	\left( \sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}  \right)^{-1}.
\end{align*}

Sob as hipóteses \textbf{SOLS.1} e \textbf{SOLS.2}, nós fazemos inferência em $\betavec$ como $\betahatbold$ fosse normalmente distribuído com média $\betavec$ e variância $\Vmathat$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Dados de Painel (POLS)}
\noindent
\citet[C.7 -- Estimating Systems of Equations by OLS and GLS. p.143-179]{wool-2010} \\
\citet[Sec.7.8 -- The Linear Panel Data Model, Revisited. p.169]{wool-2010} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelo Linear para Dados de Painel}

No caso de dados de painel, temos a seguinte amostra aleatória:

\vspace{-1 em}
\begin{align}\label{pols:mod}
	y_{it} &= \xvec_{it} \betavec + u_{it} \, , \quad i=1, \dots, N, \quad t=1, \dots, T.
\end{align}

\noindent onde
\vspace{-1 ex}
\begin{description}[noitemsep]
	\item[$y_{it}$] é um escalar.
	\item[$\betavec$] é um vetor $K \times 1$.
	\item[$\xvec_{it}$] é um vetor $1 \times K$.
	\item[$u_{it}$] é um escalar.
\end{description}

\vspace{-2 em}
\begin{align*}
	\underset{1 \times K}{\xvec_{it}} = 
	\begin{bmatrix}
		x_{1,it} & \dots & x_{K, it}
	\end{bmatrix}
	\quad
	\underset{K \times 1}{\betavec} = 
	\begin{bmatrix}
		\beta_{1} \\ \vdots \\ \beta_{K}
	\end{bmatrix}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Notação Vetorial:}
$\yvec_{i} = \Xmat_{i} \betavec + \uvec_{i}$,  pra cada $i=1, \dots, N$.

\noindent onde
\vspace{-1 ex}
\begin{description}[noitemsep]
	\item[$\yvec_{i}$] é um vetor $T \times 1$.
	\item[$\betavec$] é um vetor $K \times 1$.
	\item[$\Xmat_{i}$] é uma matriz $T \times K$.
	\item[$\uvec_{it}$] é um vetor $T \times 1$.
\end{description}

% X vecs
\vspace{-1 em}
\begin{align*}
	\underset{T \times K}{\Xmat_{i}} = 
	\begin{bmatrix}
		\xvec_{1} \\ \vdots \\ \xvec_{T}
	\end{bmatrix}
	=
	\begin{bmatrix}
		x_{1,i1} & \dots  & x_{K, i1}	\\
		\vdots          & \ddots &  \vdots \\
		x_{1,iT} & \dots  & x_{K, iT}
	\end{bmatrix}
	\quad
	\underset{T \times 1}{\yvec_{i}} = 
	\begin{bmatrix}
		y_{i1} \\ \vdots \\ y_{iT}
	\end{bmatrix}
	\quad
	\underset{T \times 1}{\uvec_{i}} = 
	\begin{bmatrix}
		u_{i1} \\ \vdots \\ u_{iT}
	\end{bmatrix}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Notação Matricial:}
$\yvec = \Xmat \betavec + \uvec$

\noindent onde
\vspace{-1 ex}
\begin{description}[noitemsep]
	\item[$\yvec$] é um vetor $NT \times 1$.
	\item[$\betavec$] é um vetor $K \times 1$.
	\item[$\Xmat$] é uma matriz $NT \times K$.
	\item[$\uvec$] é um vetor $NT \times 1$.
\end{description}

% XMAT
\begin{align*}
	\underset{NT \times K}{\ddot{\Xmat}} = 
	\begin{bmatrix}
		\ddot{\Xmat}_{1} \\ \vdots \\ \ddot{\Xmat}_{N}
	\end{bmatrix}
	=
	\begin{bmatrix}
		\ddot{x}_{1,11} & \dots & \ddot{x}_{K, 11} \\
		\vdots          &       &  \vdots          \\
		\ddot{x}_{1,1T} & \dots & \ddot{x}_{K, 1T} \\
		\ddot{x}_{1,21} & \dots & \ddot{x}_{K, 21} \\
		\vdots          &       &  \vdots          \\
		\ddot{x}_{1,2T} & \dots & \ddot{x}_{K, 2T} \\
		\vdots          &       &  \vdots          \\
		\ddot{x}_{1,N1} & \dots & \ddot{x}_{K, N1} \\
		\vdots          &       &  \vdots          \\
		\ddot{x}_{1,NT} & \dots & \ddot{x}_{K, NT}
	\end{bmatrix}
	\quad
% Y vecs
	\underset{NT \times 1}{\ddot{\yvec}} = 
	\begin{bmatrix}
		\ddot{\yvec}_{1} \\ \vdots \\ \ddot{\yvec}_{N}
	\end{bmatrix}
	=
	\begin{bmatrix}
		\ddot{y}_{11} \\ \vdots \\ \ddot{y}_{1T} \\
		\ddot{y}_{21} \\ \vdots \\ \ddot{y}_{2T} \\
		\vdots \\
		\ddot{y}_{N1} \\ \vdots \\ \ddot{y}_{NT} \\
	\end{bmatrix}
	\quad
	\underset{NT \times 1}{\ddot{\uvec}} = 
	\begin{bmatrix}
		\ddot{\uvec}_{1} \\ \vdots \\ \ddot{\uvec}_{N}
	\end{bmatrix}
	=
	\begin{bmatrix}
		\ddot{u}_{11} \\ \vdots \\ \ddot{u}_{1T} \\
		\ddot{u}_{21} \\ \vdots \\ \ddot{u}_{2T} \\
		\vdots \\
		\ddot{u}_{N1} \\ \vdots \\ \ddot{u}_{NT} \\
	\end{bmatrix}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimação}

\begin{align*}
	\Xmat' \Xmat 
	=
	\sum_{i=1}^{N} \Xmat_{i}' \Xmat_{i}
	=
	\sum_{i=1}^{N} \sum_{t=1}^{T} \xvec_{it}' \xvec_{it};
	\quad
	\Xmat' \yvec 
	=
	\sum_{i=1}^{N} \Xmat_{i}' \yvec_{i}
	=
	\sum_{i=1}^{N} \sum_{t=1}^{T} \xvec_{it}' y_{it}.
\end{align*}

Portanto, podemos escrever $\betahatbold$ como:

\vspace{-1 em}
\begin{align} \label{betahat:POLS}
	\Aboxed{
		\betahatbold^{POLS} =
		\left( \sum_{i=1}^{N} \sum_{t=1}^{T} \xvec_{it}' \xvec_{it} \right)^{-1}
		\left( \sum_{i=1}^{N} \sum_{t=1}^{T} \xvec_{it}' y_{it} \right)
	}.
\end{align}

Este estimador é chamado \textbf{estimador de Mínimos Quadrados Agrupados (POLS)} porque ele corresponde a rodar uma regressão OLS nas observações agrupadas através de $i$ e $t$. 
% This estimator is called the \textbf{pooled ordinary least squares (POLS) estimator} because it corresponds to running OLS oin the observation pooled across $i$ and $t$.
O estimador da equação \eqref{betahat:POLS} é o mesmo para unidades de \textit{cross section} amostradas em diferentes pontos do tempo.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hipóteses}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{POLS.1} $\E(\Xmat_{i}' \uvec_{i}) = \E(\xvec_{it}'u_{it}) = \zerovec_{K \times 1}$, para cada
$i=1, \dots, N$ e $t=1, \dots, T$.

\noindent
De fato, \textbf{POLS.1} $\implies$ \textbf{SOLS.1}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Obs:}
O modelo \eqref{pols:mod} permite $y_{i,t-1}$ como regressor, se satisfeita \textbf{POLS.1}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Alguns Testes}

Lembrando a equação do modelo \eqref{pols:mod}:

\vspace{-2 em}
\begin{align*}
	y_{it} = \xvec_{it} \betavec + u_{it}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Autocorrelação dos Resíduos}

Nos dois testes apresentado, primeiro precisamos guardar os resíduos estimado.
Para tanto, rodamos a regressão do modelo \eqref{pols:mod} e guardamos os resíduos:

\vspace{-1 em}
\begin{align} \label{pols:uhat}
	\uhat_{it} = y_{it} - \xvec_{it} \betavechat^{POLS}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Com Exogeneidade Estrita}
Sob exogeneidade estrita, rodamos a seguinte regressão dos resíduos:

\vspace{-1.5 em}
\begin{align*}
	\uhat_{it} = \delta_{0} + \delta \uhat_{it-1}  + \err_{it}
	\, , \quad i=1,\dots, N; \quad t=1,\dots, T.
\end{align*}

\noindent
Então, testamos

\vspace{-1 em}
\begin{align*}
% \left\{
% \begin{aligned}
	H_{0} : \delta_{1} = 0
	\\
	H_{1} : \delta_{1} \neq 0
% \end{aligned}
% \right.
\end{align*}

\noindent
via teste $t$ (pode ser robusto).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Sem Exogeneidade Estrita (Apenas exogeneidade contemporânea)}
Sem exogeneidade estrita, rodamos a seguinte regressão dos resíduos:

\vspace{-1.5 em}
\begin{align*}
	\uhat_{it} = \xvec_{it} \alphavec + \delta \uhat_{it-1}  + \err_{it}
	\, , \quad i=1,\dots, N; \quad t=1,\dots, T.
\end{align*}

\noindent
Então, testamos

\vspace{-1 em}
\begin{align*}
% \left\{
% \begin{aligned}
	H_{0} : \delta_{1} = 0
	\\
	H_{1} : \delta_{1} \neq 0
% \end{aligned}
% \right.
\end{align*}

\noindent
via teste $t$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Heterocedasticidade}
Com os resíduos da equação \eqref{pols:uhat}, rodamos a seguinte regressão:

\vspace{-1 em}
\begin{align*}
	\uhat^{2}_{it} = \gamma_{0} + \gamma_{1} \yhat_{it}' + \gamma_{2} \yhat^{2}_{it} + \err_{it}
\end{align*}

\noindent
onde $\yhat_{it} = \xvec_{it} \betavechat^{POLS}$.
Definindo $\hvec_{it} = ( \yhat_{it}', \yhat^{2}_{it} )$, podemos reescrever a equação acima como

\vspace{-1 em}
\begin{align*}
	\uhat^{2}_{it} = \gamma_{0} + \hvec_{it} \gammavec + \err_{it}
\end{align*}

\noindent
Então, testamos

\vspace{-1 em}
\begin{align*}
% \left\{
% \begin{aligned}
	H_{0} &: \gammavec = \zerovec \quad (\gamma_{1} = 0 \text{ e } \gamma_{2} = 0 )
	\\
	H_{1} &: \gammavec \neq \zerovec
% \end{aligned}
% \right.
\end{align*}

\noindent
via teste de Wald.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Teste de Wald}

Se é verdade que

\vspace{-1 em}
\begin{align*}
	\sqrt{N} (\betavechat - \betavec) 
	\arrowd
	\mathcal{N}(\zerovec, \Vmathat).
\end{align*}

\noindent
Seja $\Rmat$ uma matriz $Q \times K$ com $Q \leq K$ e $\posto(\Rmat) = Q$ (posto pleno), então

\vspace{-1 em}
\begin{align*}
	\sqrt{N} \Rmat (\betavechat - \betavec) 
	\arrowd
	\mathcal{N}(\zerovec, \Rmat \Vmathat \Rmat').
\end{align*}

e

\vspace{-1 em}
\begin{align*}
	\left[ \sqrt{N} \Rmat (\betavechat - \betavec)  \right]'
	\left( \Rmat \Vmat \Rmat' \right)^{-1}
	\left[ \sqrt{N} \Rmat (\betavechat - \betavec)  \right]
	\asim
	\chisq_{Q} 
\end{align*}

O resulado acima vale para $\Vmathat$ no lugar de $\Vmat$, desde que $\Vmathat \arrowp \Vmat$.
Ou seja, vale para estimadores \textbf{consistentes} de $\Vmat$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Teste de Wald}

\begin{align*}
% \left\{
% \begin{aligned}
	H_{0} &: \Rmat \betavec = \rvec
	\\
	H_{1} &: \Rmat \betavec \neq \rvec
% \end{aligned}
% \right.
\end{align*}

A estatística do teste acima é:

\vspace{-1 em}
\begin{align*}
	N
	\left[ \Rmat \betavechat - \rvec  \right]'
	\left( \Rmat \Vmathat \Rmat' \right)^{-1}
	\left[ \Rmat \betavechat - \rvec  \right]
	\asim
	\chisq_{Q} 
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Remarks}
\begin{enumerate}
	\item $\Vmathat$ pode ser a matriz robusta.

	\item Uma aproximação, via distribuição $F$ é dado por:
		\begin{align*}
			\frac{\text{Est. Teste}}{Q} \asim F_{Q, N-K}
		\end{align*}

		\noindent
		com $\Avar(\betavechat) = \frac{N}{N-K} \Vmathat$.
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Modelo de Efeitos Não Observados (UEM)}
\noindent
\citet[C.10 -- Basic Linear Unobserved Effects Panel Data Models, p.247--291]{wool-2010} \\
\citet[Sec.10.1 -- Motivation: The Omitted Variables Problem, p.247]{wool-2010}\\
\citet[Sec.10.2 -- Assumptions about the Unobserved Effects and Explanatory Variables, p.251]{wool-2010}\\
\citet[Sec.10.3 -- Estimating UEM by POLS, p.256]{wool-2010}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelo UEM}

O modelo básico de efeitos não observados (UEM) pode ser escrito para uma amostra \textit{cross-section} aleatória $i$ como:

\vspace{-1 em}
\begin{align}\label{uem:mod:1}
	y_{it} = \xvec_{it} \betavec + c_{i} + u_{it} \, , \quad t=1,\dots, T.
\end{align}
onde $c_{i}$ é o efeito não observado (componente não observado, variável latente, heterogeneidade não observada, efeito individual, heterogeneidade individual).
Estamos supondo $c_{i}$ \textbf{não} observável.

\vspace{1 em}
\noindent
Definindo os erros compostos $v_{it} = c_{i} + u_{it}$, temos:

\vspace{-1 em}
\begin{align}\label{uem:mod:2}
	y_{it} &= \xvec_{it} \betavec + v_{it}
\end{align}
Ou, em forma de vetor:

\vspace{-1 em}
\begin{align}\label{uem:mod:vec}
	\yvec_{i} &= \Xmat_{i} \betavec + \vvec_{i}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimação e Consistência}
\noindent
\citet[Sec.10.3 -- Estimating UEM by POLS, p.256]{wool-2010}.

\vspace{1 em}
Se usarmos o estimador POLS na equação \eqref{uem:mod:1}, o estimador será consistente se:

\vspace{-1 em}
\begin{align*}
	\E(\xvec_{it}' v_{it}) = \zerovec \, , \quad t=1, \dots T.
\end{align*}

Ou seja, precisamos que:

\vspace{-1 em}
\begin{align*}
	& 
	\E(\xvec_{it}' u_{it}) = \zerovec \, , \quad t=1, \dots T.
	\\
	&
	\E(\xvec_{it}' c_{i}) = \zerovec \, , \quad t=1, \dots T.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{description}[]
	\item [Caso 1:] $\E(\xvec_{it}' c_{i}) = \zerovec$. \\
		\textbf{POLS} é consistente, mas não é eficiente.\\
		\textbf{Efeitos Aleatórios} é consistente e eficiente.\\
		EA é o \textbf{FGLS}  do modelo.

	\item [Caso 2:] $\E(\xvec_{it}' c_{i}) \neq \zerovec$. \\
		Se POLS é \textbf{inconsistente}. \\
		Nesse caso, usaremos o Modelo de \textbf{Efeitos Fixos} ou \textbf{Primeira Diferença}. \\
		EF e PD é o POLS nos modelos transformados.
\end{description}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Obs:}
Modelos com variáveis dependentes defasadas em $\xvec_{it}$ \textit{devem} violar a hipótese $\E(\xvec_{it}'u_{it} ) = \zerovec$ uma vez que $y_{i, t-1}$ e $c_{i}$ devem ser correlacionados.
Considerando $y_{i, t-1}$ como regressor:

\vspace{-1 em}
\begin{align*}
	\left.
		\begin{aligned}
			y_{it} &= \alpha y_{i, t-1} + \xvec_{it} \betavec + v_{it}
			\\
			y_{it-1} &= \alpha y_{i, t-2} + \xvec_{i,t-1} \betavec + v_{i,t-1}
		\end{aligned}
	\right\}
	\Cov(y_{i, t-1}, v_{it}) \neq 0.
\end{align*}
Mesmo se $\E(\xvec_{it}'u_{it} ) = \zerovec$  é verdadeiro, os erros compostos serão serialmente correlacionados devido a presença de $c_{i}$ em cada período de tempo.
Portanto, a inferência do POLS requer um estimador robusto de matriz de covariância e estatísticas robustas de teste.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Modelo de Efeitos Fixos (EF), (Fixed Effects FE)}
\noindent
\citet[Sec.10.5 -- Fixed Effects Methods, p.265]{wool-2010}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelo}

O modelo linear de \textbf{efeitos individuais não observados (UEM)}:

\vspace{-1 em}
\begin{align} \label{fe:mod:1}
	y_{it} &= \xvec_{it} \betavec + c_{i} + u_{it} 
	\, , \qquad	i = 1, \dots, N; \quad t = 1, \dots, T.
\end{align}

\noindent
Estamos supondo $c_{i}$ \textbf{não} observável.
Definindo $v_{it} = c_{i} + u_{it}$.

\vspace{-1 em}
\begin{align}\label{fe:mod:2}
	y_{it} &= \xvec_{it} \betavec + v_{it}.
	\, , \qquad	i = 1, \dots, N; \quad t = 1, \dots, T.
\end{align}

\noindent
No modelo FE permitimos $\Cov(\xvec_{it}, c_{i} ) \neq 0$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Transformação \textit{Within}}

Tirando a média do modelo ao longo de 
$t = 1, \dots, T$:

\vspace{-1 em}
\begin{align} \label{fe:mod:av} % eq:10.45 p.267
	\overline{y}_{i} = 
	\overline{\xvec}_{i} \betavec + \overline{c}_{i} + \overline{u}_{i} 
	\, , \qquad	i = 1, \dots, N.
\end{align}

\noindent
onde:

\vspace{-1 em}
\begin{align*}
	\overline{y}_{i} = T^{-1} \sum_{t=1}^{T} y_{it};
	\quad
	\overline{\xvec}_{i} = T^{-1} \sum_{t=1}^{T} \xvec_{it};
	\quad
	\overline{c}_{i} = T^{-1} \sum_{t=1}^{T} c_{i} = c_{i};
	\quad
	\overline{u}_{i} = T^{-1} \sum_{t=1}^{T} u_{it}.
\end{align*}

Então, subtraindo \eqref{fe:mod:av} de \eqref{fe:mod:1}:

\vspace{-1 em}
\begin{align*} % \label{fe:mod:ddot}
	y_{it} - \overline{y}_{i} &=
	(\xvec_{it} - \overline{\xvec}_{i}) \betavec +
	\underbracket[1pt]{c_{i} - \overline{c}_{i}}_{=0} +
	u_{it} - \overline{u}_{i} 
	\, , \qquad	i = 1, \dots, N; \quad t = 1, \dots, T.
\end{align*}

\noindent
Finalmente, obtemos:

\vspace{-1 em}
\begin{align} \label{fe:ddot}
	\ddot{y}_{it} &= \ddot{\xvec}_{it} \betavec + \ddot{u}_{it} 
	\, , \qquad	i = 1, \dots, N; \quad t = 1, \dots, T.
\end{align}

\noindent
Onde 
$\ddot{y}_{it} \equiv y_{it} - \overline{y}_{i}$, 
$\ddot{\xvec}_{it} \equiv \xvec_{it} - \overline{\xvec}_{i}$ e
$\ddot{u}_{it} \equiv u_{it} - \overline{u}_{i}$.
E eliminamos variáveis que não variam ao longo do tempo.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Notação Vetorial e Matriz Centralizadora (\textit{Centering Matrix}) $\Mzero$}

Utilizando notação vetorial para o modelo linear de \textbf{efeitos individuais não observados (UEM)}:

\vspace{-1 em}
\begin{align} \label{fe:mod:1:vec}
	\yvec_{i} &= \Xmat_{i} \betavec + c_{i} \onevec + \uvec_{i} 
	\, , \qquad	i = 1, \dots, N.
	\\
	\label{fe:mod:2:vec}
	\yvec_{i} &= \Xmat_{i} \betavec + \vvec_{it}.
	\, , \qquad	i = 1, \dots, N.
\end{align}

\noindent
Agora, definimos a matriz $\Mzero$ (\citet[p. 268]{wool-2010} usa a notação $\Qmat_{T}$ para essa matriz) como:

\vspace{-1 em}
\begin{align*}
	\Mzero &=
	\Imat_{T} - \onevec_{T} (\onevec_{T}' \onevec_{T})^{-1} \onevec_{T}'.
	=
	\Imat_{T} - T^{-1} \onevec_{T} \onevec_{T}'
\end{align*}

\noindent
A matriz $\Mzero$ tem dimensão $T \times T$.
Além disso, ela é idempotente ($\Mzero \Mzero = \Mzero$) e simétrica (\Mzero' = \Mzero).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{1 em}
Podemos transformar o modelo \eqref{fe:mod:1:vec} ao premultiplicarmos todo o modelo por $\Mzero$:

\vspace{-1 em}
\begin{align*} 
	\Mzero \yvec_{i} &= ( \Imat_{T} - T^{-1} \onevec_{T} \onevec_{T}')\yvec_{i}	= \yvec_{i} - \onevec_{T} \overline{y}_{i} = \ddot{\yvec}_{i}
	\\
	\Mzero \Xmat_{i} &=
	\Xmat_{i} - T^{-1} \onevec_{T} \onevec_{T}' \overline{\Xmat}_{i} =
	\Xmat_{i} - \onevec_{T} \overline{\xvec}_{i} =
	\ddot{\Xmat}_{i}
	\\
	\Mzero \uvec_{i} &= ( \Imat_{T} - T^{-1} \onevec_{T} \onevec_{T}')\uvec_{i}	= \uvec_{i} - \onevec_{T} \overline{u}_{i} = \ddot{\uvec}_{i}
	\\
	\Mzero ( c_{1} \onevec_{T} ) &= 
	( \Imat_{T} - T^{-1} \onevec_{T} \onevec_{T}' ) c_{i} \onevec_{T} =
	c_{i}( \Imat_{T} - T^{-1} \onevec_{T} \onevec_{T}' ) \onevec_{T} =
	c_{i}( \onevec_{T} - \onevec_{T} ) = \zerovec_{T}
\end{align*}

\noindent
onde $\overline{\xvec}_{i}$ é o vetor $1 \times K$ com a média dos $K$ regressores.

\vspace{-1 em}
\begin{align}
	\notag
	\Mzero \yvec_{i} &= \Mzero \Xmat_{i} \betavec + \Mzero ( c_{1} \onevec_{T} ) + \Mzero \uvec_{i},
	\quad i = 1, \dots, N.
	\\
	\label{fe:mod:ddot:vec}
	\ddot{\yvec}_{i} &= \ddot{\Xmat}_{i} \betavec + \ddot{\uvec_{i}},
	\quad i = 1, \dots, N.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Exemplo:} \citep[p.266]{wool-2010} Considere o modelo:

\vspace{-1.5 em}
\begin{align*}
	y_{it} =
	\beta_{0} + \beta_{2} d2_{t} + \dots + \beta_{T} dT_{t}
	+
	\zvec_{i} \deltavec + d2_{t} \zvec_{i} \deltavec_2  + \dots + dT_{t} \zvec_{i} \deltavec_{T} + 
	\xvec_{it} \alphavec + v_{it}
\end{align*}

Após a transformação:

\vspace{-1.5 em}
\begin{align*}
	\ddot{y}_{it} =
	\beta_{2} \ddot{d2}_{t} + \dots + \beta_{T} dT_{t}
	+
	\ddot{d2}_{t} \zvec_{i} \deltavec_2  + \dots + dT_{t} \zvec_{i} \deltavec_{T} + 
	\ddot{\xvec}_{it} \alphavec + \ddot{u}_{it}
\end{align*}

Então, não podemos estimar o coeficiente da variável sexo do indivíduo, por exemplo.
Mas podemos estimar se houve mudança desse efeito ao longo do tempo, em relação a categoria de referência.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hipóteses}

\paragraph{FE.1:} Exogeneidade Estrita:
$\E( u _{it} \, | \, \xvec_{i1}, \dots, \xvec_{iT}, c_{i}) = 0$, para $t=1, \dots, T$ e $i = 1, \dots, N$.

\paragraph{FE.2:} Posto pleno de $\E( \ddot{\Xmat}_{i}' \ddot{\Xmat}_{i} )$ (para inverter a matriz).
$\posto[ \E( \ddot{\Xmat}_{i}'  \ddot{\Xmat}_{i} ) ]  = K$.

\paragraph{FE.3:} Homoscedasticidade:
$\E( \uvec_{i} \uvec_{i}' \,|\, \Xmat_{i}, c_{i}) = \sigma_{u}^{2} \Imat_{T}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimação POLS}
\noindent
\citet[p.269]{wool-2010} 

\noindent
Aplicando POLS no modelo transformado \eqref{fe:mod:ddot:vec}, temos:

\vspace{-1 em}
\begin{align} \label{fe:betahat}
	\Aboxed{
		\betavechat^{FE} =
		\left( \sum_{i=1}^{N} \ddot{\Xmat}_{i}' \ddot{\Xmat}_{i} \right)^{-1}
		\left( \sum_{i=1}^{N} \ddot{\Xmat}_{i}' \ddot{\yvec}_{i} \right)
		=
		\left( \sum_{i=1}^{N} \sum_{t=1}^{T} \ddot{\xvec}_{it}' \ddot{\xvec}_{it} \right)^{-1}
		\left( \sum_{i=1}^{N} \sum_{t=1}^{T} \ddot{\xvec}_{it}' \ddot{y}_{it} \right)
	}
\end{align}

\noindent
Este estimador também é chamado de \textbf{estimador within}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Consistência}
\noindent
\citet[sec.10.5.1 -- Consistency of the Fixed Effects Estimator, p.265--269]{wool-2010} \\
\citet[p.269]{wool-2010} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1 em}
\noindent
Usando a equação \eqref{fe:mod:ddot:vec} em \eqref{fe:betahat}, temos:

\vspace{-1 em}
\begin{align} \label{fe:betahat:2}
	\betavechat^{FE} =
	\betavec +
	\left[ N^{-1} \sum_{i=1}^{N} \ddot{\Xmat}_{i}' \ddot{\Xmat}_{i} \right]^{-1}
	\left[ N^{-1} \sum_{i=1}^{N} \ddot{\Xmat}_{i}' \ddot{\uvec}_{i} \right]
\end{align}

\noindent
Nota que para $\E(\ddot{\Xmat}_{i}' \ddot{\uvec}_{i}) = \zerovec$ é necessário não haver correlação entre todos os erros 
$u_{it}$ $t=1, \dots, T$
e todos os regressores
$\xvec_{it}'$ $t=1, \dots, T$.
\textbf{FE.1} implica a condição acima.
Além disso, sob \textbf{FE.1}, $\betavechat^{FE}$ é não viciado.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Formato Totalmente Matricial}

Empilhando os vetores $N$ vezes, vamos definir:

\vspace{-1 em}
\begin{align*}
	\underset{NT \times K}{\ddot{\Xmat}} = 
	\begin{bmatrix}
		\ddot{\Xmat}_{1} \\ \vdots \\ \ddot{\Xmat}_{N}
	\end{bmatrix}
	=
	\begin{bmatrix}
		\ddot{x}_{1,11} & \dots & \ddot{x}_{K, 11} \\
		\vdots          &       &  \vdots          \\
		\ddot{x}_{1,1T} & \dots & \ddot{x}_{K, 1T} \\
		\ddot{x}_{1,21} & \dots & \ddot{x}_{K, 21} \\
		\vdots          &       &  \vdots          \\
		\ddot{x}_{1,2T} & \dots & \ddot{x}_{K, 2T} \\
		\vdots          &       &  \vdots          \\
		\ddot{x}_{1,N1} & \dots & \ddot{x}_{K, N1} \\
		\vdots          &       &  \vdots          \\
		\ddot{x}_{1,NT} & \dots & \ddot{x}_{K, NT}
	\end{bmatrix}
	\quad
% Y vecs
	\underset{NT \times 1}{\ddot{\yvec}} = 
	\begin{bmatrix}
		\ddot{\yvec}_{1} \\ \vdots \\ \ddot{\yvec}_{N}
	\end{bmatrix}
	=
	\begin{bmatrix}
		\ddot{y}_{11} \\ \vdots \\ \ddot{y}_{1T} \\
		\ddot{y}_{21} \\ \vdots \\ \ddot{y}_{2T} \\
		\vdots \\
		\ddot{y}_{N1} \\ \vdots \\ \ddot{y}_{NT} \\
	\end{bmatrix}
	\quad
% u vecs
	\underset{NT \times 1}{\ddot{\uvec}} = 
	\begin{bmatrix}
		\ddot{\uvec}_{1} \\ \vdots \\ \ddot{\uvec}_{N}
	\end{bmatrix}
	=
	\begin{bmatrix}
		\ddot{u}_{11} \\ \vdots \\ \ddot{u}_{1T} \\
		\ddot{u}_{21} \\ \vdots \\ \ddot{u}_{2T} \\
		\vdots \\
		\ddot{u}_{N1} \\ \vdots \\ \ddot{u}_{NT} \\
	\end{bmatrix}
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1 em}
\noindent
A matriz $\Xmat$ é $NT \times K$, a matriz $\Mzero$ é $T \times T$ e a matriz $\Imat_{N}$ é $N \times N$.
A produto \textbf{Kronecker} de $\Imat_{N}$ por $\Mzero$,

\vspace{-1 em}
\begin{align*}
	\underset{N \times N}{\Imat_{N}},
	\otimes
	\underset{T \times T}{\Mzero}
\end{align*}

\noindent
é uma matriz $NT \times NT$.
Dessa forma, podemos definir:

\vspace{-1 em}
\begin{align*}
	\ddot{\yvec} = ( \Imat_{N} \otimes \Mzero ) \yvec,
	\\
	\ddot{\Xmat} = ( \Imat_{N} \otimes \Mzero ) \Xmat
	\\
	\ddot{\uvec} = ( \Imat_{N} \otimes \Mzero ) \uvec,
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1 em}
\noindent
E com isso, reescrevermos \eqref{fe:betahat:2} como:

\vspace{-1 em}
\begin{align} \label{fe:betahat:3}
	\betavechat^{FE} =
	\betavec +
	\left( \ddot{\Xmat}' \ddot{\Xmat} \right)^{-1}
	\ddot{\Xmat}' \ddot{\uvec}
\end{align}

\noindent
Ou ainda, como:

\vspace{-1 em}
\begin{align}
	\notag
	\betavechat^{FE} &=
	\betavec +
	\left[ \Xmat' ( \Imat_{N} \otimes \Mzero )( \Imat_{N} \otimes \Mzero ) \Xmat \right]^{-1}
	\left[ \Xmat' ( \Imat_{N} \otimes \Mzero )( \Imat_{N} \otimes \Mzero ) \uvec \right]
	\\
	\label{fe:betahat:4}
	\betavechat^{FE} &=
	\betavec +
	\left[ \Xmat' ( \Imat_{N} \otimes \Mzero ) \Xmat \right]^{-1}
	\left[ \Xmat' ( \Imat_{N} \otimes \Mzero ) \uvec \right]
\end{align}

\noindent
onde usamos as propriedades de simetria e idempotência da matriz $\Mzero$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Matriz de Covariância Robusta}
\noindent
\citet[sec.10.5.2 -- Asymptotic Inference with Fixed Effects, p.269--272]{wool-2010} \\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
A matriz de covariância assintótica fica:

\vspace{-1 em}
\begin{align*}
	\E\left( \ddot{\Xmat}' \ddot{\Xmat} \right)^{-1}
	\E\left( \ddot{\Xmat}' \ddot{\uvec} \ddot{\uvec}' \ddot{\Xmat} \right)
	\E\left( \ddot{\Xmat}' \ddot{\Xmat} \right)^{-1}
\end{align*}

A qual pode ser estimada por

\vspace{-1 em}
\begin{align*}
	\left( \ddot{\Xmat}' \ddot{\Xmat} \right)^{-1}
	\left( \sum_{i=1}^{N} \ddot{\Xmat}'
		\widehat{\ddot{\uvec}} \widehat{\ddot{\uvec}}'
	\ddot{\Xmat} \right)
	\left( \ddot{\Xmat}' \ddot{\Xmat} \right)^{-1}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
Analisando
$\E\left( \ddot{\Xmat}_{i}' \ddot{\uvec}_{i} \ddot{\uvec}_{i}' \ddot{\Xmat}_{i} \right)$:

\vspace{-1 em}
\begin{align*}
	\E\left( \ddot{\Xmat}_{i}' \ddot{\uvec}_{i} \ddot{\uvec}_{i}' \ddot{\Xmat}_{i} \right)
	&=
	\E\left[
		(\Xmat_{i}' \Mzero')  (\Mzero \uvec_{i}) (\uvec_{i}' \Mzero')  (\Mzero \Xmat_{i})
	\right]
	\\ &=
	\E\left[
		(\Xmat_{i}' \Mzero') \uvec_{i} \uvec_{i}' (\Mzero \Xmat_{i})
	\right]
	\\ &=
	\E\left[
		\ddot{\Xmat}_{i}' \uvec_{i} \uvec_{i}'  \ddot{\Xmat}_{i}
	\right]
\end{align*}

\noindent
onde usamos as propriedades de simetria e idempotência da matriz $\Mzero$ e as definições de $\ddot{\Xmat}_{i}$ e $\ddot{\uvec}_{i}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
Sob \textbf{FE.3}, 
$\E(\uvec_{i} \uvec_{i}' | \ddot{\Xmat}_{i}) = \sigma^2_{u} \Imat_{T}$,
temos:

\vspace{-1 em}
\begin{align*}
	\E\left( \ddot{\Xmat}_{i}' \uvec_{i} \uvec_{i}' \ddot{\Xmat}_{i} \right)
	=
	\E\left[
		\E\left( 
			\ddot{\Xmat}_{i}' \uvec_{i} \uvec_{i}'  \ddot{\Xmat}_{i}
		| \Xmat_{i}, c_{i} \right)
	\right]
	=
	\E\left[
		\ddot{\Xmat}_{i}'
		\E\left( 
			\uvec_{i} \uvec_{i}' 
		| \Xmat_{i}, c_{i} \right)
		\ddot{\Xmat}_{i}
	\right]
	=
	\sigma^{2}_{u} \Imat_{T} \E(\ddot{\Xmat}_{i}' \ddot{\Xmat}_{i})
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Assim, a matriz de covariância fica:

\vspace{-1 em}
\begin{align*}
	\E\left( \ddot{\Xmat}' \ddot{\Xmat} \right)^{-1}
	\sigma^{2}_{u} \Imat_{T} \E(\ddot{\Xmat}_{i}' \ddot{\Xmat}_{i})
	\E\left( \ddot{\Xmat}' \ddot{\Xmat} \right)^{-1}
	=
	\boxed{
		\sigma^{2}_{u} 
	\E\left( \ddot{\Xmat}' \ddot{\Xmat} \right)^{-1} }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Estimando Elementos da Matriz de Covariância}
\noindent
Queremos estimar $\sigma^2_{u}$ por valores amostrais:

\vspace{-1 em} 
\begin{align*}
	\E( u_{it}^2) &= \sigma^2_{u}
	\quad \text{ e } \quad
	\E( \ddot{u}_{it}^2) = \sigma^{2}_{\ddot{u}}
\end{align*}

\vspace{-1 em} 
\begin{align*}
	\sigma^{2}_{\ddot{u}} &=
	\E[ ( u_{it} - \overline{u}_{i})^2] = 
	\E( u_{it}^2) + \E( \overline{u}_{i}^2) - 2 \E( u_{it} \overline{u}_{i})
\end{align*}

\noindent
utilizando $\overline{u}_{i} = T^{-1} \sum_{t=1}^{T} u_{it}$:

\vspace{-1 em} 
\begin{align*}
	\E( \ddot{u}_{it}^2) 
	&=
	\E( u_{it}^2) + T^{-1} \sum_{t=1}^{T} \E( u_{it}^2) - 2 \E( u_{it} T^{-1} \sum_{t=1}^{T} u_{it})
	= 
	\E( u_{it}^2) + T^{-1} \sum_{t=1}^{T} \E( u_{it}^2) - 2T^{-1} \sum_{t=1}^{T} \E( u^2_{it})
	\\
	&= 
	\sigma^2_{u} + \sigma^2_{u}/T - 2\sigma^2_{u}/T = \sigma^2_{u}(1 - 1/T)
	\implies
	\boxed{\sigma^2_{u} = \frac{T}{T-1} \sigma^2_{\ddot{u}} }
\end{align*}

\noindent
Utilizando estimadores amostrais para $\E(\ddot{u}_{it}^{2})$:

\vspace{-1 em} 
\begin{align*}
	\sigmahat_{u}^2
	&= 
	\frac{T}{T-1} \frac{1}{NT}
	\sum_{i=1}^{N} \sum_{t=1}^{T}  
	\widehat{\ddot{u}}_{it}^2
\end{align*}

\noindent
Ajustando os Graus de Liberdade (Cortando $T$s e subtraindo $K$ do número de regressores):

\vspace{-1 em} 
\begin{align}\label{fe:sighat}
	\sigmahat^2_{u} = \frac{1}{N(T-1) - K}
	\sum_{i=1}^{N} \sum_{t=1}^{T} \widehat{\ddot{u}}^2_{it}.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Teste para Autocorrelação AR(1)}
\noindent
\citet[p.275]{wool-2010}

\vspace{-1 em} 
\begin{align*}
	\E( \ddot{u}_{it} \ddot{u}_{i,t-1} ) &=
	\E\left[ ( u_{i,t} - \overline{u}_{i} ) ( u_{i,t-1} - \overline{u}_{i} ) \right]
	\\
	&=
	\E( u_{i,t} u_{i,t-1} ) - 
	\E( u_{i,t} \overline{u}_{i} ) -
	\E( \overline{u}_{i} u_{i,t-1} ) +
	\E( \overline{u}^2_{i} )
	\\
	&=
	0 - T^{-1} \sigma^2_{u} - T^{-1} \sigma^2_{u} + T^{-1} \sigma^2_{u}
	\\
	\E( \ddot{u}_{it} \ddot{u}_{i,t-1} ) 
	&= -T^{-1} \sigma_{u}^2
\end{align*}

\vspace{-1 em} 
\begin{align*}
	\Corr( \ddot{u}_{it}, \ddot{u}_{i,t-1} ) &=
	\frac{\E( \ddot{u}_{it} \ddot{u}_{i,t-1} )}{\E( \ddot{u}_{it}^2 )} =
	\frac{-T^{-1} \sigma_{u}^2}{\frac{T-1}{T}\sigma^2_{u}} = 
	\frac{-1}{T-1}
\end{align*}

Vamos testar 

\vspace{-1 em} 
\begin{align*}
	H_{0}: \delta = \frac{-1}{T-1}
\end{align*}
(ausência de correlação em \red{$\uvec$}) na equação:

\vspace{-1 em} 
\begin{align*}
	\widehat{\ddot{u}}_{it} = \delta \widehat{\ddot{u}}_{it-1} + e_{it}
\end{align*}
para $t=2, \dots, T$.
Fazer teste $t$ robusto.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Primeira Difereça (First Difference, FD, PD)}
\noindent
\citet[Sec.10.6 -- First Difference Methods, p.279]{wool-2010}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelo}

O modelo linear de \textbf{efeitos não observados}:

\vspace{-1 em}
\begin{align} \label{fd:mod:1}
y_{it} &= \xvec_{it} \betavec + c_{i} + u_{it},
\qquad i = 1, \dots, N, \text{ e } \quad t = 1, \dots, T.
\\
\notag
y_{i,t-1} &= \xvec_{i,t-1} \betavec + c_{i} + u_{i,t-1},
\end{align}

\vspace{-2 em}
\begin{align} 
	\notag
y_{it} - y_{i,t-1} &=
(\xvec_{it} - \xvec_{i,t-1}) \betavec
+ c_{i} - c_{i} + u_{it} - u_{i,t-1}
\\
\label{fd:mod:delta}
\Delta y_{it} &= \Delta \xvec_{it}\betavec + \Delta u_{it}
\qquad i = 1, \dots, N, \text{ e } \quad t = 2, \dots, T.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Definindo 
$e_{it} = \Delta u_{it}$, 
reescrvemos \eqref{fd:mod:delta} no formato matricial empilhando $T$:

\vspace{-1 em}
\begin{align} \label{fd:mod:delta:mat}
\Delta \yvec_{i} = \Delta \Xmat_{i} \betavec + \mbs{e}_{i}
\qquad i = 1, \dots, N.
\end{align}

\noindent onde, 
\vspace{-1 ex}
\begin{itemize}\itemsep0pt
  \item[$\Delta \yvec_{i}$] é um vetor $( T - 1 ) \times 1$ 
  \item[$\Delta \Xmat_{i}$] é uma matriz  $( T - 1 ) \times K$
  \item[$\Delta \xvec_{it}$] é a $(t-1)$-ésima linha da matriz $\Delta \Xmat_{i}$.
  \item[$\betavec$] é um vetor $K \times 1$
  \item[$\mbs{e}_{i}$] é um vetor $(T - 1 ) \times 1$
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Matriz $\Dmat$}

Vamos definir $\Dmat$ como  a matriz $(T-1) \times T$ como a matriz bidiagonal cuja diagonal inferior é $-1$ e a diagonal superior é $1$.
Assim, 

\vspace{-1 em}
\begin{align*}
\Dmat =
\begin{bmatrix}
-1 & 1 &  & 0
\\
& \ddots & \ddots & 
\\
0 & & -1 & 1
\end{bmatrix}
\end{align*}

\noindent
E podemos escrever $\Delta \yvec_{i}$ como:

\vspace{-1 em}
\begin{align*}
	\Delta \yvec_{i} = \Dmat \yvec_{i}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimação POLS}

O estimador $\betavechat^{FD}$ é o POLS da regressão no modelo \eqref{fd:mod:delta:mat}, assim:

\vspace{-1 em}
\begin{align} \label{fd:betahat}
\Aboxed{
\betavechat^{FD} =
\left( \sum_{i=1}^{N} \Delta \Xmat_{i}' \Delta \Xmat_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \Delta \Xmat_{i}' \Delta \yvec_{i} \right)
}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hipóteses}

As Hipóteses que usamos para $\widehat{\betavec}^{FD}$ são:

\begin{description}
\setlength\itemsep{.5 em}

\item [FD.1:] Exogeneidade Estrita:
$\E( u _{it} \, | \, \xvec_{i1}, \dots, \xvec_{iT}, c_{i}) = 0$, para $t=1, \dots, T$ e $i = 1, \dots, N$.

\item  [FD.2:] Posto completo de $\E( \Delta \Xmat_{i}' \Delta \Xmat_{i} )$ (para inverter a matriz).
$\posto[ \E( \Delta \Xmat_{i} ' \Delta \Xmat_{i} ) ]  = K$.

\item [FD.3:] Homoscedasticidade:
$\E(\mbs{e}_{i} \mbs{e}_{i}' \,|\, \Xmat_{i}, c_{i}) = \sigma_{e}^{2} \Imat_{T-1}$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Consistência}

Usando \eqref{fd:mod:delta:mat} em \eqref{fd:betahat}:

\vspace{-1 em}
\begin{align*}
\betavec^{FD} &=
\betavec +
\left( \sum_{i=1}^{N} \Delta \Xmat_{i}' \Delta \Xmat_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \Delta \Xmat_{i}' \mbs{e}_{i} \right)
\end{align*}

\noindent
\textbf{FD.1} é suficiente para
$\E(\Delta \Xmat_{i}' \ebold) = \zerovec$, $i =1, \dots, N$.
Uma condição necessária para $\betavechat^{FD} \arrowp \betavec$ é 
$\E(\xvec_{it} u_{it}) = \E(\xvec_{it} u_{i,t-1}) = 0$, para $i =1, \dots, N$ e $t=2, \dots, T$.
Note que sob \textbf{FD.1}, 
$\E(\betavechat^{FD} | \xvec_{i1}, \dots, \xvec_{it}, c_{i}) = \betavec$.
Ou seja, $\betavechat^{FD}$ é não viciado.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variância}

\vspace{-1 em}
\begin{align*} 
\Cov( \betavec^{FD} ) = 
\E\left( \Delta \Xmat_{i}' \Delta \Xmat_{i} \right)^{-1}
\E\left( \Delta \Xmat_{i}' \ebold_{i}  \ebold_{i}' \Delta \Xmat_{i} \right)
\E\left( \Delta \Xmat_{i}' \Delta \Xmat_{i} \right)^{-1}
\end{align*}

Usando estimadores amostrais:

\begin{center}
	\red{\Large rever}
\end{center}

\vspace{-1 em}
\begin{align*} 
\widehat{\Cov}( \betavec^{FD} ) &= 
\left( N^{-1} \sum_{i=1}^{N} \Delta \Xmat_{i}' \Delta \Xmat_{i} \right)^{-1}
\left( N^{-1} \sum_{i=1}^{N} \Delta \Xmat_{i}' \ebold_{i}  \ebold_{i}' \Delta \Xmat_{i} \right)
\left( N^{-1} \sum_{i=1}^{N} \Delta \Xmat_{i}' \Delta \Xmat_{i} \right)^{-1}
\\ &=
\red{N}
( \Delta \Xmat' \Delta \Xmat_{i} )^{-1}
\left( \sum_{i=1}^{N} \Delta \Xmat_{i}' \ebold_{i}  \ebold_{i}' \Delta \Xmat_{i} \right)
( \Delta \Xmat' \Delta \Xmat )^{-1} 
\end{align*}

\noindent onde $\Delta \Xmat$ é a matriz $N(T-1) \times K$ das matrizes $\Delta \Xmat_{i}$ empilhadas.

\begin{align*}
\Delta \Xmat = 
\begin{bmatrix}
	\Delta \Xmat_{1} \\	\vdots \\ \Delta \Xmat_{N}
\end{bmatrix}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Variância sob Homocedasticidade}
Usando \textbf{FD.3}, temos

\vspace{-1 em}
\begin{align*} 
\E\left( \Delta \Xmat_{i}' \ebold_{i} \ebold_{i} \Delta \Xmat_{i} \right) &= 
\E\left[ \E\left(
\Delta \Xmat_{i}' \ebold_{i} \ebold_{i} \Delta \Xmat_{i} |   
\xvec_{i1}, \dots, \xvec_{iT}, c_{i}
\right) \right]
\\ &=
\E\left( \Delta \Xmat_{i}' \sigma_{e}^2 \Imat_{T-1} \Delta \Xmat_{i} \right)
=
\sigma_{e}^2 \E\left( \Delta \Xmat_{i}'\Delta \Xmat_{i} \right)
\end{align*}

Então, sob \textbf{FD.3}:

\vspace{-1 em}
\begin{align*} 
\widehat{\Cov}( \betavec^{FD} ) &= 
\sigma_{e}^2 \E\left( \Delta \Xmat_{i}'\Delta \Xmat_{i} \right)^{-1}
\end{align*}

\noindent onde


\vspace{-1 em}
\begin{align*} 
\sigma_{e}^2 = 
\frac{1}{N(T-1) - K} \sum_{i=1}^{N}\sum_{t=2}^{T} \widehat{e}^{2}_{it}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Teste para autocorrelação AR(1) dos resíduos}

A equação do erro $e_{it}$ é 

\vspace{-1 em}
\begin{align*}
e_{it} = u_{it} - u_{i,t-1}
\end{align*}

\noindent 
rearranjando os termos, encontramos

\vspace{-1 em}
\begin{align*}
	\boxed{u_{it} =  u_{i,t-1} + e_{it} } ,
\qquad t=2, \dots, T.
\end{align*}

\noindent
que é um passeio aleatório.
Sob \textbf{FD.3}, 
$e_{it} \sim \rb(0, \sigma^2_{e})$ e $u_{it}$ é um passeio aleatório.

\vspace{-1 em}
\begin{align*}
	\ehat_{it} = \delta \ehat_{it-1}  + \err_{it},
	\qquad  t=2, \dots, T.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
Note que sob \textbf{FD.1}, 

\vspace{-1 em}
\begin{align*}
	\E\left( e_{it} | \xvec_{i1}, \dots, \xvec_{iT}, c_{i} \right) = 0
\end{align*}

\vspace{-2 em}
\begin{align*}
	H_{0}: \delta = 0
	\\
	H_{1}: \delta \neq 0
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Sob \textbf{FD.3},

\vspace{-1 em}
\begin{align*}
\E\left( e_{it}^2 \right) &=
\E\left[ (u_{it} - u_{it-1})^2 \right]
\\ &=
\E\left[ u_{it}^2 + u_{it-1}^2 - 2u_{it}u_{it-1} \right]
= 2 \sigma_{u}^2
\end{align*}

\vspace{-1.5 em}
\begin{align*}
\E\left( e_{it} e_{i,t-1} \right) &=
\E\left[ (u_{it} - u_{it-1}) (u_{it-1} - u_{it-2}) \right]
\\ &=
\E\left[ u_{it}u_{it-1} - u_{it}u_{it-2} - u_{it-1}^{2} + u_{it-1}u_{it-2} \right]
\\ &=
\E(- u_{it-1}^2) = - \E(u_{it-1}^2) = - \sigma_{u}^2
\end{align*}

\vspace{-1.5 em}
\begin{align*}
\E\left( e_{it} e_{i,t-2} \right) &=
\E\left[ (u_{it} - u_{it-1}) (u_{it-2} - u_{it-3}) \right]
= 0
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Assim, temos que, sob \textbf{FD.3},
$\E(\ebold_{i} \ebold_{i}')$
é uma matriz tridiagonal \red{$XX \times XX$}:

\vspace{-1 em}
\begin{align*}
\E(\ebold_{i} \ebold_{i}') =
\begin{bmatrix}
2 \sigma_{u}^{2} & - \sigma_{u}^{2} & & & 0
\\
- \sigma_{u}^{2} & \ddots & \ddots
\\
 & \ddots & \ddots & \ddots
%%%%
\\
& & \ddots & \ddots & - \sigma_{u}^{2}
\\
0 & & & - \sigma_{u}^{2} & 2 \sigma_{u}^{2}
\end{bmatrix}
\end{align*}

\noindent
onde
\begin{align*}
	\Cov(e_{it}, e_{it-1}) = \frac{-\sigma_{u}^{2}}{2 \sigma_{u}^{2}} = \frac{-1}{2}.
\end{align*}

\noindent
Assim, podemos testar $H_{0}: \delta = -1/2$ na seguinte equação

\vspace{-1 em}
\begin{align*}
	\ehat_{it} = \delta \ehat_{it-1} + \err_{it}.
\end{align*}

\noindent
Se \textbf{não rejeitar} $H_{0}$ temos evidência \textbf{favorável} a FE.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Teste de Exogeneidade Estrita}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Teste para o Estimador FD}

\vspace{-2 em}
\begin{align*}
	\Delta y_{it} = \Delta \xvec_{it} \betavec + \wvec_{it} \gammavec + e_{it}
\end{align*}

\noindent
onde $\wvec_{it}$ é um subconjunto de $\xvec_{it}$ (excluindo as dummies de tempo).

Testamos

\vspace{-2 em}
\begin{align*}
	H_{0}: \gammavec = \zerovec
	\\
	H_{1}: \gammavec \neq \zerovec
\end{align*}

\noindent
via teste de Wald (Robusto).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Teste para o Estimador FE}

\vspace{-2 em}
\begin{align*}
	y_{it} = \xvec_{it} \betavec + \wvec_{it} \gammavec + v_{it}
\end{align*}

Testamos

\vspace{-2 em}
\begin{align*}
	H_{0}: \gammavec = \zerovec
	\\
	H_{1}: \gammavec \neq \zerovec
\end{align*}

\noindent
Sob $H_{0}$, podemos estimar o modelo via FE.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Alguns Detalhes}

\begin{remark}
Como no modelo FE, perdemos as variáveis constantes no tempo.
Por exemplo, uma variável cujo incremento é igual a $1$ a cada período (e.g. experiência profissional), causa multicolinearidade perfeita, pois

\vspace{-1 em}
\begin{align*}
	\Delta d_{2} + \dots + \Delta d_{T} = \Delta exper.
\end{align*}
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Avaliação de Políticas}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
Se $T=2$:

\vspace{-1.5 em}
\begin{align*}
y_{i1} &= \xvec_{i1} \betavec + \delta prog_{i1} + v_{i1}
\\
y_{i2} &= \xvec_{i2} \betavec + \delta prog_{i2} + v_{i2}
\end{align*}

\noindent
onde $prog_{i1} =0$ para todo mundo.
Vamos considerar

\vspace{-1 em}
\begin{align*}
\Delta y_{i2} &= \Delta \xvec_{i2} \betavec + \delta prog_{i2} + e_{i2}
\end{align*}

\noindent
como $prog_{i1} =0$, então $prog_{i2} = prog_{i2} - prog_{i1} = \Delta prog_{i2}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1 em}
\noindent
Para $T>2$:

\vspace{-1.5 em}
\begin{align*}
\Delta y_{i2} &= \Delta\xvec_{i2}\betavec + \delta \Delta prog_{i2} + e_{i2},
\qquad t=2, \dots, T.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Exemplo}
\begin{align*}
	\log(scrap_{it}) =
	\xvec_{it} \betavec + \delta_{1} grant_{t} + \delta_{2} grant_{t-1} + v_{it}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{System GLS (SGLS)}

\noindent
\citet[Sec.7.4 -- Consistency and Asymptotic Normality of Generalized Least Squares, p.153]{wool-2010} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelo Linear}

\vspace{-2 em}
\begin{align*}
\yvec_{i} &= \Xmat_{i}\betavec + \uvec_{i},
\end{align*}

\noindent
onde

\vspace{-1 em}
\begin{description}[noitemsep]
	\item[$\yvec$] é um vetor $G \times 1$
	\item[$\Xmat_{i}$] é uma matriz $G \times K$
	\item[$\betavec$] é um vetor $K \times 1$
	\item[$\uvec$] é um vetor $G \times 1$
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hipóteses}

\paragraph{SGLS.1:} $\E(\Xmat_{i} \otimes \uvec_{i}) = {\zerovec}_{G^2 \times K}$.

\noindent
Isso implica que se $\Xmat_{i}$ contém constante, 

\vspace{-1 em}
\begin{align*}
	\E(u_{ig}) = 0, \quad g=1,\dots, G.
\end{align*}

\noindent
Nesse caso, implica também que $\E(x_{ikg} u_{ig})=0$, $k=1,\dots,K$, $g=1,\dots,G$.

\paragraph{SGLS.2:}  $\Omegamat = \E(\uvec_{i} \uvec_{i}^{\prime})$.

\noindent
Com $\Omegamat$ positiva definida (para ter inversa).

\paragraph{SGLS.3:}
$\E\left( \Xmat_{i}' \Omegamatinv \uvec_{i} \uvec_{i}' \Omegamatinv \Xmat_{i} \right) = \E\left( \Xmat_{i}' \Omegamatinv \Xmat_{i} \right)$.

\noindent
É suficiente para \textbf{SGLS.3} supor 
$\E\left( \uvec_{i} \uvec_{i}' | \Xmat_{i} \right) = \Omegamat$,
pois

\vspace{-1 em}
\begin{align*}
\E\left( \Xmat_{i}' \Omegamatinv \uvec_{i} \uvec_{i}' \Omegamatinv \Xmat_{i} \right) &=
\E\left[ 
\E\left( \Xmat_{i}' \Omegamatinv \uvec_{i} \uvec_{i}' \Omegamatinv \Xmat_{i} | \Xmat_{i} \right)
\right]
\\ &=
\E\left[ 
\Xmat_{i}' \Omegamatinv
\underbracket[.75 pt]{\E\left(  \uvec_{i} \uvec_{i}'  | \Xmat_{i} \right)}_{=\Omegamat}
\Omegamatinv \Xmat_{i}
\right]
\\ &=
\E\left( \Xmat_{i}' \Omegamatinv \Xmat_{i} \right)
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimação}
O estimador SOLS é consistente, nesse caso, mas sua matriz de covariância é dada por:

\vspace{-1 em}
\begin{align*}
	\Avar(\betavechat^{SOLS}) = 
	\E\left( \Xmat_{i}' \Xmat_{i} \right)^{-1}
	\E\left( \Xmat_{i}' \uvec_{i} \uvec_{i}' \Xmat_{i} \right)
	\E\left( \Xmat_{i}' \Xmat_{i} \right)^{-1},
\end{align*}

\noindent
supondo $\E\left( \uvec_{i} \uvec_{i}' | \Xmat_{i} \right) = \Omegamat$,

\vspace{-1 em}
\begin{align*}
	\Avar(\betavechat^{SOLS}) = 
	\E\left( \Xmat_{i}' \Xmat_{i} \right)^{-1}
	\E\left( \Xmat_{i}' \Omegamat \Xmat_{i} \right)
	\E\left( \Xmat_{i}' \Xmat_{i} \right)^{-1}.
\end{align*}

É possível encontrar $\Omegamat^{1/2}$ e $\Omegamat^{-1/2}$ tais que:

\vspace{-1 em}
\begin{align*}
\Omegamat^{1/2} \Omegamat^{1/2} = \Omegamat 
\quad \text{ e } \quad
\Omegamat^{-1/2} \Omegamat^{-1/2} = \Omegamatinv
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Estimação}

Agora, transformamos o sistema de equações ao realizarmos a pré-multiplicação do sistema por $\Omegamat^{-1/2}$:
% \vspace{-1.5 em}
\begin{align}
\notag
\Omegamat^{-1/2} \yvec_{i} &= \Omegamat^{-1/2}\Xmat_{i}\betavec + \Omegamat^{-1/2} \uvec_{i}
\\
\label{sgls:mod:star}
\yvec_{i}^{*} &= \Xmat_{i}^{*}\betavec + \uvec^{*}_{i}
\end{align}

\vspace{-1.5 em}
\begin{align*}
\E\left( \uvec_{i}^{*}\uvec_{i}^{*}' \right) =
\E\left( \Omegamat^{-1/2} \uvec_{i} \uvec_{i}' \Omegamat^{-1/2} \right) =
\Omegamat^{-1/2} 
\underbracket[.75 pt]{\E\left( \uvec_{i} \uvec_{i}' \right)}_{\Omegamat = \Omegamat^{1/2} \Omegamat^{1/2}} 
\Omegamat^{-1/2} =
\Imat_{G}.
\end{align*}

\vspace{-1.5 em}
\begin{align*}
\E\left( \Xmat_{i}^{*}' \uvec_{i}^{*} \right) =
\E\left( \Xmat_{i}' \Omegamat^{-1/2} \Omegamat^{-1/2} \uvec_{i} \right) =
\E\left( \Xmat_{i}' \Omegamatinv \uvec_{i} \right) =
\zerovec_{K \times 1}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Então, o estimador SOLS aplicado ao modelo transformado \eqref{sgls:mod:star} nos fornece um estimador consistente e eficiente.

\vspace{-1.5 em}
\begin{align*}
\betavechat^{SGLS}
&=
\left( \sum_{i=1}^{N} \Xmat_{i}^{*'} \Xmat_{i}^{*} \right)^{-1}
\left( \sum_{i=1}^{N} \Xmat_{i}^{*'} \yvec_{i}^{*} \right)
\\
&=
\left( \sum_{i=1}^{N} \Xmat_{i}^{'} \Omegamat^{-1/2} \Omegamat^{-1/2} \Xmat_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \Xmat_{i}^{'} \Omegamat^{-1/2} \Omegamat^{-1/2} \yvec_{i} \right)
\\
&=
\left( \sum_{i=1}^{N} \Xmat_{i}^{'} \Omegamat^{-1} \Xmat_{i} \right)^{-1}
\left( \sum_{i=1}^{N} \Xmat_{i}^{'} \Omegamat^{-1} \yvec_{i} \right)
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
No formato puramente matricial:

\vspace{-1.5 em}
\begin{align*}
\Aboxed{
\betavechat^{SGLS} &=
\left[ \Xmat \left( \Imat_{N} \otimes \Omegamatinv \right) \Xmat \right]^{-1}
\left[ \Xmat \left( \Imat_{N} \otimes \Omegamatinv \right) \yvec \right]
}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variância}

\vspace{-1 em}
\begin{align*}
\Avar(\betavechat^{SGLS}) &= 
\E\left( \Xmat_{i}^{*}' \Xmat_{i}^{*} \right)^{-1}
\E\left( \Xmat_{i}^{*}' \uvec_{i}^{*} \uvec_{i}^{*}' \Xmat_{i}^{*} \right)
\E\left( \Xmat_{i}^{*}' \Xmat_{i}^{*} \right)^{-1}
\\ &=
\E\left( \Xmat_{i}' \Omegamatinv \Xmat_{i} \right)^{-1}
\E\left( \Xmat_{i}' \Omegamatinv \uvec_{i} \uvec_{i}' \Omegamatinv \Xmat_{i} \right)
\E\left( \Xmat_{i}' \Omegamatinv \Xmat_{i} \right)^{-1}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
Sob \textbf{SGLS.3:}

\vspace{-1 em}
\begin{align*}
\Avar(\betavechat^{SGLS}) &= 
\E\left( \Xmat_{i}' \Omegamatinv \Xmat_{i} \right)^{-1}
\E\left( \Xmat_{i}' \Omegamatinv  \Xmat_{i} \right)
\E\left( \Xmat_{i}' \Omegamatinv \Xmat_{i} \right)^{-1}
=
\E\left( \Xmat_{i}' \Omegamatinv \Xmat_{i} \right).
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Esimtação FSGLS (SGLS Factível)}
\noindent
\citet[Sec.7.5 -- Feasible GLS, p.153]{wool-2010} \\

Para obtermos $\betavec^{SGLS}$ precisamos conhecer $\Omegamat$, o que não ocorre na prática.
Então, precisamos estimar $\Omegamat$ com um estimador consistente.
Para tanto usamos um procedimento de dois passos:

\begin{enumerate}
\item  % Passo 1
Estimar $\yvec_{i} = \Xmat_{i} \betavec + \uvec_{i}$ via \textbf{SOLS} e guardar o resíduo estimado $\widehat{\uvec}_{i}$.

\item  %Passo 2
Estimar $\Omegamat$ com o seguinte estimador $\widehat{\Omegamat}$:

\vspace{-1.5 em}
\begin{align*}
\widehat{\Omegamat} 
= 
N^{-1} \sum_{i=1}^{N} \uvec_{i} \uvec_{i}'
\end{align*}

\noindent
Note que $\Omegamathat \arrowp \Omegamat$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Com a estimativa $\widehat{\Omegamat}$ feita, podemos obter $\betavechat^{FSGLS}$ pela fórmula do $\betavec^{SGLS}$:

\vspace{-1.5 em}
\begin{align}
\label{fgls:betahat}
\betavechat^{FGLS}
= 
\left[ \sum_{i=1}^{N} \Xmat_{i}' \widehat{\Omegamat}^{-1} \Xmat_{i} \right]^{-1}
\left[ \sum_{i=1}^{N} \Xmat_{i}' \widehat{\Omegamat}^{-1} \yvec_{i} \right]
\end{align}

Empilhando as $N$ observações, podemos expressar $\betavechat^{FGLS}$ matricialmente:

\vspace{-1.5 em}
\begin{align}
\label{fgls:betahat:mat}
\betavechat^{FGLS}
= 
\left[ \Xmat' \left( \Imat_{N} \otimes \widehat{\Omegamat}^{-1} \right) \Xmat \right]^{-1}
\left[ \Xmat' \left( \Imat_{N} \otimes \widehat{\Omegamat}^{-1} \right) \yvec \right]
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Consistência FGLS}
Reescrevendo a equação \eqref{fgls:betahat:mat}:

\vspace{-1.5 em}
\begin{align*}
\betavechat^{FGLS} &= 
\left[ \Xmat' \left( \Imat_{N} \otimes \widehat{\Omegamat}^{-1} \right) \Xmat \right]^{-1}
\left[ \Xmat' \left( \Imat_{N} \otimes \widehat{\Omegamat}^{-1} \right) (\Xmat \betavec + \uvec) \right]
\\ &= 
\left[ \Xmat' \left( \Imat_{N} \otimes \widehat{\Omegamat}^{-1} \right) \Xmat \right]^{-1}
\left\{ 
\left[ \Xmat' \left( \Imat_{N} \otimes \widehat{\Omegamat}^{-1} \right) \Xmat \betavec \right] +
\left[ \Xmat' \left( \Imat_{N} \otimes \widehat{\Omegamat}^{-1} \right) \uvec \right]
\right\}
\\
\Aboxed{ \betavechat^{FGLS} &= 
\betavec +
\left[ \Xmat' \left( \Imat_{N} \otimes \widehat{\Omegamat}^{-1} \right) \Xmat \right]^{-1}
\left[ \Xmat' \left( \Imat_{N} \otimes \widehat{\Omegamat}^{-1} \right) \uvec \right] }
\end{align*}

\noindent
Então, 
$\betavechat^{FSGLS} \arrowp \betavec$,
se 
$\Omegamathat \arrowp \Omegamat$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variância FGLS}

\vspace{-1 em}
\begin{align*}
\Varhat(\betavechat^{FGLS}) &= 
\left[ \Xmat' \left( \Imat_{N} \otimes \Omegamathatinv \right) \Xmat \right]^{-1}
\left( \sum_{i=1}^{N}
\Xmat_{i}' \Omegamathatinv \uvechat_{i} \uvechat_{i}' \Omegamathatinv \Xmat_{i} 
\right)
\left[ \Xmat' \left( \Imat_{N} \otimes \Omegamathatinv \right) \Xmat \right]^{-1}
\end{align*}

\noindent
onde
$\uvechat_{i} = \yvec_{i} - \Xmat_{i} \betavechat^{FGLS}$.

\paragraph{Sob Homocedasticidade (SGLS.3)}

\begin{align*}
\Varhat(\betavechat^{FGLS}) &= 
\left[ \Xmat' \left( \Imat_{N} \otimes \Omegamathatinv \right) \Xmat \right]^{-1}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Testes}

Podemos fazer o teste $t$ e de Wald.

Além disso, podemos comparar o modelo restrito e irrestrito.

\begin{align*}
\text{Est. Teste} =
\left[ 
\uvectil'\left( \Imat \otimes \Omegamathatinv \right) \uvectil
-
\uvechat' \left( \Imat \otimes \Omegamathatinv \right) \uvechat
\right]
\end{align*}

\noindent
Sob $H_{0}$, 
$\text{Est. Teste} \sim \chisq_{Q}$.
Onde,

\begin{description}[noitemsep]
	\item[$\uvectil$] é o vetor de resíduos FGLS do modelo restrito.
	\item[$\uvechat$] é o vetor de resíduos FGLS do modelo irrestrito.
	\item[$\Omegamathat$] é construída com os resíduos SOLS do modelo irrestrito.
	\item[$Q$] é o número de restrições.
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Modelo de Efeitos Aleatórios (EA, Random Effects, RE)}
\noindent
\citet[Sec.10.4 -- Random Effects Methods, p.257]{wool-2010}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelo}

O modelo linear de \textbf{efeitos não observados}:

\vspace{-1 em}
\begin{align} 
\label{re:mod}
y_{it} &= \xvec_{it}\betavec + c_{i} + u_{it},
\qquad t = 1, \dots, T \quad \text{ e } \quad i = 1, \dots, N.
\\
\label{re:mod:vec}
\yvec_{i} &= \Xmat_{i}\betavec + c_{i} \onevec + \uvec_{i},
\qquad i = 1, \dots, N.
\\
\label{re:mod:mat}
\yvec &= \Xmat\betavec + c_{i} \onevec + \uvec.
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hipóteses}

As Hipóteses que usamos para $\mbs{\widehat{\beta}}^{RE}$ são:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{hypo}[\textbf{RE.1}]
Usamos o modelo correto e $c_{i}$ não é endógeno.

\begin{enumerate}[label =\alph*)]
\item 
$\E( u_{it} \, | \,  \xvec_{i1}, \dots, \xvec_{iT}, c_{i} ) = 0, \qquad i = 1, \dots, N$.

\item        
$\E( c_{i} \, | \, \xvec_{i1}, \dots, \xvec_{iT} ) = \E( c_{i} ) = 0, \qquad i = 1, \dots, N$.
\end{enumerate}
\end{hypo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{hypo}[\textbf{RE.2}]
Posto completo de $\E( \Xmat_{i}' \Omegamat^{-1} \Xmat_{i} )$.
\end{hypo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Definindo a matriz $T \times T$,
$\Omegamat \equiv \E(\mbs{v}_{i} \mbs{v}_{i}')$,
queremos que 
$\E( \Xmat_{i} \Omegamat^{-1} \Xmat_{i} )$
tenha posto completo (posto = $K$).

A matriz $\Omegamat$ é simétrica 
$\Omegamat' = \Omegamat$
e positiva definida $\det(\Omegamat) > 0$.
Assim podemos achar $\Omegamat^{1/2}$ e $\Omegamat^{-1/2}$ com 
$\Omegamat = \Omegamat^{1/2} \Omegamat^{1/2}$ e
$\Omegamat^{-1} = \Omegamat^{-1/2} \Omegamat^{-1/2}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{hypo}[\textbf{RE.3}]
\item

\begin{enumerate}[label =\alph*)]
\item 
$\E( \uvec_{i} \uvec_{i}' \, | \,  \xvec_{i1}, \dots, \xvec_{iT}, c_{i} ) = \sigmasq_{u}\Imat_{T}$;

\item        
$\E( c_{it} \, | \, x_{i1}, \dots, x_{iT} ) = \E( c_{i} ) = \sigmasq_{c}$.
\end{enumerate}
\end{hypo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark} \item
	\textbf{RE.1 (a)} está presente em FE e FD.

\noindent
\textbf{RE.1 (b)} está presente em POLS e implica ausência de correlação entre $c_{i}$ e os regressores do modelo.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimação}

Vamos considerar o modelo:

\vspace{-1 em}
\begin{align*} 
y_{it} &= \xvec_{it}\betavec + v_{it},
\qquad t = 1, \dots, T \quad \text{ e } \quad i = 1, \dots, N.
\\
\yvec_{i} &= \Xmat_{i}\betavec + \vvec_{i},
\qquad i = 1, \dots, N.
\\
\yvec &= \Xmat\betavec + \vvec.
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Sob \textbf{RE.3:}
\begin{align*} 
\E(v_{it}v_{it}) &= 
\E( c_{i}^{2} + 2c_{i} u_{it} + u_{it}^{2}) =
\sigmasq_{c} + \sigmasq_{u} = \sigmasq_{v}
\\
\E(v_{it}v_{is})	&=
\E[ ( c_{i} + u_{it} ) ( c_{i} + u_{is} ) ]
=
\E( c_{i}^{2} + c_{i} u_{is} + u_{it} c_{i} + u_{it} u_{is} )
=
\sigma_{c}^{2}.
\end{align*}

\noindent
Então, 
\begin{align*}
\E(\mbs{v}_{i} \mbs{v}_{i}') =
\begin{bmatrix}
	\sigmasq_{c} + \sigmasq_{u} & \sigmasq_{c} & \dots & \sigmasq_{c} \\	
	\sigmasq_{c} & \sigmasq_{c} + \sigmasq_{u} & \dots & \sigmasq_{c} \\	
	\vdots & \vdots & \ddots & \vdots \\
	\sigmasq_{c} & \sigmasq_{c} & \dots & \sigmasq_{c} + \sigmasq_{u}
\end{bmatrix}
=
\begin{bmatrix}
\sigmasq_{c} + \sigmasq_{u}  & & \sigmasq_{c} \\	
& \ddots & \\
\sigmasq_{c} & & \sigmasq_{c} + \sigmasq_{u}
\end{bmatrix}
\equiv
\Omegamat
\end{align*}

\noindent
Também poderíamos escrever:

\vspace{-1 em}
\begin{align*}
\boxed{\Omegamat = \E(\mbs{v}_{i} \mbs{v}_{i}') =
\sigmasq_{u} \Imat_{T} + \sigmasq_{c} \onevec_{T} \onevec_{T}'}
\end{align*}

\noindent
onde
$\sigma^{2}_{u} \Imat_{T}$ 
é uma matriz diagonal, e 
$\sigma_{c}^{2} \onevec_{T} \onevec_{T}'$ é uma matriz com todos os elementos iguais a $\sigma_{c}^{2}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Estimador RE}

\vspace{-1.5 em}
\begin{align}
\label{re:betahat}
\betavechat^{RE}
= 
\left[ \Xmat' \left( \Imat_{N} \otimes \Omegamathatinv \right) \Xmat \right]^{-1}
\left[ \Xmat' \left( \Imat_{N} \otimes \Omegamathatinv \right) \yvec \right]
\end{align}

\noindent
onde

\vspace{-1 em}
\begin{align*}
\Omegamathat =
\begin{bmatrix}
\sigmasqhat_{v} & & \sigmasqhat_{c} \\	
& \ddots & \\
\sigmasqhat_{c} & & \sigmasqhat_{v}
\end{bmatrix}
\end{align*}

\noindent
onde conseguimos estimar $\sigma_{v}^{2}$ e $\sigma_{c}^{2}$ por estimadores amostrais:

\vspace{-1.5 em}
\begin{align*}
\hat{\sigma}_{v}^{2} &=
(NT - K)^{-1} \sum_{i=1}^{N} \sum_{t=1}^{T} \vhat_{it}^2
\\
\hat{\sigma}_{c}^{2} &=
\left[ N \frac{T ( T-1 )}{2} - K  \right]^{-1}
\sum_{i=1}^{N} \sum_{t=1}^{T-1} \sum_{s=t+1}^{T} \vhat_{it} \vhat_{is}
\end{align*}

\noindent
e $\vhat_{it}$ é o resíduo de POLS do modelo \eqref{re:mod}

\vspace{-1 em}
\begin{align*}
\vhat_{it}^{POLS} = 
y_{it} - \yhat_{it}^{POLS} =
y_{it} - \xvec_{it} \mbs{\hat{\beta}}^{POLS}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inferência}

Robusta:
\begin{align*} 
\Var( \betavechat^{RE} ) = 
\E \left\{ 
\left[ \Xmat' ( \Imat_{N} \otimes \Omegamathatinv ) \Xmat \right]^{-1}
\left[ \sum_{i=1}^{N}
\Xmat_{i}'\Omegamathatinv\vvechat_{i} \vvechat_{i}' \Omegamathatinv \Xmat_{i} \right]
\left[ \Xmat' ( \Imat_{N} \otimes \Omegamathatinv ) \Xmat \right]^{-1}
\right\},
\end{align*}

Sob Homocedasticidade:
\begin{align*} 
\Aboxed{
\Var( \betavechat^{RE} ) = 
E
\left[ \Xmat' ( \Imat_{N} \otimes \Omegamathatinv ) \Xmat \right] }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Testes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Teste para presença de componente não observável}

\begin{align*}
H_{0}&: \sigmasq_{c} = 0
\\     
H_{1}&: \sigmasq_{c} \neq 0
\end{align*}

Estatística do teste:
\begin{align*}
\text{Est. Teste} =
\dfrac{ \sum_{i=1}^{N} \sum_{t=1}^{T-1} \sum_{s=t+1}^{T} \vhat_{it}\vhat_{is} }
{\left[ \sum_{i=1}^{N} \left(  \sum_{t=1}^{T-1} \sum_{s=t+1}^{T} \vhat_{it}\vhat_{is} \right)^{2} \right]^{1/2}}
\end{align*}

\noindent
Sob $H_{0}$, $\text{Est. Teste} \sim \nor(0,1)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Teste de Hausman (FE v. RE)}

\begin{align*}
H_{0}&: \text{Tanto FE, quanto RE são consistentes}
\\    
H_{1}&: \text{Apenas FE é consistente}
\end{align*}

Estatística do teste:
\begin{align*}
\text{Est. Teste} =
\left( \deltavechat_{FE} - \deltavechat_{RE}  \right)
\left[ \Varhat(\deltavechat_{FE}) - \Varhat(\deltavechat_{RE})  \right]^{-1}
\left( \deltavechat_{FE} - \deltavechat_{RE}  \right)
\end{align*}

\noindent
onde $\deltavechat$ é um vetor $M \times 1$ e $M$ é o número de regressores que variam no tempo.

\noindent
Sob $H_{0}$, 
\begin{align*}
\text{Est. Teste} \asim \chisq_{M}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Variáveis Instrumentais}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelo \textit{cross-section}}

\vspace{-2 em}
\begin{align} \label{iv:mod}
y_{i} = \beta_{0} + \beta_{1} x_{1i} + \beta_{2} x_{2i} + \err_{i}
\; ; \quad i = 1, \dots, N.
\end{align}

\noindent
Vamos supor: 

\vspace{-1 em}
\begin{itemize}[noitemsep]
\item[] $x_{1}$ é exógena.
\item[] $x_{2}$ é endógena.
\end{itemize}

Assim, precisamos encontrar um instrumento $z_{i}$ para $x_{2i}$, uma vez que queremos estimar $\beta_{0}$, $\beta_{1}$ e $\beta_{2}$ de maneira consistente.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Endogeneidade}
A variável explicativa $x_{k}$ é dita \textbf{endógena} se ela for correlacionada com erro.
Se $x_{k}$ for não correlacionada com o erro, então $x_{k}$ é dita \textbf{exógena}.

Endogeneidade surge, normalmente, de três maneiras diferentes:

\vspace{-1 em}
\begin{enumerate}[noitemsep]
\item Variável Omitida;
\item Simultaneidade;
\item Erro de Medida.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Características de Bom Instrumento}

Para $z_{i}$ ser um bom instrumento precisamos que $z$ tenha:

\vspace{-1 em}
\begin{enumerate}[noitemsep]
\item $Cov(z_{i}, \err_{_i}) = 0$ $\implies$  $z$ é exógena.
\item $Cov(z_{i}, x_{2i}) \neq 0$ $\implies$  correlação com $x_{2i}$ após controlar para outras variáveis.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimação}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{1$\textordmasculine$ Estágio}
Estimar $\hat{x}_{2i}$ via MQO e guardar valores \textit{fittados} de:
% \vspace{-1 em}
\begin{align*}
	x_{2i} = \alpha_{0} + \alpha_{1} x_{1i} + \alpha_{2} z_{i} + v_{i}.
\end{align*}

\noindent
\textbf{Note que:}
	$\Cov(x_{1i}, \err_{i}) = \Cov(z_{i}, \err_{i}) = 0 \implies \Cov(\xhat_{2i}, \err_i) = 0$.	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{2$\textordmasculine$ Estágio}

\begin{align*}
	y_{i} = \beta_{0} + \beta_{1} x_{1i} + \beta_{2} \xhat_{2i} + \err_{i}
\end{align*}

\vspace{-2 em}
\begin{align*}
	\betahat_{1} \arrowp \beta_{1}
\quad	\text{ e } \quad
	\betahat_{2} \arrowp \beta_{2}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Formato Matricial}

\vspace{-2 em}
\begin{align*}
	\betavechat^{MQLE} = \left( \Xmathat' \Xmathat \right)^{-1} \Xmathat' \yvec
\end{align*}

\noindent 
$\Xmathat$ são as variáveis exógenas do modelo estrutural e preditos no primeiro estágio.
Podemos representar $\Xmathat$ por

\vspace{-1 em}
\begin{align*}
	\Xmathat = \Zmat \left( \Zmat' \Zmat  \right)^{-1} \Zmat' \Xmat = \PZ \Xmat
\end{align*}

\noindent
onde $\Zmat$ é a matriz contendo todas as variáveis exógenas do problema (exógenas da eq. estrutural) mais os instrumentos.

Além disso, definimos a matriz de projeção em $\Zmat$.
\begin{align*}
	\PZ = \Zmat \left( \Zmat' \Zmat  \right)^{-1} \Zmat' 
\end{align*}

\noindent
Pelas propriedades das matrizes de projeção, $\PZ$ é simétrica e idempotente:
\begin{align*}
	\PZ' = \PZ \quad \text{e} \quad \PZ \PZ = \PZ.
\end{align*}

Então, 
% \vspace{-1 em}
\begin{align*}
\betavechat^{MQLE} &= \left( \Xmathat' \Xmathat \right)^{-1} \Xmathat' \yvec
= \left( \Xmat' \PZ \PZ \Xmat \right)^{-1} \Xmat' \PZ \yvec
= \left( \Xmat' \PZ \Xmat \right)^{-1} \Xmat' \PZ \yvec 
= \betavechat^{IV}
\\
\betavechat^{IV} &= \betavec + 
\left( \Xmat' \PZ \Xmathat \right)^{-1} \Xmat' \PZ \mbs{\varepsilon}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{System Instrumental Variable (SIV)}

\vspace{-2 em}
\begin{align} \label{siv:mod}
\yvec_{i} = \Xmat_{i} \betavec + \uvec_{i};
\quad i = 1, \dots, N,
\quad t = 1, \dots, T.
\end{align}

\noindent
onde 
\begin{description}[noitemsep]
\item[$\yvec_{i}$] é um vetor $T \times 1$,
\item[$\Xmat_{i}$] é uma matriz $T \times K$,
\item[$\betavec$] é o vetor de coeficientes $K \times 1$,
\item[$\uvec_{i}$] é o vetor de erros $T \times 1$.
\end{description}

Se é verdade que há endogeneidade em \eqref{siv:mod}, então:

\vspace{-1 em}
\begin{align*}
\E(\Xmat_{i}^{\prime} \uvec_{i}) \neq 0
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Seja $\Zmat_{i}$ uma matriz $T \times L$ com $L \geq K$ de variáveis exógenas (incluindo o instrumento).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{hypo}[\textbf{SIV.1}]
\begin{align*}
\E(\Zmat_{i}' \uvec_{i}) = 0
\end{align*}
\end{hypo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{hypo}[\textbf{SIV.2}]
\begin{align*}
	\posto \left[ \E\left( \Xmat_{i}' \Zmat_{i} \right) \right] = 0
\end{align*}
\end{hypo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{align*}
\yvec_{i} = 
\begin{bmatrix}
	y_{1} \\ \vdots \\ y_{T}		
\end{bmatrix},
\quad
\uvec_{i} = 
\begin{bmatrix}
	u_{1} \\ \vdots \\ u_{T}		
\end{bmatrix},
\quad
\Xmat_{i} = 
\begin{bmatrix}
	\xvec_{i1} \\ \vdots \\ \xvec_{iT}		
\end{bmatrix},
\end{align*}

Se os instrumentos forem os mesmos em todas as equações:
\begin{align*}
	\Zmat_{i} = 
\begin{bmatrix}
	\zvec_{i1} \\ \vdots \\ \zvec_{iT}		
\end{bmatrix}.
\end{align*}

Se os instrumentos forem os diferentes:
\begin{align*}
	\Zmat_{i} = 
\begin{bmatrix}
\zvec_{i1} & & \\
& \ddots & \\
& &	\zvec_{iT}		
\end{bmatrix}_{T \times L},
\end{align*}

\noident
onde $L = L_{1}, \dots, L_{T}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Estimação}

O estimador \textbf{SIV} é dado por:
\begin{align*}
\betavechat^{SIV} &=
\left[
	\left( \sum_{i=1}^{N} \Xmat_{i}' \Zmat_{i} \right) 
	\left( \sum_{i=1}^{N} \Zmat_{i}' \Zmat_{i} \right)^{-1} 
	\left( \sum_{i=1}^{N} \Zmat_{i}' \Xmat_{i} \right) 
\right]^{-1}
\left[
	\left( \sum_{i=1}^{N} \Xmat_{i}' \Zmat_{i} \right) 
	\left( \sum_{i=1}^{N} \Zmat_{i}' \Zmat_{i} \right)^{-1} 
	\left( \sum_{i=1}^{N} \Zmat_{i}' \yvec_{i} \right) 
\right]
\end{align*}

\noindent
No formato puramente matricial:
\begin{align*}
\betavechat^{SIV} &=
\left[ \Xmat' \Zmat \left( \Zmat' \Zmat \right)^{-1} \Zmat' \Xmat \right]^{-1}
\left[ \Xmat' \Zmat \left( \Zmat' \Zmat \right)^{-1} \Zmat' \yvec \right]
\\
\betavechat^{SIV} &= 
\left( \Xmat' \PZ \Xmat  \right)^{-1} \left( \Xmat' \PZ \yvec \right).
\end{align*}

\noindent
O estimador SIV também é chamado de S2SLS.

\noindent
A consistência é garantida por \textbf{SIV.1} e utilizando estimador amostral:
\begin{align*}
	N^{-1} \sum_{i=1}^{N} \Zmat_{i}' \uvec_{i} \arrowp \E(\Zmat_{i}' \uvec_{i} ) = 0.
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Matriz de Covariância}

\vspace{-2 em}
\begin{align*}
\Avar(\betavechat^{SIV}) = &
\left[ \E( \Xmat_{i}' \Zmat_{i} ) \E( \Zmat_{i}' \Zmat_{i} )^{-1} \E( \Zmat_{i}' \Xmat_{i} ) \right]^{-1}
\\ &
\left[
\E( \Xmat_{i}' \Zmat_{i} ) \E( \Zmat_{i}' \Zmat_{i} )^{-1}
\E( \Zmat_{i}' \uvec_{i} \uvec_{i}' \Zmat_{i} ) 
\E( \Zmat_{i}' \Zmat_{i} )^{-1} \E( \Zmat_{i}' \Xmat_{i} ) 
\right]
\\ &
\left[ \E( \Xmat_{i}' \Zmat_{i} ) \E( \Zmat_{i}' \Zmat_{i} )^{-1} \E( \Zmat_{i}' \Xmat_{i} ) \right]^{-1}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{hypo}[\textbf{SIV.3}]
\begin{align*}
	\E( \Zmat_{i}' \uvec_{i} \uvec_{i}' \Zmat_{i} )  = \sigmasq_{u} \E(\Zmat_{i}' \Zmat_{i})
\end{align*}
\end{hypo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Nesse caso,
\begin{align*}
\Avar(\betavechat^{SIV}) &=
\sigmasq_{u}
\left[ \E( \Xmat_{i}' \Zmat_{i} ) \E( \Zmat_{i}' \Zmat_{i} )^{-1} \E( \Zmat_{i}' \Xmat_{i} ) \right]^{-1}
\\
\Var(\betavechat^{SIV}) &= 
\sigmasqhat_{u} \left[ (\Xmat' \Zmat) (\Zmat' \Zmat)^{-1} (\Zmat' \Xmat) \right]^{-1}
= \sigmasqhat_{u} \left( \Xmat' \PZ \Xmat \right)^{-1}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Matriz Robusta}

\begin{align*}
\Var(\betavechat^{SIV}) &= 
\left( \Xmat' \PZ \Xmat \right)^{-1}
\left[ \Xmat'\Zmat 
	\left( \sum_{i=1}^{N} \Zmat_{i}' \uvechat_{i} \uvechat_{i}' \Zmat_{i} \right)	
\Zmat'\Xmat \right]
\left( \Xmat' \PZ \Xmat \right)^{-1}
\end{align*}

\noindent
onde
\begin{align*}
	\uvechat_{i} = \yvec - \yvechat^{SIV} = \yvec - \Xmat_{i} \betavechat^{SIV}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{GMM}
\noindent
\citet[Sec 8.3, p.188]{wool-2010} \\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
A condição de ortogonalidade em \textbf{SIV.1} sugere uma estratégia de estimação.
Sob hipóteses \textbf{SIV.1} e \textbf{SIV.2}, $\betavec$ é o \emph{único} vetor $K \times 1$ que resolve o conjunto de condições de momentos populacionais
\begin{align*}
	\E\left[ \Zmat_{i}' \left( \yvec_{i} - \Xmat_{i} \betavec \right) \right] = \zerovec
\end{align*}

Assim, temos que $\betavec$ é identificado.
Uma vez que médias amostrais são estimadores consistentes para momentos populacionais, podemos aplicar o princípio da analogia e um estimador $\betavechat$ de $\betavec$ pode ser achado quando resolvemos a seguinte equação:
\begin{align}\label{gmm:cond}
N^{-1} \sum_{i=1}^{N} \Zmat_{i}' \left( \yvec_{i} - \Xmat_{i} \betavechat \right) = \zerovec.
\end{align}

\noindent
A equação acima é um sistema de $L$ equações com $K$ icógnitas em $\betavechat$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Primeiro, resolvemos o caso onde $L = K$ (apenas substituímos a variável endógena por um instrumento).
Então, se a matriz $K \times K$, $\sum_{i=1}^{N}\Zmat_{i}' \Xmat$, é não singular podemos resolver para $\betavechat$ e achamos:
\begin{align*}
\betavechat &=
\left( N^{-1} \sum_{i=1}^{N} \Zmat_{i}^{\prime} \Xmat_{i} \right)^{-1}
\left( N^{-1} \sum_{i=1}^{N} \Zmat_{i}^{\prime} \yvec_{i} \right)
\implies
\boxed{\betavechat = ( \Zmat' \Xmat )^{-1} ( \Zmat^{\prime} \yvec ) }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1 em}
\noindent
No entanto, quando $L > K$, temos mais colunas na matriz IV, $\Zmat$, do que precisamos para identificação.
Desse modo, a escolha de $\betavechat$ é mais complicada.
Exceto em casos especiais, a equação \eqref{gmm:cond} não tem solução.
Então, temos que escolher $\betavechat$ de modo a minimizar \eqref{gmm:cond}.
Para tanto, uma ideia é minimizar o quadrado da norma Euclidiana do vetor $L \times 1$ na equação \eqref{gmm:cond}.
Assim, tirando $N^{-1}$, temos:
\begin{align*}
\underset{\betavec}{\text{Min}} \;
\norm{\sum_{i=1}^{N} \Zmat_{i}' \left( \yvec_{i} - \Xmat_{i} \betavechat \right)}^2
\implies 
\underset{\betavec}{\text{Min}} \;
\left[ \sum_{i=1}^{N} \Zmat_{i}' \left( \yvec_{i} - \Xmat_{i} \betavechat \right) \right]'
\left[ \sum_{i=1}^{N} \Zmat_{i}' \left( \yvec_{i} - \Xmat_{i} \betavechat \right) \right]
\end{align*}

\noindent
onde:
\begin{align*}
\left[ \sum_{i=1}^{N} \Zmat_{i}' \left( \yvec_{i} - \Xmat_{i} \betavechat \right) \right]'
\left[ \sum_{i=1}^{N} \Zmat_{i}' \left( \yvec_{i} - \Xmat_{i} \betavechat \right) \right]
&=
( \Zmat' \yvec - \Zmat' \Xmat \betavec )' ( \Zmat' \yvec - \Zmat' \Xmat \betavec )
\\ &=
\yvec' \Zmat\Zmat' \yvec -
\yvec' \Zmat\Zmat' \Xmat \betavec -
\betavec' \Xmat' \Zmat\Zmat' \yvec +
\betavec' \Xmat' \Zmat\Zmat' \Xmat \betavec
\end{align*}

Derivando em relação em $\betavec$ e igualando a zero:
\begin{align*}
-2 \yvec' \Zmat\Zmat' \Xmat + 2 \betavec'\Xmat' \Zmat\Zmat' \Xmat &= 0
\implies
\betavec'\Xmat' \Zmat\Zmat' \Xmat = \yvec' \Zmat\Zmat' \Xmat 
\\
\betavec' &= ( \yvec' \Zmat\Zmat' \Xmat ) ( \Xmat' \Zmat\Zmat' \Xmat )^{-1}
\implies
\boxed{\betavec = ( \Xmat' \Zmat\Zmat' \Xmat )^{-1} ( \Xmat' \Zmat\Zmat' \yvec ) }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Estimador mais Eficiente}

Uma classe mais geral de estimadores é obtido usando uma \textbf{matriz peso} na forma quadrática.
Seja $\Wmathat$ uma matriz $L \times L$ simétrica e positiv semidefinida, onde incluímos o chapéu para enfatizarmos que $\Wmathat$ é, geralmente, um estimador.
Um \textbf{estimador GMM} (Método Generalizado dos Momentos) de $\betavec$ é um vetor $\betavechat$ que resolve o problema:
\begin{align*}
\underset{\betavechat}{\text{Min}} \; &
\left[\sum_{i=1}^{N} \Zmat_{i}' \left( \yvec_{i} - \Xmat_{i} \betavechat \right) \right]'
\Wmathat
\left[\sum_{i=1}^{N} \Zmat_{i}' \left( \yvec_{i} - \Xmat_{i} \betavechat \right) \right]
\end{align*}

\noindent
Usando notação vetorial temos:
\begin{align*}
\underset{\betavechat}{\text{Min}} \;
\left[
\yvec' \Zmat \Wmathat \Zmat' \yvec -
\yvec' \Zmat \Wmathat \Zmat' \Xmat \betavechat -
\betavechat' \Xmat'  \Zmat \Wmathat \Zmat' \yvec +
\betavechat' \Xmat'  \Zmat \Wmathat \Zmat' \Xmat \betavechat
\right]
\end{align*}

Derivando em relação em $\betavechat$ e igualando a zero:

\vspace{-1 em}
\begin{align*}
-2 \yvec' \Zmat \Wmathat \Zmat' \Xmat + 2 \betavechat'\Xmat' \Zmat \Wmathat \Zmat' \Xmat &= 0
\implies
\betavechat'\Xmat' \Zmat \Wmathat \Zmat' \Xmat = \yvec' \Zmat \Wmathat \Zmat' \Xmat 
\\
\betavechat' &= ( \yvec' \Zmat \Wmathat \Zmat' \Xmat ) ( \Xmat' \Zmat \Wmathat \Zmat' \Xmat )^{-1}
\\
\Aboxed{
\betavechat^{GMM} &= ( \Xmat' \Zmat \Wmathat' \Zmat' \Xmat )^{-1} ( \Xmat' \Zmat \Wmathat' \Zmat' \yvec ) }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Valor Esperado} 

\vspace{-1 em}
\begin{align*}
\Aboxed{
\E( \betavechat^{GMM} ) &=
\betavec +
\E[ ( \Xmat' \Zmat \Wmathat' \Zmat' \Xmat )^{-1} ( \Xmat' \Zmat \Wmathat' \Zmat' \uvec ) ] }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Matriz de Covariância Assintótica} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{hypo}[\textbf{SIV.4}]
\begin{align*}
	\Wmathat \arrowp \Wmat.
\end{align*}
\end{hypo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-2 em}
\begin{align*}
\Avar( \betavec^{GMM} ) &=
\left[ \E( \Xmat_{i}'\Zmat_{i}) \Wmat \, \E( \Zmat_{i}'\Xmat_{i}) \right]^{-1}
\left[ \E( \Xmat_{i}'\Zmat_{i}) \Lambdamat \Wmat \Lambdamat \, \E( \Zmat_{i}'\Xmat_{i}) \right]
\left[ \E( \Xmat_{i}'\Zmat_{i}) \Wmat \, \E( \Zmat_{i}'\Xmat_{i}) \right]^{-1}
\end{align*}

\noindent
onde $\Lambdamat = \E(\Zmat_{i}'\uvec_{i} \uvec_{i}' \Zmat_{i})$ com $\Lambdamat = \Wmatinv$:
\begin{align*}
\Avar( \betavec^{GMM} ) &=
\left[ \E( \Xmat_{i}'\Zmat_{i})	\Lambdamat \, \E( \Zmat_{i}'\Xmat_{i}) \right]^{-1}
\end{align*}

O estimador com $\Wmat = \Lambdamat^{-1}$ é eficiente na classe dos GMM.
Também é chamado de minimum chi-square estimator.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{obs}
	Se $\Wmat = \left( \Zmat'\Zmat \right)^{-1}$ temos S2LS.	
\end{obs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{GMM - 3SLS} 

Podemos supor alguma estrutra para $\E( \Zmat_{i} \uvec_{i} \uvec_{i}' \Zmat_{i})$, por exemplo:
\begin{align*}
	\E(\uvec_{i} \uvec_{i} | \Zmaat_{i}) = \Omegamat
	\quad \text{ ou } \quad
	\E(\uvec_{i} \uvec_{i} | \Zmaat_{i}) = \sigmasq_{u} \Imat_{G}.
\end{align*}

Nesse caso:
\begin{align*}
	\Omegamathat &= N^{-1} \sum_{i=1}^{N} \uvechat_{i} \uvechat_{i}'
	\\
	\Wmathat &= \left[ \Zmat \left( \Imat_{N} \otimes \Omegamathat \right) \Zmat \right]^{-1}.
\end{align*}

Supondo correta uma estrutura particular para $\Omegamat$, o GMM-3SLS é mais eficiente que oo GMM (min. chi-square).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelo de Painel Dinâmico} 

\vspace{-2 em}
\begin{align*}
y_{it} &= \rho y_{i,t-1} + \xvec_{it} \betavec + c_{i} + u_{it}; 
\quad t=2, \dots, T
\\
y_{it-1} &= \rho y_{i,t-2} + \xvec_{i,t-1} \betavec + c_{i} + u_{i,t-1}; 
\end{align*}

\noindent
onde $c_{i} + u_{it} = v_{it}$ e temos $\Cov(y_{i,t-1}, v_{it}) \neq 0$.

\vspace{-1 em}
\begin{align*}
\Delta y_{it} &= \rho \Delta y_{i,t-1} + \Delta \xvec_{it} \betavec + \Delta u_{it}; 
\quad t=3, \dots, T
\end{align*}

\noindent
definindo
\begin{align*}
\Delta y_{i, t-1} &= y_{i, t-1} - y_{i, t-2}
\\
\Delta u_{i, t} &= u_{i, t} - u_{i, t-1}
\end{align*}

\noindent
temos que
\begin{align*}
\Cov(y_{i,t-1}, u_{i,t-1}) & \neq 0 \implies 
\Cov(\Delta y_{i, t-1}, \Delta u_{i,t}) \neq 0 
\end{align*}

Instrumentos para $\Delta y_{i,t-1}$ são $y_{i,t-2}, y_{i,t-3}, \dots, y_{i1}$.
Podemos usar todos instrumentos disponíveis ou uma subconjunto deles 
(regra de bolso: 4 instrumentos).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{S2SLS}

\vspace{-2 em}
\begin{align*}
y_{it} = \xvec_{it}\betavec + u_{it}, 
\qquad i = 1, \dots, N, \quad t = 1, \dots, T
\end{align*}

\begin{description}[noitemsep]
	\item [$y_{it}$] escalar;
	\item [$\xvec_{it}$]  vetor $1 \times K$;
	\item [$\betavec$] vetor $K \times 1$;
	\item [$u_{it}$] escalar.
\end{description}

\begin{description}[noitemsep]
	\item [$\xvec_{it}$]  possui pelo menos um regressor endógeno
	\item [$\zvec_{it}$]  é um vetor de instrumentos.
\end{description}

\vspace{-2 em}
\begin{align*}
\boxed{
\betavechat^{S2SLS} =  ( \Xmat' \PZ \Xmat )^{-1} ( \Xmat' \PZ \yvec ) }
\end{align*}

\noindent
com
\begin{align*}
\boxed{\PZ = \Zmat'(\Zmat'\Zmat)^{-1}\Zmat }
\end{align*}

\noindent
onde
$\PZ$ é a matriz de projeção em $Z$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{FEIV}

\vspace{-2 em}
\begin{align*}
y_{it} &= \xvec_{it}\betavec + c_{i} + u_{it}, 
\qquad i = 1, \dots, N, \quad t = 1, \dots, T
\\
\ddot{y}_{it} &= \ddot{\xvec}_{it}\betavec + \ddtos{u}_{it}, 
\qquad i = 1, \dots, N, \quad t = 1, \dots, T
\end{align*}

\noindent
Suponha que $\ddot{\xvec}_{it}$ possui pelo menos um regressor endógeno.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{hypo}[\textbf{FEIV.1}]
\begin{align*}
	\E(u_{it} | \zvec_{i1}, \dots, \zvec_{iT}, c_{i}) = 0	
\qquad i = 1, \dots, N, \quad t = 1, \dots, T
\end{align*}
\end{hypo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{hypo}[\textbf{FEIV.2}]
\item
\begin{enumerate}[label= \alph*)]
\item  $\posto \sum_{t=1}^{T} \E( \ddot{\zvec}_{it}\ddot{\zvec}_{it}' ) = L$
\item  $\posto \sum_{t=1}^{T} \E( \ddot{\xvec}_{it}\ddot{\zvec}_{it}' ) = K$
\end{enumerate}
\end{hypo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{hypo}[\textbf{FEIV.3}]
\begin{align*}
\E(\uvec_{i} \uvec_{i}' | \zvec_{i1}, \dots, \zvec_{iT}, c_{i}) = \sigmasq_{u} \Imat_{T}
\end{align*}
\end{hypo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{obs}
	Podemos usar $\zvec$ ou $\ddot{\zvec}$.
	O importante é usar instrumentos que variam no tempo.
\end{obs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Então, 
\begin{align*}
\Aboxed{ \betavec^{FEIV} &= 
\left( \ddot{\Xmat}' \PZdot \ddot{\Xmat} \right)^{-1}
\left( \ddot \Xmat' \PZdot \ddot \yvec\right)
}
\\
\qquad
\Aboxed{
\PZdot &= \ddot \Zmat ( \ddot \Zmat' \ddot \Zmat)^{-1} \ddot \Zmat'}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{obs}
\begin{align*}
	\ddot \Xmat' \ddot \Zmat = \sum_{i=1}^{N} \ddot \Xmat_{i}' \ddot \Zmat_{i}
\end{align*}

\begin{description}[noitemsep]
	\item[$\ddot \Xmat'$] é uma matriz $K \times NT$ 
	\item[$\ddot \Xmat_{i}'$] é uma matriz $K \times T$ 
	\item[$\ddot \Zmat$] é uma matriz $NT \times L$ 
	\item[$\ddot \Zmat_{i}$] é uma matriz $T \times L$ 
\end{description}

\end{obs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Variância de FEIV}

\paragraph{Robusta}
\begin{align*}
\Avar(\betavec^{FEIV}) = & 
\left[ 
\E\left( \ddot{\Xmat}_{i}' \ddot{\Zmat}_{i} \right)
\E\left( \ddot{\Zmat}_{i}' \ddot{\Zmat}_{i} \right)^{-1}
\E\left( \ddot{\Zmat}_{i}' \ddot{\Xmat}_{i} \right)
\right]^{-1}
\\ &
\left[ 
\E\left( \ddot{\Xmat}_{i}' \ddot{\Zmat}_{i} \right)
\E\left( \ddot{\Zmat}_{i}' \ddot{\Zmat}_{i} \right)^{-1}
\E\left( \ddot{\Zmat}_{i}' \uvec_{i} \uvec_{i} \ddot{\Zmat}_{i} \right)
\E\left( \ddot{\Zmat}_{i}' \ddot{\Zmat}_{i} \right)^{-1}
\E\left( \ddot{\Zmat}_{i}' \ddot{\Xmat}_{i} \right)
\right]
\\ &
\left[ 
\E\left( \ddot{\Xmat}_{i}' \ddot{\Zmat}_{i} \right)
\E\left( \ddot{\Zmat}_{i}' \ddot{\Zmat}_{i} \right)^{-1}
\E\left( \ddot{\Zmat}_{i}' \ddot{\Xmat}_{i} \right)
\right]^{-1}
\end{align*}

\paragraph{Homocedástica}
\begin{align*}
\Avar(\betavec^{FEIV}) & = 
\sigmasq_{u}
\left[ 
\E\left( \ddot{\Xmat}_{i}' \ddot{\Zmat}_{i} \right)
\E\left( \ddot{\Zmat}_{i}' \ddot{\Zmat}_{i} \right)^{-1}
\E\left( \ddot{\Zmat}_{i}' \ddot{\Xmat}_{i} \right)
\right]^{-1}
\end{align*}

\noindent
Estimando $\sigmasq_{u}$:
\begin{align*}
	\sigmasq_{u} = \left[ N(T-1) -K \right]^{-1} \sum_{i=1}^{N} \sum_{t=1}^{T} \widehat{\ddot{u}}^{2}_{it}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{obs}
	FEIV é um estimador S2SLS com $\ddot \Xmat$, $\ddot \Zmat$ e $\ddot \yvec$, mas com $gl= N(T-1) - K$.	
\end{obs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{FDIV}

\vspace{-2 em}
\begin{align*}
y_{it} &= \xvec_{it}' \betavec + c_{i} + u_{it}, 
\qquad i = 1, \dots, N; \quad t = 1, \dots, T
\\
\Delta y_{it} &= \Delta \xvec_{it}' \betavec + \Delta u_{it},
\qquad i = 1, \dots, N; \quad t = 2, \dots, T
\end{align*}

Vamos supor $\Delta \xvec_{it}'$ tem variável endógena.
$\wvec_{it}$ é um vetor $1 \times L_{t}$ de instrumentos, onde $L_{t} \geq K$.
Se os instrumentos forem diferentes:
\begin{align*}
\Wmat_{i} = \diag( \wvec_{i2}', \wvec_{i3}', \dots, \wvec_{iT}')
\end{align*}

\noindent
onde $\Wmat_{i}$ é uma matriz $( T - 1 ) \times L$

\vspace{-1 em}
\begin{align*}
L = L_{2} + L_{3} + \dots + L_{T}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{hypo}[\textbf{FDIV.1}]
\begin{align*}
\E( \mbs{w}_{it} \Delta u_{it}') = \zerovec, \qquad
i = 1, \dots, N; \quad t = 2, \dots, T.
\end{align*}
\end{hypo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{hypo}[\textbf{FDIV.2}]
\begin{align*}
&\posto\left[ \E( \Wmat_{i}' \Wmat_{i} ) \right] = L
\\
&\posto\left[ \E( \Wmat_{i}' \Delta \Xmat_{i} ) \right] = K
\end{align*}
\end{hypo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-1 em}
\begin{align*}
\Aboxed{ \betavec^{FDIV} &=
\left( \Delta \Xmat' \PW \Delta \Xmat  \right)^{-1}
\left( \Delta \Xmat' \PW \Delta \yvec \right) }
\\
\Aboxed{\PW &= \Wmat (\Wmat' \Wmat)^{-1} \Wmat'}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1 em}
\begin{align*}
\E( \betavec^{FDIV} ) =  
\betavec + 
\left( \Delta \Xmat' \PW \Delta \Xmat \right)^{-1}
\left( \Delta \Xmat' \PW \mbs{e} \right)
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Variância Robusta}

\begin{center}
\red{REVER}
\end{center}

\vspace{-1 em}
\begin{align*}
\Var( \betavec^{FDIV} ) &=
\E\left\{  
\left[ \E( \betavec^{FDIV} ) - \betavec \right] 
\left[ \E( \betavec^{FDIV} ) - \betavec \right]'
\right\}
\\
&=
\E\left\{  
\left[ \Delta \Xmat' \PW \Delta \Xmat \right]^{-1}
\left[ \Delta \Xmat' \PW \ebold \right]
\left[ \Delta \Xmat' \PW \ebold \right]'
\left[ \Delta \Xmat' \PW \Delta \Xmat \right]^{-1}
\right\}
\\
&=
\E\left[
\left( \Delta \Xmat' \PW \Delta \Xmat \right)^{-1}
\left( \Delta \Xmat' \PW \ebold \ebold' \PW \Delta \Xmat \right)
\left( \Delta \Xmat' \PW \Delta \Xmat \right)^{-1}
\right]
\end{align*}

\noindent
onde
$e_{i} = \Delta u_{it}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Variância Homoceddástica}
\begin{align*}
\E(\Wmat_{i}' \ebold_{i} \ebold_{i}' \Wmat_{i}) &=
\sigmasq_{e} \E(\Wmat_{i}' \Wmat_{i}).
\\
\Avar{\betavechat^{FDIV}} &=
\sigmasq_{e} \left[ \E\left( \Delta \Xmat' \PW \Delta \Xmat \right) \right]^{-1}
\\ &=
\sigmasq_{e}
\left[
\E(\Delta\Xmat_{i}'\Wmat_{i})
\E(\Wmat_{i}'\Wmat_{i})^{-1}
\E(\Wmat_{i}'\Delta\Xmat_{i})
\right]^{-1}
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{obs}
Podemos fazer GMM usando matriz ótima:
\begin{align*}
	& \E(\Wmat_{i}' \eboldhat_{i} \eboldhat_{i} \Wmat_{i})^{-1}
	\qquad \text{GMM 2SLS}
	\\ & \text{ou} \\
	& \E(\Wmat_{i}' \Omegamathat \Wmat_{i})^{-1}
	\qquad \text{GMM 3SLS}
\end{align*}

\end{obs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{REIV}

Estimador GIV (Generalized IV):
\begin{align*}
\yvec_{i} = \Xmat_{i} \betavec + \vvec_{i}, \qquad i=1, \dots, N.
\end{align*}

\noindent onde
\begin{align*}
\E(\vvec_{i} \vvec_{i}') = \Omegamat
\end{align*}

\noindent
temos o seguinte modelo transformado
\begin{align*}
\Omegamat^{-1/2} \yvec_{i} = \Omegamat^{-1/2}\Xmat_{i} \betavec + \Omegamat^{-1/2}\vvec_{i}
\end{align*}

\noindent ainda
\begin{align*}
\E(\Omegamat^{-1/2}\vvec_{i} \vvec_{i}'\Omegamat^{-1/2}) = 
\Omegamat^{-1/2}\E(\vvec_{i} \vvec_{i}')\Omegamat^{-1/2} =
\Omegamat^{-1/2}\Omegamat \Omegamat^{-1/2} = \Imat_{T}.
\end{align*}

Suponha endogeneidade e matriz de intrumentos $\Zmat_{i}$.
Vamos usar $\Omegamat^{-1/2} \Zmat_{i}$ como instrumento do modelo transformado.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsection{Alguns Testes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Máxima Verossimilhança}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Latent Variables, Probit and Logit}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelo}

Suponha $y^{*}$ não observável (\textbf{latente}) seguindo o seguinte modelo:

\vspace{-1 em}
\begin{align} \label{mod1:probit}
y_{i}^{*} = \xvec_{i}' \betavec + \err_{i}.
\end{align}

\noindent
Defina $y$ como:

\vspace{-1 em}
\begin{align*}
y_{i} =
\begin{cases}
1 \, , \quad y^{*}_{i} \geq 0
\\
0 \, , \quad y^{*}_{i} < 0
\end{cases}
\end{align*}

\noindent
temos que:

\vspace{-1 em}
\begin{align*}
P( y_{i} = 1 | \xvec ) &= p( \xvec )
\\
P( y_{i} = 0 | \xvec ) &= 1 - p( \xvec ).
\end{align*}

Além disso, pela definição de $y_{i}$, equação \eqref{mod1:probit}, temos:

\vspace{-1 em}
\begin{align*}
P( y_{i} = 1 | \xvec ) &= P(y_{i}^{*} \geq 0 \, | \xvec )
\\
&= P( \xvec_{i}' \betavec + \err_{i} \geq 0 \, | \xvec )
\\
&= P( \err_{i} \geq - \xvec_{i}' \betavec  \, | \xvec ).
\end{align*}

\noindent
Agora, supondo que $\err_{i}$ tem FDA, $G$, tal que $G'=g$ é simétrica ao redor de zero:

\vspace{-1 em}
\begin{align*}
P( y_{i} = 1 | \xvec ) 
&= 1 - P( \err_{i} < - \xvec_{i}' \betavec  \, | \xvec )
\\
&= 1 - G( - \xvec_{i}' \betavec  \, | \xvec )
\\
&= G( \xvec_{i}' \betavec ).
\end{align*}

Se $G(\cdot)$ for uma distribuição:

\begin{description}
\item [Normal Padrão:] $\hat{\betavec}$ é o estimador \textbf{probit}.
\item [Logística:] $\hat{\betavec}$ é o estimador \textbf{logit}.
\end{description}

Supondo $\yvec_{i} \, | \, \xvec \sim Bernoulli(p(\xvec))$, sua fmp é dada por:

\vspace{-1 em}
\begin{align*}
f( y_{i} \, | \, \xvec_{i} ; \betavec ) 
&= 
\left[ G(\xvec_{i}' \betavec )  \right]^{y_{i}}
\left[ 1 - G(\xvec_{i}' \betavec )  \right]^{1 - y_{i}}
\; , \quad y=0,1.
\end{align*}

Para estimarmos $\hat{\betavec}$ por máxima verossimilhança, temos de encontrar $\betavec \in B$, onde $B$ é o espaço paramétrico, tal que $\betavec$ maximize o valor da distribuição conjunta de $\yvec$, ou seja:

\vspace{-1 em}
\begin{align*}
\underset{\betavec \in B}{\text{Max }} 
\prod_{i=1}^{N}
f( y_{i} \, | \, \xvec_{i} ; \betavec ).
\end{align*}

\noindent

Tirando o logaritmo e dividindo tudo por $N$ (podemos fazer isso pois são transformações monotônicas e não alteram o lugar onde $\betavec$ ótimo irá parar):

\vspace{-1 em}
\begin{align*}
\underset{\betavec \in B}{\text{Max }} 
\left\{ 
N^{-1} \sum_{i=1}^{N}
\ln \left[ f( y_{i} \, | \, \xvec_{i} ; \betavec ) \right]
\right\}.
\end{align*}

\noindent
Podemos definir
$\ell_{i}( \betavec ) = \ln[ f( y_{i} \, | \, \xvec_{i} ; \betavec ) ]$
como sendo a verossimilhança condicional da observação $i$:

\vspace{-1 em}
\begin{align*}
\underset{\betavec \in B}{\text{Max }} 
\left\{ 
N^{-1} \sum_{i=1}^{N} \ell_{i} (\betavec)
\right\}.
\end{align*}

Dessa forma, podemos ver que o problema acima é a analogia amostral de:

\vspace{-1 em}
\begin{align*}
\underset{\betavec \in B}{\text{Max }} 
\E \left[ 
\ell_{i} ( \betavec )
\right].
\end{align*}

Definindo o \textit{vector score} da observação $i$:

\vspace{-1 em}
\begin{align*}
s_{i} (\betavec) = 
\left[ \nabla_{\betavec} \ell_{i} (\betavec) \right]'
=
\begin{bmatrix}
\dfrac{\partial{\ell_{i} (\betavec)}}{\partial{\beta_{1}}},
\dots,
\dfrac{\partial{\ell_{i} (\betavec)}}{\partial{\beta_{K}}}
\end{bmatrix}
\end{align*}

Definindo a \textbf{Matriz Hessiana} da observação $i$:

\vspace{-1 em}
\begin{align*}
H_{i} (\betavec) = 
\nabla_{\betavec} s_{i} (\betavec) = 
\nabla_{\betavec}^2 \ell_{i} (\betavec)
\end{align*}

Tendo essas definições, o \textbf{Teorema do Valor Médio} (TVM) nos diz que no intervalo $[a, b]$, existe um número, $c$, tal que:

\vspace{-1 em}
\begin{align*}
f'(c) = \frac{f(b) - f(a)}{b - a}.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1 em}
\begin{center}
\red{FAZER DESENHO}
\end{center}
\vspace{1 em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Trocando 
$f(\cdot)$ por $s_{i}(\cdot)$, 
$a$ por $\betavec_{0}$, 
$b$ por $\widehat{\betavec}$ e
$c$ por $\bar{\betavec}$,
temos:

\vspace{-1 em}
\begin{align*}
H_{i} ( \bar{\betavec} ) =
\frac{s_{i}( \widehat{\betavec} ) - s_{i}( \betavec_{0} )}{\widehat{\betavec} - \betavec_{0}},
\end{align*}

\noindent
tirando médias dos dois lados:

\vspace{-1 em}
\begin{align*}
N^{-1} \sum_{i=1}^{N} 
H_{i} ( \bar{\betavec} ) 
=
\frac{1}{\widehat{\betavec} - \betavec_{0}}
N^{-1} \sum_{i=1}^{N} 
\left[ 
s_{i}( \widehat{\betavec} ) - s_{i}( \betavec_{0} )
\right]
\end{align*}

Supondo que
$\widehat{\betavec}$
maximiza
$\ell (\betavec \, | \, \yvec, \xvec)$,
temos que:
$N^{-1} \sum_{i=1}^{N} s_{i}(\widehat{\betavec}) = 0$.
E podemos reescrever a equação anterior como:

\vspace{-1 em}
\begin{align*}
\widehat{\betavec} - \betavec_{0}
&=
(-1)
\left[ N^{-1} \sum_{i=1}^{N} H_{i} ( \bar{\betavec} ) \right]^{-1}
N^{-1} \sum_{i=1}^{N} s_{i}( \betavec_{0} ) 
\\
\sqrt{N} ( \widehat{\betavec} - \betavec_{0} )
&=
\left[
- N^{-1} \sum_{i=1}^{N} H_{i} ( \bar{\betavec} )
\right]^{-1}
\sqrt{N} \cdot N^{-1} \sum_{i=1}^{N} s_{i}( \betavec_{0} ) 
\\
\Aboxed{
\sqrt{N} ( \widehat{\betavec} - \betavec_{0} )
&=
\left[
- N^{-1} \sum_{i=1}^{N} H_{i} ( \bar{\betavec} )
\right]^{-1}
N^{-1/2} \sum_{i=1}^{N} s_{i}( \betavec_{0} ) }.
\end{align*}

\noindent
Onde

\vspace{-1 em}
\begin{align*}
\left[ 
- N^{-1} \sum_{i=1}^{N}
H_{i} ( \bar{\betavec} ) \right]^{-1}
\xrightarrow{p}
A_{0}^{-1} \, ,
&&
N^{-1/2} \sum_{i=1}^{N} s_{i}( \betavec_{0} ) 
\xrightarrow{d}
N( 0, B_{0} ).
\end{align*}

\noindent
Assim, temos que:

\vspace{-1 em}
\begin{align*}
\Aboxed{
\sqrt{N} ( \widehat{\betavec} - \betavec_{0} )
\to
N ( 0, A_{0}^{-1} B_{0} A_{0}^{-1} )}.
\end{align*}

A forma mais simples de achar $\Var ( {\widehat{\betavec}} )$ é:

\vspace{-1 em}
\begin{align*}
\Aboxed{
\Var( \widehat{\betavec} )
&=
- \E[ H_{i} ( \widehat{\betavec} ) ]^{-1} }.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{ATT, ATE, Propensity Score}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modelo}

\begin{itemize}\itemsep0pt
\item
$y_{1}$ $\rightarrow$ variável de interesse com tratamento

\item
$y_{0}$ $\rightarrow$ variável de interesse sem tratamento
\end{itemize}

\vspace{-1 em}
\begin{align*}
w = 
\begin{cases}
1 & \text{se tratam}
\\
0 & \text{se não tratam}
\end{cases}
\end{align*}

Idealmente, para isolarmos completamente o efeito de $w=1$, gostaríamos de pode calcular:

\vspace{-1 em}
\begin{align*}
N^{-1} \sum_{i=1}^{N}
\left( y_{i1} - y_{i0} \right).
\end{align*}

Ou seja, o efeito que o tratamento causa sobre um indivíduo com todo o resto permanecendo constante.
Em outras palavras, queríamos que houvesse dois mundos paralelos observáveis onde seria possível observar o que acontece com $y_{i}$ com e sem tratamento.
Infelizmente, para ccada indivíduo $i$, observamos apenas $y_{i1}$ ou $y_{i0}$, nunca ambos.

Antes de continuarmos, faremos as seguintes definições:

\begin{description}
\item[ATE:]  $\E( y_{1} - y_{0} )$
\item[ATT:]  $\E( y_{1} - y_{0} \, | \, w = 1 )$ (ATE no tratado).
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{ATE e ATT condicional a variáveis dependentes}

\vspace{-1 em}
\begin{align*}
ATE( \xvec ) &= \E( y_1 - y_0 \, | \xvec)
\\
ATT( \xvec ) &= \E( y_1 - y_0 \, | \xvec, w = 1)
\end{align*}

\noindent
\underline{OBS:}

\vspace{-1 em}
\begin{align*}
\E( y_1 - y_0 ) &= \E \left[ \E( y_1 - y_0 \, | w) \right]
\\
\E( y_1 - y_0 \, | w ) &=
\E( y_1 - y_0 \, | w = 0 ) \cdot P(w=0)
+
\E( y_1 - y_0 \, | w = 1 ) \cdot P(w=1).
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Métodos Assumindo Ignorabilidade do Tratamento}

\begin{description}
\item[ATE.1:] Ignorabilidade. 
\\
$w$ e $(y_{1}, y_{0})$ são independentes condicionais a $\xvec$.

\item[ATE.1':] Ignorabilidade da Média. 

\vspace{-.75 em}
\begin{enumerate}[label =\alph*)] \itemsep0pt
\item $\E( y_{0} \, | \, w, \xvec ) = \E( y_{0} \, | \, \xvec )$
\item $\E( y_{1} \, | \, w, \xvec ) = \E( y_{1} \, | \, \xvec )$
\end{enumerate}

\end{description}

Vamos definir

\vspace{-1 em}
\begin{align*}
	\E( y_{0} \, | \, \xvec ) &= \mu_{0}( \xvec )
	\\
	\E( y_{1} \, | \, \xvec ) &= \mu_{1}( \xvec ).
\end{align*}

Sob \textbf{ATE.1} e \textbf{ATE.1'}:

\vspace{-1 em}
\begin{align*}
	ATE( \xvec ) &= \E( y_{1} - y_{0} \, | \xvec ) = \mu_{1}(\xvec) - \mu_{0}(\xvec) 
	\\
	ATT( \xvec ) &= \E( y_{1} - y_{0} \, | \xvec, w=1 ) = \mu_{1}(\xvec) - \mu_{0}(\xvec) 
\end{align*}

\begin{description}
	\item[ATE.2:] \textit{Overlap} \\
		Para todo $\xvec$, $P(w=1 \, | \, \xvec ) \in ( 0, 1 )$, 
		$p(\xvec) = p(w=1 | \xvec)$.
\end{description}

$p(\xvec)$ é o \textit{Propensity Score}, ele representa a probabilidade de $y_{i}$ ser tratado dado o valor das covariáveis $\xvec$.
Essa hipótese é importante visto que podemos expressar o $ATE$ em função de $p(\xvec)$.

\vspace{1 em}
Para o $ATT$ vamos supor:

\begin{description}
	\item[ATT.1':] 
		$\E( y_{0} \, | \xvec, w ) = \E( y_{0} \, | \, \xvec )$

	\item[ATT.2:] \textit{Overlap:} Para todo $\xvec$, $P(w=1 | \xvec ) < 1$.
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Propensity Score}

Como foi dito anteriormente, apenas observamos ou $y_{1}$ ou $y_{0}$ para a mesma pessoa, mas não ambos.
Mais precisamente, junto com $w$, o resultado observado é:

\vspace{-1 em}
\begin{align*}
	y = wy_{1} + (1 - w) y_{0}
\end{align*}

\noindent
como  $w$ é binário, $w^2 = w$, assim, temos:

\vspace{-1 em}
\begin{align*}
	w y &= w^{2} y_{1} + (w - w^{2}) y_{0}
	\implies
	\boxed{w y = w y_{1} }
	\\
	( 1 - w ) y &= (w - w^{2}) y_{1} + ( w^{2} - 2w + 1 ) y_{0}
	\implies
	\boxed{( 1 - w ) y = (1 - w) y_{0}}.
\end{align*}

Fazemos isso para tentar isolar $\mu_{0}(\xvec)$ e $\mu_{1}(\xvec)$:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{$\mu_{1}( \xvec )$}

\begin{align*}
	\E( w y | \xvec ) &= \E\left[  \E \left( w y_{1} | \xvec, w  \right) | \xvec \right]
	\\ &=
	\E \left[ w \mu_{1}(\xvec) | \xvec \right]
	\\ &=
	\mu_{1}(\xvec) \E(w | \xvec ).
\end{align*}

\noindent
Como $w$ é binaria: $\E(w| \xvec) = P(w=1 | \xvec) = p(\xvec)$.
Assim:

\vspace{-1 em}
\begin{align*}
	\E( w y | \xvec ) &= \mu_{1}(\xvec) p(\xvec)
	\\
	\Aboxed{ \mu_{1}(\xvec) &= \frac{\E(w y | \xvec)}{p(\xvec)} }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{$\mu_{0}( \xvec )$}

\vspace{-1 em}
\begin{align*}
	\E[ (1-w) y | \xvec ] &= \E\left[  \E \left( (1 - w) y_{0} | \xvec, w  \right) | \xvec \right]
	\\ &=
	\E \left[ (1 - w) \mu_{0}(\xvec) | \xvec \right]
	\\ &=
	\mu_{0}(\xvec) \E(w | \xvec )
	\\ 
	\E[ (1-w) y | \xvec ] 
	&=
	\mu_{0}(\xvec) [1 - p(\xvec)] \implies
	\\ 
	\Aboxed{
		\mu_{0}(\xvec)
		&=
	\frac{\E[ (1-w) y | \xvec ] }{1 - p( \xvec ) } }
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{ATE:}

\begin{align*}
	\mu_{1}(\xvec) - \mu_{0}(\xvec) =
	\E\left[ 
		\frac{[w - p(\xvec)] y}{p(\xvec) [1 - p(\xvec)]}
		| \xvec
	\right]
\end{align*}

\begin{align*}
	\Aboxed{
		\widehat{ATE} =
		N^{-1} \sum_{i=1}^{N}
		\frac{[ w_{i} - p(\xvec_{i} ) ] y_{i} }{ p( \xvec_{i} ) [1 - p( \xvec_{i} ) ] }
	}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{ATT:}

\begin{align*}
	\E( y_{1} | \xvec, w=1) - \E(y_{0} | \xvec) =
	\frac{1}{\hat{P}(w=1)}
	\E\left[ 
		\frac{[w - \hat{p}(\xvec)] y}{[ 1 - \hat{p}(\xvec) ]}
		| \xvec
	\right]
\end{align*}

\vspace{-1 em}
\begin{align*}
	\hat{P} (w = 1) = N^{-1} \sum_{i=1}^{N} w_{i}
\end{align*}

\vspace{-1.5 em}
\begin{align*}
	\widehat{ATT} &=
	\frac{N}{\sum_{i=1}^{N} w_{i} }
	N^{-1} \sum_{i=1}^{N}
	\frac{[ w_{i} - \hat{p}(\xvec_{i} ) ] y_{i} }{[ 1 - \hat{p}( \xvec_{i} ) ]}
	\\
	\Aboxed{
		\widehat{ATT} &=
		\frac{1}{\sum_{i=1}^{N} w_{i} }
		\sum_{i=1}^{N}
		\frac{[ w_{i} - \hat{p}(\xvec_{i} ) ] y_{i} }{[1 - \hat{p}( \xvec_{i} ) ]}
	}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Regressão Descontínua}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Álgebra Linear}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Vetores}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Operações com Vetores}

Vamos definir os vetores $\xvec$ e $\yvec$ com dimensão $1 \times N$:

\begin{align*}
	\xvec = 
	\begin{bmatrix}
		x_{1} \\ \vdots \\ x_{N}	
	\end{bmatrix}
	\quad
	\yvec = 
	\begin{bmatrix}
		y_{1} \\ \vdots \\ y_{N}	
	\end{bmatrix}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Multiplicação por escalar}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Soma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Subtração}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Multiplicação de vetores}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Poduto Interno (Produto Escalar, Dot Product, Inner Product)}

\begin{align}\label{innerp}
	\xvec' \yvec = \xvec \cdot \yvec = \innerp{\xvec}{\yvec} = \sum_{i=1}^{N} x_{i} y_{i} = x_{1}y_{1} + \dots + x_{N}y_{N}.
\end{align}

Podemos utilizar a equação \eqref{innerp} para denotarmos a soma dos elementos de um vetor.
Para tanto, definimos o vetor $\onevec_{N}$ como sendo o vetor cujos elementos são 1 e tem dimensão $N \times 1$.
\cite[p. 977, A.2.7]{greene-7ed}

% SOMA
\begin{align*}
	x_{1} + \dots + x_{N} =
	\sum_{i=1}^{N} x_{i} =
	\xvec' \onevec_{N} = 
	\onevec_{N}' \xvec = 
	(\xvec' \onevec_{N})'
\end{align*}

Com a definição do vetor $\onevec_{N}$, também podemos escrever:
$\onevec_{N}' \onevec_{N} = N$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Média Aritmética
Usando a definição de \textbf{média aritmética}, também podemos representá-la da seguinte forma:

\begin{align*}
	\frac{x_{1} + \dots + x_{N}}{N} =
	\overline{x} =
	N^{-1} \sum_{i=1}^{N} x_{i} =
	N^{-1} \xvec' \onevec_{N}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Média Ponderada
Usando a definição de \textbf{média ponderada}, também podemos representá-la da seguinte forma:

\begin{align*}
	w_{1}x_{1} + \dots + w_{N}x_{N} =
	\sum_{i=1}^{N} w_{i} x_{i} =
	\wvec'\xvec
\end{align*}
onde $\wvec'\onevec=1$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Poduto Externo (Outer Product)}

\begin{align*}
	\onevec_{N} \onevec_{N}' =
	\begin{bmatrix}
		1 & \dots & 1	 \\
		\vdots & \ddots & \vdots \\
		1 & \dots & 1	
	\end{bmatrix}_{N \times N}
	\quad
	\onevec_{N} \xvec' =
	\begin{bmatrix}
		x_{1} & \dots & x_{N} \\
		\vdots & \ddots & \vdots \\
		x_{1} & \dots & x_{N}	
	\end{bmatrix}_{N \times N}
	\qquad
	\xvec \onevec_{N}' =
	\begin{bmatrix}
		x_{1} & \dots & x_{1} \\
		\vdots & \ddots & \vdots \\
		x_{N} & \dots & x_{N}	
	\end{bmatrix}_{N \times N}
\end{align*}

\begin{align}\label{eq:xx:outer}
	\xvec \xvec' =
	\begin{bmatrix}
		x_{1} \\ \vdots \\ x_{N}
	\end{bmatrix}
	\begin{bmatrix}
		x_{1} & \dots & x_{N}
	\end{bmatrix}
	=
	\begin{bmatrix}
		x_{1}^{2}  & x_{1} x_{2} & \dots  & x_{1}x_{N} \\
		x_{2}x_{1} & x_{2}^2     & \dots  & x_{2}x_{N} \\
		\vdots     & \vdots      & \ddots & \vdots \\
		x_{N}x_{1} & x_{N} x_{2} & \dots  & x_{N}^{2}
	\end{bmatrix}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Distância e Ângulo}

O coseno do ângulo, $\alpha$, entre dois vetores é:
\begin{align*}
	\cos(\alpha) = \frac{\uvec'\vvec}{\|\uvec \|\cdot\|\vvec \|}
\end{align*}

Dois vetores são \textbf{ortogonais} se $\uvec'\vvec = 0$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Projeções Ortogonal}

\vspace{-1 em}
\begin{align*}
	\proj_{\uvec}(\yvec) =
	\yvechat =
	\frac{\yvec' \uvec}{\uvec'\uvec} \uvec
\end{align*}
é a \textbf{projeção ortogonal} de $\yvec$ em $\uvec$.

\vspace{-1 em}
\begin{align*}
	\zvec = \yvec - \yvechat = \yvec - \frac{\yvec' \uvec}{\uvec'\uvec} \uvec
\end{align*}
é a \textbf{componente de $\yvec$ ortogonal a $\uvec$} \textbf{(Rejeição)}.

Por construção, temos:
\vspace{-1 em}
\begin{align*}
	\zvec + \yvechat = \yvec.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\red{[FIGURA AQUI]}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Outras Notações Projeções Ortogonal}
Dado que

\vspace{-1 em}
\begin{align*}
	\proj_{\uvec}(\yvec) =
	\frac{\yvec' \uvec}{\uvec'\uvec} \uvec
\end{align*}
é a \textbf{projeção ortogonal} de $\yvec$ em $\uvec$.
Usando $\yvec'\uvec = \uvec'\yvec$, temos

\vspace{-1 em}
\begin{align*}
	\proj_{\uvec}(\yvec) =
	\frac{\uvec \uvec'}{\uvec'\uvec} \yvec
\end{align*}

\noindent
Tirando o $\yvec$ da equação, obtemos o \red{operador de projeção} \red{(Matriz de projeção em $\uvec$?)}:

\vspace{-1 em}
\begin{align*}
	\frac{\uvec \uvec'}{\uvec'\uvec} 
	=
	\uvec(\uvec'\uvec)^{-1} \uvec'
	=
	(\uvec'\uvec)^{-1} \uvec \uvec'
	=
	(\|\uvec \|^2)^{-1} \uvec \uvec'
\end{align*}

Agora, podemos definir o \red{operador rejeição} como:

\vspace{-1 em}
\begin{align*}
	\Imat_{N} - \frac{\uvec \uvec'}{\uvec'\uvec} 
\end{align*}

Usando o caso especial onde $\uvec$ é um vetor de uns ($\uvec = \onevec_{N}$), temos a \textbf{projeção} no eixo de 45 graus:

\vspace{-1 em}
\begin{align*}
	( \onevec_{N}' \onevec_{N} )^{-1} \onevec_{N} \onevec_{N}'
	=
	N^{-1} 
	\begin{bmatrix}
		1      & \dots  & 1	 \\
		\vdots & \ddots & \vdots \\
		1      & \dots  & 1	
	\end{bmatrix}_{N \times N}
	=
	\begin{bmatrix}
		1/N    & \dots  & 1/N	 \\
		\vdots & \ddots & \vdots \\
		1/N    & \dots  & 1/N	
	\end{bmatrix}_{N \times N}
\end{align*}

A \textbf{rejeição} no eixo de 45 graus:

\vspace{-1 em}
\begin{align*}
	\Imat_{N} - \frac{\onevec_{N} \onevec_{N}'}{\onevec_{N}' \onevec_{N}} 
	=
	\begin{bmatrix}
		1      & 0 &\dots  & 0	 \\
		0      & 1 &\dots  & 0	 \\
		\vdots & \vdots & \ddots & \vdots\\
		0      & 0 & \dots  & 1	
	\end{bmatrix}
	-
	\begin{bmatrix}
		1/N    & 1/N    & \dots  & 1/N	 \\
		1/N    & 1/N    & \dots  & 1/N	 \\
		\vdots & \vdots & \ddots & \vdots \\
		1/N    & 1/N    & \dots  & 1/N	
	\end{bmatrix}
	=
	\begin{bmatrix}
		(N-1)/N & 1/N     & \dots  & 1/N    \\
		1/N     & (N-1)/N & \dots  & 1/N    \\
		\vdots  & \vdots  & \ddots & \vdots \\
		1/N     & 1/N     & \dots  & (N-1)/N	
	\end{bmatrix}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Centering Matrix} \cite[p. 978, A.28]{greene-7ed}

\vspace{-1 em}
\begin{align*}
	\Mzero &= 
	\Imat_{N} - \onevec_{N} ( \onevec_{N}' \onevec_{N} )^{-1} \onevec_{N}'
	= 
	\Imat_{N} - N^{-1} \onevec_{N} \onevec_{N}' 
\end{align*}

A Matriz $\Mzero$ é \textbf{idempotente} e \textbf{simétrica}.

\begin{description}[noitemsep]
	\item [Idempotência:] $\Amat \Amat = \Amat$
	\item [Simetria:] $\Amat'=\Amat$
\end{description}

\vspace{-2 em}
\begin{align*}
\Mzero \xvec &= 
( \Imat_{N} - N^{-1} \onevec_{N} \onevec_{N}' ) \xvec 	= 
\xvec - N^{-1} \onevec_{N} (\onevec_{N}' \xvec) = 
\xvec - \onevec_{N} \overline{x}
\end{align*}

\noindent
onde podemos denotar

\vspace{-1 em}
\begin{align*}
\overline{\xvec} 	=
\onevec_{N} \overline{x}	=
\begin{bmatrix}
	\overline{x} \\ \vdots \\ \overline{x}
\end{bmatrix}
\end{align*}

\vspace{-1 em}
\begin{align*}
\Mzero \xvec 	= 	\xvec - \overline{\xvec} 
\end{align*}

\vspace{-1 em}
\begin{align*}
\Mzero \onevec &= ( \Imat_{N} - N^{-1} \onevec_{N} \onevec_{N}' ) \onevec_{N}	= 
\onevec_{N} - N^{-1} \onevec_{N} (\onevec_{N}' \onevec_{N})	= \onevec_{N} - \onevec_{N}	= \zerovec_{N} 
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Operações com Matrizes}

\begin{description}
	\item[Multiplicação por escalar] 
	\item[Soma] 
	\item[Subtração] 
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Multiplicação de Matriz}

\begin{align*}
	A_{2 \times 2} =
	\begin{bmatrix}
		a_{11} & a_{12} \\
		a_{21} & a_{22}		
	\end{bmatrix}
	\quad
	B_{2 \times 3} =
	\begin{bmatrix}
		b_{11} & b_{12} & b_{13} \\
		b_{21} & b_{22} & b_{23}		
	\end{bmatrix}
\end{align*}

\begin{align*}
	[AB]_{2 \times 3} &=
	\begin{bmatrix}
		a_{11} \\ a_{21}
	\end{bmatrix}
	\begin{bmatrix}
		b_{11} & b_{12} & b_{13}
	\end{bmatrix}
	+
	\begin{bmatrix}
		a_{12} \\ a_{22}
	\end{bmatrix}
	\begin{bmatrix}
		b_{21} & b_{22} & b_{23}
	\end{bmatrix}
	\implies
	AB = \sum_{i=1}^{2} a_{i}b_{i}
\end{align*}

\noindent
onde
$a_{i}$ é a $i$-ésima \textbf{coluna} da matriz $A$.
$b_{i}$ é a $i$-ésima \textbf{linha} da matriz $B$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Projeções}

\vspace{-2 em}
\begin{align*}
	\PX &= \Xmat (\Xmat' \Xmat)^{-1} \Xmat'
	\\
	\MX &= (\Imat - \Xmat (\Xmat' \Xmat)^{-1} \Xmat')
\end{align*}

Usando o modelo

\vspace{-1 em}
\begin{align*}
	\yvec = \Xmat \betavec + \uvec
\end{align*}

E definindo $\betavechat = (\Xmat' \Xmat)^{-1} \Xmat' \yvec$, temos

\vspace{-1 em}
\begin{align*}
	\PX \yvec  &= \Xmat (\Xmat' \Xmat)^{-1} \Xmat' \yvec = \Xmat \betavechat = \widehat{\yvec}
	\\
	\MX \yvec  &= \Imat \yvec  - \Xmat (\Xmat' \Xmat)^{-1} \Xmat' \yvec 
	=
	\yvec  - \Xmat \betavechat 
	=
	\yvec - \widehat{\yvec} = \uvechat
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Cálculo com Matrizes}
\noindent
\citet[A.8, p.1007]{greene-7ed}\\
\citet[C.23, p.157]{matman} \\
\citet[Sec.2.3, p.10]{matcook2012} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1 em}
Uma \textbf{função linear} pode ser escrita como:
\begin{align*}
	y(\xvec) = \avec' \xvec = \xvec' \avec = \sum_{i=1}^{N}a_{i} x_{i}
\end{align*}

\noindent
então
\begin{align*}
	\pdf{y(\xvec)}{\xvec} = \pdf{\avec' \xvec}{\xvec} = \avec
\end{align*}

\noindent
Em particular, note que $\pdf{\avec'\xvec}{\xvec} = \avec$ não $\avec'$.

\vspace{1 em}
\noindent
Em um conjunto de funções lineares:
\begin{align*}
	\yvec (\xvec) = \Amat \xvec 
\end{align*}

\noindent
onde cada elemento $y_{i}$ de $\yvec$ é:
\begin{align*}
	y_{i}(\xvec) = \avec_{i}' \xvec 
\end{align*}

\noindent
onde $\avec_{i}$ é a $i$-ésima linha de $\Amat$.
Portanto
\begin{align*}
	\pdf{y_{i}}{\xvec} = \avec_{i} = \text{transposta da $i$-ésima linha de $\Amat$}.
\end{align*}

\noindent e
\begin{align*}
\begin{bmatrix}
\pdf{y_{1}}{\xvec'}	\\ \vdots \\ \pdf{y_{N}}{\xvec'}	
\end{bmatrix}
=
\begin{bmatrix}
\avec_{1}'	\\ \vdots \\ \avec_{N}'
\end{bmatrix}
= \Amat
\end{align*}

\noindent
ou seja,
\begin{align*}
	\pdf{\yvec(\xvec)}{\xvec'} = \pdf{\Amat \xvec}{\xvec'} = \Amat,
\end{align*}

\noindent
ou também,
\begin{align*}
	\pdf{\Amat \xvec}{\xvec} = \Amat'.
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{1 em}
Uma \textbf{forma quadrática} pode ser escrita como:
\begin{align*}
\xvec' \Amat \xvec = \sum_{i=1}^{N}\sum_{j=1}^{N} x_{i} x_{j} a_{ij}.
\end{align*}

\noindent
A derivada em relação a $\xvec$ é:
\begin{align*}
	\pdf{\xvec' \Amat \xvec}{\xvec} =  (\Amat + \Amat') \xvec
\end{align*}

\noindent
no caso de simetria de $\Amat$ (\Amat = \Amat'), temos:
\begin{align*}
	\pdf{\xvec' \Amat \xvec}{\xvec} =  2\Amat\xvec
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Conceitos Básicos de Convergência Estatística} \label{app:est}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}[\textbf{Convergência em Probabilidade}]
	\citet[Def 3.3, p.36 ]{wool-2010}

	Uma sequência de variáveis aleatórias:
	$\{ X_{N} \}_{N \geq 1}$ 
	\textbf{converge em probabilidade} para uma variável aleatória $X$ se, dado $\err > 0$, 

	\vspace{-1 em}
	\begin{align*}
		P(| X_{N} - X | > \err ) \to 0,
	\end{align*}

	\noindent
	quando $N \to + \infty$.
	E denotamos

	\vspace{-1 em}
	\begin{align*}
		\plim X_{N} = X,
		\quad \text{ou}	\quad
		X_{N} \arrowp X,
		\quad \text{ou}	\quad
		X_{N} - X \arrowp 0.
	\end{align*}
\end{defn}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}[\textbf{Estimador Consistente}]
	\citet[Def 3.8, p.40 ]{wool-2010}

	Seja $\seq{\thetabold_{N}: N=1, 2, \dots}$ uma sequência de estimadores do vetor $\thetabold \in \Thetabold$ com dimensão $P \times 1$, onde $N$ indexa o tamanho da amostra.
	Se

	\vspace{-1 em}
	\begin{align} \label{consist}
		\widehat{\thetabold} \overset{p}{\longrightarrow} \thetabold
	\end{align}
	Para qualquer valor de $\thetabold$, então dizemos que $\thetabold_{N}$ é um estimador consistente de $\thetabold$.
\end{defn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{teo}[\textbf{LGN -- Lei dos Grandes Números}] \label{teo:lgn}
	\red{Achar referência}
% \citet[Teo 3.1, p.39 ]{wool-2010}

	Seja $\seq{X_{i}}_{i \geq 1}$ uma sequência de variáveis aleatórias $iid$ com $\E(X_{i}) = \mu$.
	Então, 

	\vspace{-1 em}
	\begin{align*}
		N^{-1} \sum_{i=1}^{N} X_{i} \arrowp \mu.
	\end{align*}
\end{teo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{teo}[\textbf{LGN -- Caso Matricial}] \label{teo:lgn:mat:1}
	\red{Achar referência. O Teorema \eqref{teo:lgn:mat}, abaixo, é diferente.}

	Seja $\seq{\xvec_{i}}_{i=1}^{N}$, uma sequência $iid$ de vetores aleatórios $K \times 1$ com $\E(\xvec_{i} \xvec_{i}') = Q_{K \times K}$ finita.
	Então, 

	\vspace{-1 em}
	\begin{align*}
		N^{-1} \sum_{i=1}^{N}
		\xvec_{i} \xvec_{i}'
		\arrowp Q.
	\end{align*}

	Se $Q$ for positiva definida, $Q$ terá inversa.
\end{teo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{teo}[\textbf{LGNF -- WLLN}] \label{teo:lgn:mat}
	\citet[Teo 3.1, p.39 ]{wool-2010}

	Seja $\seq{\wvec_{i} : i=1,2, \dots}$, uma sequência $iid$ de vetores aleatórios $G \times 1$ com
	$\E(|w_{ig}|) < \infty$ para $g = 1, \dots, G$.
	Então, a seguência satisfaz a \textbf{Lei dos Grandes Números Fraca (WLLN)}:

	\vspace{-1 em}
	\begin{align*}
		N^{-1} \sum_{i=1}^{N}
		\wvec_{i} \arrowp \muvec_{w},
	\end{align*}
	onde $\muvec_{w} \equiv \E(\wvec_{i})$.
\end{teo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}[$\litop$]
\item
	\citet[Def 3.4, p.36 ]{wool-2010}\\
	\citet[Lemma 3.2, p.36 ]{wool-2010}

	\vspace{-1 em}
	\begin{align*}
		X_{n} = \litop(1) & \implies X_{n} \arrowp 0
		\\
		X_{n} = \litop(Y_{n}) & \implies
		\frac{X_{n}}{Y_{n}} = \litop(1) \implies
		\frac{X_{n}}{Y_{n}} \arrowp 0
		\\
		X_{n} = W_{n} + \litop(1) & \implies
		(X_{n} - W_{n}) = \litop(1) \implies
		(X_{n} - W_{n}) \arrowp 0
	\end{align*}
\end{defn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}[\textbf{Limitação em Probabilidade:} $\bigOp$]
	\citet[Def 3.3 (3), p.36 ]{wool-2010}

	Dizemos que $X_{n}$ é \textbf{limitado em probabilidade} e denotado por 
	$X_{n} = \bigOp(1)$,
	se existe $M$ maior que zero, tal que para todo $\err$ maior que zero, $P( | X_{n} | > 0) < \err$.

	\vspace{-1 em}
	\begin{align*}
		X_{n} = \bigOp(1) \implies \exists M > 0 \, ; \;
		\forall \err > 0 \, , \;
		P( | X_{n} | > 0) < \err.
	\end{align*}
\end{defn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}
	\citet[Def 3.4, p.36]{wool-2010}

	Dizemos que
	$X_{n} = \bigOp(Y_{n})$ se existe $M$ maior que zero, tal que para todo $\err$ maior que zero, $P( | X_{n}/Y_{n} | > 0) < \err$.

	\vspace{-1 em}
	\begin{align*}
		X_{n} = \bigOp(Y_{n}) \implies \exists M > 0 \, ; \;
		\forall \err > 0 \, , \;
		P( | X_{n}/Y_{n} | > 0) < \err.
	\end{align*}
\end{defn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lem}
	\citet[Lemma 3.2, p.36]{wool-2010}

	Se
	$X_{n} = \bigOp(1)$ e $Y_{n} = \litop(1)$, então

	\vspace{-1 em}
	\begin{align*}
		X_{n} Y_{n} = \bigOp(1) \litop(1) = \litop(1).
	\end{align*}
\end{lem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lem}[\textbf{Equivalência Assintótica}] \label{lem:equiv:assin}
	\citet[Lemma 3.7, p.39]{wool-2010}

	Seja
	$\seq{\xvec_{n}}$ e $\seq{\mbs{z}_{n}}$
	sequências de vetores aleatórios $K \times 1$.
	Se $\mbs{z}_{n} \arrowd \mbs{z}$ e 
	$\xvec_{n} - \mbs{z}_{n} \arrowp \zerovec_{K}$.
	Então, 

	\vspace{-1 em}
	\begin{align*}
		\xvec_{n} \arrowd \mbs{z}.
	\end{align*}

\end{lem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}[\textbf{Convergência em Distribuição}]
	\citet[Def 3.6, p.38]{wool-2010}

	Seja
	$\seq{X_{n}}_{n \geq 1}$  uma sequência de variáveis aleatórias e $X$ uma variável aleatória com $F_{n}$ e $F$ suas respectivas FDAs, então

	\vspace{-1 em}
	\begin{align*}
		X_{n} \arrowd X, \text{ se } F_{n}(X) \to F(X)
	\end{align*}
	\noindent
	para todo $X$ onde $F$ é contínuo.
\end{defn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lem}[\textbf{Convergência em Distribuição e Limitação em Probabilidade}]
	\citet[Lemma 3.5, p.39]{wool-2010}

	Se $X_{n} \arrowd X$, $X$ um variável aleatória qualquer; então $X_{n} = \bigOp(1)$.
\end{lem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}[\textbf{TCL -- Teorema Central do Limite}] \label{def:tcl}
	\red{Achar Referência}
% \citet[Lemma 3.5, p.XX ]{wool-2010}

	Seja $\seq{X_{n}}_{n = 1}^{N}$ $iid$ com $\E(X_{n}) = \mu$ e $\Var(X_{n}) = \sigma^{2} < + \infty$.
	Então, para $S_{N} = \sum_{n=1}^{N} X_{n}$:

	\begin{align*}
		\frac{S_{N} - N \mu }{ \sqrt{N} \sigma}
		=
		\frac{N ( \overline{X} - \mu ) }{ \sqrt{N} \sigma}
		=
		\frac{N ( \overline{X} - \mu ) }{ \sqrt{N} \sigma}
		=
		\boxed{
			\frac{\sqrt{N} ( \overline{X} - \mu ) }{\sigma}
		\arrowd Z \sim N(0,1)}.
	\end{align*}
\end{defn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{teo}[\textbf{TCL -- Lindeberg-Levy}] \label{def:tcl:vec}
	\citet[Teo 3.2, p.40]{wool-2010}

	Seja $\seq{\wvec_{i}:i=1,2,\dots}$ uma sequência $iid$ de vetores aleatórios $G \times 1$ com
	$\E( w_{ig}^{2} ) < \infty$ para $g= 1, \dots G$ 
	e
	$\E( \mbs{w}_{i} ) = \zerovec$.
	Então, $\seq{\wvec_{i}:i=1,2,\dots}$ satisfaz o \textbf{Teorema Central do Limite (CLT)}; 
	qual seja:

	\vspace{-1 em}
	\begin{align*}
		N^{-1/2} \sum_{i=1}^{N} \mbs{w}_{i} \arrowd \nor(\zerovec, \Bmat),
	\end{align*}

	\noindent
	onde, $\Bmat = \Var(\mbs{w}_{i}) = \E( \mbs{w}_{i} \mbs{w}_{i}')$ é necessariamente positiva semidefinida.
	Para nossos propósitos, $\Bmat$ será sempre positiva definida.
\end{teo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{cor}[]
	\citet[Cor 3.2, p.39]{wool-2010}

	Seja $\seq{\zvec_{N}}$ uma sequência de vetores $K \times 1$ aleatórios tal que
	$\zvec_{N} \arrowd \nor(\zerovec, \Vmat)$, então:

	\begin{enumerate}
		\item 
			Para qualquer matriz $\Amat$ de dimensão $K \times M$ \textbf{não} estocástica, temos:
			\begin{align*}
				\Amat' \zvec_{N} \arrowd \nor(\zerovec, \Amat'\Vmat\Amat).
			\end{align*}

		\item $\zvec_{N}'\Vmat^{-1}\zvec_{N} \arrowd \chisq_{K}$ (ou $\zvec_{N}'\Vmat^{-1}\zvec_{N} \asim \chisq_{K}$).
	\end{enumerate}


\end{cor}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}[\textbf{raiz de $N$ assintoticamente normalmente distribuido}]
	\citet[Def 3.9, p.40]{wool-2010}

	Seja $\seq{\thetaboldhat_{N}: N=1,2,\dots}$ uma sequência de estimadores do vetor $\thetabold \in \Thetabold$ com dimensão $P \times 1$.
	Suponha que

	\vspace{-1 em} 
	\begin{align}\label{nroot:conv}
		\sqrt{N}(\thetaboldhat_{N} - \thetabold) 
		\arrowd 
		\nor(\zerovec, \Vmat)
	\end{align}
	onde $\Vmat$ é uma matriz $P \times P$ positiva semidefinida.
	Então dizermos que $\thetaboldhat_{N}$ é $\sqrt{N}$-asintoticamente normalmente distribuido e
	$\Vmat$ é a \textbf{variância assintótica} de 
	$\sqrt{N}(\thetaboldhat_{N} - \thetabold)$,
	denotada $\Avar \sqrt{N}(\thetaboldhat_{N} - \thetabold) = \Vmat$
\end{defn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
	Apesar de $\Vmat/N = \Var(\thetaboldhat_{N})$ ser verdade apenas em casos especiais, e raramente $\thetaboldhat_{N}$ ter uma distribuição exatamete normal, tratamos $\thetaboldhat_{N}$ como se 

	\vspace{-1 em}
	\begin{align}\label{eq:3.4}
		\thetaboldhat_{N} \sim \nor(\thetabold, \Vmat/N).
	\end{align}
	sempre que a equação \eqref{nroot:conv} for verdade.
	Por essa razão, $\Vmat/N$ é chamado de \textbf{variância assintótica} de $\thetaboldhat_{N}$, e escrevemos:

	\vspace{-1 em}
	\begin{align}\label{eq:3.5}
		\Avar(\thetaboldhat_{N}) = \Vmat/N.
	\end{align}

\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hline
\vspace{1 ex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\red{Abaixo, temos as definições necessárias para mostrar como verificar se um estimador é consistente por EQM.
	Quero saber se mantemos essa parte.
Se sim, precisamos de referências.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}[\textbf{Desigualdade de Markov}]
	\red{Achar referência}

	Seja
	$\{ X_{n} \}_{n \geq 1}$ 
	uma sequência de variáveis aleatórias com
	$E|X_{n}|^{K} < +\infty$, $K>0$. 
	Então, dado $\err >0$

	\vspace{-1 em}
	\begin{align*}
		P(| X_{n} | > \err ) \leq \frac{E|X_{n}|^{K}}{\err^{K}}
	\end{align*}
\end{defn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn} %[\textbf{}]
	\red{Achar referência}

	\begin{align*}
		0 \leq P(| \thetahat - \theta | > \err ) \leq \frac{E|X_{n}|^{2}}{\err^{2}}
	\end{align*}
\end{defn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}[\textbf{Erro Quadrático Médio}]
	\red{Achar referência}

	\begin{align*}
		EQM(\thetahat) 
		=
		\E\left[ \left( \thetahat - \theta \right)^2 \right] 
		=
		\left[ Bias(\thetahat)^{2} + \Var(\thetahat) \right]
	\end{align*}
\end{defn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
\red{Achar referência?}

Então, se $Bias(\thetahat) \to 0$ e $\Var(\thetahat) \to 0$, temos que $EQM(\thetahat) \to 0$.
Pelo \textbf{Teorema do Sanduíche}, $P(|\thetahat - \theta| > \err) \to 0$; logo, $\thetahat \arrowp \theta$.
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Bibliografia
\clearpage
\bibliographystyle{authordate3} % bibliography style
\renewcommand\bibname{REFERENCES} 
\bibliography{../refs.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}


